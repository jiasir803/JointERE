{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/notebooks/sinica/dataset/'\n",
    "\n",
    "relation_data = root+'facial_r3.train'\n",
    "schema_root = root+'schema_2.txt'\n",
    "dev_data = root+'facial_r3.dev'\n",
    "test_data = root+'skincare.dev'\n",
    "\n",
    "UNKOWN_TAG = \"<UNKNOWN>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "REL_NONE = 'Rel-None'\n",
    "REL_PAD = 'Rel-Pad'\n",
    "\n",
    "MAX_LEN = 80     # original 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(data):\n",
    "    with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().splitlines()\n",
    "        \n",
    "    return content\n",
    "\n",
    "\n",
    "def filter_MAX_LEN(content, MAX_LEN):\n",
    "    init_loc = 0\n",
    "    new_content = []\n",
    "    \n",
    "    for now_loc, now_token in enumerate(content):\n",
    "        if now_token=='' and (now_loc-init_loc <= MAX_LEN):\n",
    "            new_content.extend(content[init_loc:now_loc])\n",
    "            new_content.append('')\n",
    "            init_loc = now_loc+1\n",
    "            \n",
    "        elif now_token=='' and (now_loc-init_loc > MAX_LEN):\n",
    "            init_loc = now_loc+1\n",
    "        \n",
    "    return new_content\n",
    "\n",
    "\n",
    "# ================================================\n",
    "\n",
    "def get_word_and_label(_content, start_w, end_w):\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "    \n",
    "    for word_set in _content[start_w:end_w]:\n",
    "        word_set = word_set.split()\n",
    "        if len(word_set)==1:\n",
    "            word_list.append(' ')\n",
    "            ent_list.append('O')\n",
    "            rel_list.append(REL_NONE)\n",
    "        \n",
    "        else:\n",
    "            word_list.append(word_set[0])\n",
    "            ent_list.append(word_set[1])\n",
    "\n",
    "            try:\n",
    "                testerror = word_set[2]\n",
    "            except IndexError:\n",
    "                rel_list.append(REL_NONE)\n",
    "            else:\n",
    "                rel_list.append(word_set[2:])\n",
    "    \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "\n",
    "def split_to_list(content):\n",
    "    init = 0\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "\n",
    "    for now_token, c in enumerate(content):\n",
    "        if c=='':\n",
    "            words, ents, rels = get_word_and_label(content, init, now_token)\n",
    "            init = now_token+1\n",
    "            word_list.append(words)\n",
    "            ent_list.append(ents)\n",
    "            rel_list.append(rels)\n",
    "            \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "# ================================================\n",
    "\n",
    "\n",
    "def gen_dataset_in_MAX_LEN(data, MAX_LEN):\n",
    "    content = readfile(data)\n",
    "    new_content = filter_MAX_LEN(content, MAX_LEN)\n",
    "    \n",
    "    return new_content\n",
    "\n",
    "def save_file(file_name, dataset):      \n",
    "    with open(file_name, 'w', encoding='utf8') as text_file:   \n",
    "        for item in dataset:\n",
    "            text_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = gen_dataset_in_MAX_LEN(relation_data, MAX_LEN)\n",
    "new_dev = gen_dataset_in_MAX_LEN(dev_data, MAX_LEN)\n",
    "new_test = gen_dataset_in_MAX_LEN(test_data, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_path = root+'facial_r3_len'+str(MAX_LEN)+'.train'\n",
    "new_dev_path = root+'facial_r3_len'+str(MAX_LEN)+'.dev'\n",
    "new_test_path = root+'facial_r3_len'+str(MAX_LEN)+'.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list, ent_list, rel_list = split_to_list(new_dev)\n",
    "# len(word_list), len(ent_list), len(rel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(new_train_path, new_train)\n",
    "save_file(new_dev_path, new_dev)\n",
    "save_file(new_test_path, new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
