{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(data):\n",
    "    with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().splitlines()\n",
    "        \n",
    "    return content\n",
    "\n",
    "def schema_load(schema_root):\n",
    "    raw_dict = \"\".join(readfile(schema_root))\n",
    "    dict2json = \"\".join(raw_dict.split()[2:])\n",
    "\n",
    "    json_acceptable_string = dict2json.replace(\"'\", \"\\\"\")\n",
    "    schema = json.loads(json_acceptable_string)\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def define_entity(schema):\n",
    "    tag_type = list(schema['tagging'])\n",
    "    \n",
    "    entity_tag = []\n",
    "    for k in list(schema['entity'].keys()):\n",
    "        entity_tag.append(schema['entity'][k]['tag'])\n",
    "        \n",
    "    TAG = []\n",
    "    for t in tag_type:\n",
    "        for e in entity_tag:\n",
    "            if t!='O':\n",
    "                TAG.append(t+'-'+e)  \n",
    "                \n",
    "    TAG = [UNKOWN_TAG, PAD_TAG] + TAG + ['O']   \n",
    "\n",
    "    return TAG\n",
    "\n",
    "def tag2ix(TAG):\n",
    "    tag_to_ix={t:i for i,t in enumerate(TAG)}\n",
    "    return tag_to_ix\n",
    "\n",
    "def define_relation(schema):\n",
    "    relation_type = list(schema['relation'])\n",
    "    \n",
    "    relation_tag = []\n",
    "    for k in list(schema['relation'].keys()):\n",
    "        relation_tag.append(schema['relation'][k]['tag'])\n",
    "    \n",
    "    relation_tag = [REL_PAD] + [REL_NONE] + relation_tag\n",
    "        \n",
    "    return relation_tag\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def get_word_and_label(_content, start_w, end_w):\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "    \n",
    "    for word_set in _content[start_w:end_w]:\n",
    "        word_set = word_set.split()\n",
    "        if len(word_set)==1:\n",
    "            word_list.append(' ')\n",
    "            ent_list.append('O')\n",
    "            rel_list.append(REL_NONE)\n",
    "        \n",
    "        else:\n",
    "            word_list.append(word_set[0])\n",
    "            ent_list.append(word_set[1])\n",
    "\n",
    "            try:\n",
    "                testerror = word_set[2]\n",
    "            except IndexError:\n",
    "                rel_list.append(REL_NONE)\n",
    "            else:\n",
    "                rel_list.append(word_set[2:])\n",
    "    \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "def split_to_list(content):\n",
    "    init = 0\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "\n",
    "    for now_token, c in enumerate(content):\n",
    "        if c=='':\n",
    "            words, ents, rels = get_word_and_label(content, init, now_token)\n",
    "            init = now_token+1\n",
    "            word_list.append(words)\n",
    "            ent_list.append(ents)\n",
    "            rel_list.append(rels)\n",
    "            \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def word2index(word_list):\n",
    "    word_to_ix = {\"<UNKNOWN>\":0, \"<PAD>\":1}\n",
    "    for sentence in word_list:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "                \n",
    "    return word_to_ix\n",
    "\n",
    "def dict_inverse(tag_to_ix):\n",
    "    ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "    return ix_to_tag\n",
    "\n",
    "def index2tag(indexs, ix_to):\n",
    "    to_tags = [ix_to[i] for i in indexs.cpu().numpy()]\n",
    "    return to_tags\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def find_max_len(word_list):\n",
    "    max_len = 0\n",
    "    for i in range(len(word_list)):\n",
    "        if max_len<len(word_list[i]):\n",
    "            max_len=len(word_list[i])\n",
    "            \n",
    "    return max_len\n",
    "\n",
    "# ====== filter the length of sentence more than MAX_LEN =======\n",
    "\n",
    "def filter_len(word_list):\n",
    "    reserved_index = []\n",
    "    for i in range(len(word_list)):\n",
    "        if len(word_list[i])<MAX_LEN:\n",
    "            reserved_index.append(i)\n",
    "            \n",
    "    return reserved_index\n",
    "\n",
    "\n",
    "def filter_sentence(reserved_index, word_list, ent_list, rel_list):\n",
    "    filter_word = list(word_list[i] for i in reserved_index)\n",
    "    filter_ent = list(ent_list[i] for i in reserved_index)\n",
    "    filter_rel = list(rel_list[i] for i in reserved_index)\n",
    "    return filter_word, filter_ent, filter_rel\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def pad_seq(seq, isrel):\n",
    "    if isrel:\n",
    "        seq += [REL_NONE for i in range(MAX_LEN-len(seq))]\n",
    "    else:\n",
    "        seq += [PAD_TAG for i in range(MAX_LEN-len(seq))]\n",
    "    return seq\n",
    "\n",
    "def pad_all(filter_word, filter_ent, filter_rel):\n",
    "    input_padded = [pad_seq(s, False) for s in filter_word]\n",
    "    ent_padded = [pad_seq(s, False) for s in filter_ent]\n",
    "    rel_padded = [pad_seq(s, True) for s in filter_rel]\n",
    "    \n",
    "    return input_padded, ent_padded, rel_padded\n",
    "\n",
    "def deep_copy_lists(filter_word, filter_ent, filter_rel):\n",
    "    f_w = copy.deepcopy(filter_word)\n",
    "    f_e = copy.deepcopy(filter_ent)\n",
    "    f_r = copy.deepcopy(filter_rel)\n",
    "    \n",
    "    return f_w, f_e, f_r\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w not in to_ix:\n",
    "            idxs.append(to_ix[UNKOWN_TAG])\n",
    "        else:\n",
    "            idxs.append(to_ix[w])\n",
    "    \n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_all(seqs, to_ix):\n",
    "    seq_list = []\n",
    "    for i in range(len(seqs)):\n",
    "        seq_list.append(prepare_sequence(seqs[i], to_ix))\n",
    "        \n",
    "    seq_list = torch.stack(seq_list)\n",
    "        \n",
    "    return seq_list\n",
    "\n",
    "\n",
    "\n",
    "def prepare_rel(rel_padded, to_ix):\n",
    "    \n",
    "    rel_ptr = torch.zeros(len(rel_padded), MAX_LEN, MAX_LEN, dtype=torch.long) \n",
    "    \n",
    "    # 對當前的token，去比較之前所有出現過的entity，是否有關係，建成矩陣\n",
    "    # [B*ML*ML]，第二維ML是當前token，第三維ML是根據當前token對之前出現過的entity紀錄關係，以index紀錄\n",
    "    for i, rel_seq in enumerate(rel_padded):\n",
    "        rel_dict = {}\n",
    "        for j, token_seq in enumerate(rel_seq):\n",
    "            rel_ptr[i][j][:j+1] = 1\n",
    "            if token_seq != REL_NONE:\n",
    "                for k, rel in enumerate(token_seq):\n",
    "\n",
    "                    # if 是第一次出現，紀錄後面數字(標第幾對)和關係位置(A OR B)\n",
    "                    # 假如下次出現又是同個關係位置(A)，依然紀錄\n",
    "                    # 直到下次出現關係位置B，依照之前紀錄的A位置的字，然後在第三維去標關係\n",
    "\n",
    "                    rel_token = rel.split('-')\n",
    "                    if rel_token[1] not in rel_dict:\n",
    "                        rel_dict[rel_token[1]] = {'rel':rel_token[0], 'loc':rel_token[2], 'idx':[j]}\n",
    "\n",
    "                    elif rel_token[1] in rel_dict and rel_dict[rel_token[1]]['loc']==rel_token[2]:\n",
    "                        rel_dict[rel_token[1]]['idx'].append(j)\n",
    "\n",
    "                    else:\n",
    "                        record_loc = rel_dict[rel_token[1]]['idx']\n",
    "                        for idxx in record_loc:\n",
    "                            rel_ptr[i][j][idxx] = to_ix[rel_token[0]]\n",
    "                            \n",
    "    return rel_ptr\n",
    "                \n",
    "\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def dataload(input_var, ent_var, rel_var, raw_input):\n",
    "    torch_dataset = Data.TensorDataset(input_var, ent_var, rel_var, raw_input)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=2,       \n",
    "#         drop_last=True\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# ==================================================\n",
    "def softmax_entity(entity):\n",
    "    entity = entity.view(BATCH_SIZE,ent_size).argmax(1)\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, attn_input, attn_output, rel_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.attn_input = attn_input\n",
    "        self.attn_output = attn_output\n",
    "        self.rel_size = rel_size\n",
    "        \n",
    "        self.w1 = nn.Linear(self.attn_input, self.attn_output)\n",
    "        self.w2 = nn.Linear(self.attn_input, self.attn_output)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.v = nn.Linear(self.attn_output, self.rel_size, bias=False)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        \n",
    "        decoder = encoder_outputs[:,-1,:].unsqueeze(1)                       #B*1*(ts+LE) [128,1,8]\n",
    "        encoder_score = self.w1(encoder_outputs)                             #B*now len*ATTN_OUT\n",
    "        decoder_score = self.w2(decoder)                                     #B*1*ATTN_OUT\n",
    "        energy = self.tanh(encoder_score+decoder_score)                      #B*now len*ATTN_OUT            \n",
    "        \n",
    "        energy = self.v(energy)                                              #B*now len*rel_size\n",
    "        \n",
    "        \n",
    "        # 針對每個entity做softmax，去顯示他們的關係權重\n",
    "        # 主要都會是rel_none\n",
    "        # 對第二維(rel)做softmax\n",
    "        p = self.softmax(energy)                                         #B*now len*rel_size\n",
    "        \n",
    "        return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity_Typing(nn.Module):\n",
    "    def __init__(self, vocab_size, ent_tag_to_ix, embedding_dim, hidden_dim1, hidden_dim2, \\\n",
    "                 label_embed_dim, rel_tag_to_ix):\n",
    "        \n",
    "        super(Entity_Typing, self).__init__()\n",
    "        self.embedding_dim = embedding_dim                   #E\n",
    "        self.hidden_dim1 = hidden_dim1                       #h1\n",
    "        self.hidden_dim2 = hidden_dim2                       #h2\n",
    "        self.label_embed_dim = label_embed_dim               #LE\n",
    "        self.vocab_size = vocab_size                         #vs\n",
    "        self.ent_to_ix = ent_tag_to_ix\n",
    "        self.ent_size = len(ent_tag_to_ix)                   #es\n",
    "        self.rel_to_ix = rel_tag_to_ix\n",
    "        self.rel_size = len(rel_tag_to_ix)                   #rs           \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn = nn.BatchNorm1d(DENSE_OUT, momentum=0.5, affine=False)\n",
    "        \n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         self.bilstm = nn.LSTM(embedding_dim, hidden_dim1 // 2,\n",
    "#                             num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)        \n",
    "        self.bilstm = nn.GRU(embedding_dim, hidden_dim1 // 2,\n",
    "                            num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.dense = nn.Linear(hidden_dim1, DENSE_OUT)\n",
    "        self.top_hidden = nn.LSTMCell(DENSE_OUT+label_embed_dim, hidden_dim2)          \n",
    "        \n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim2, self.ent_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.label_embed = nn.Linear(self.ent_size, self.label_embed_dim)\n",
    "        \n",
    "        self.attn = Attn(ATTN_IN, ATTN_OUT, self.rel_size)\n",
    "        \n",
    "        \n",
    "    def init_hidden1(self):       \n",
    "        hidden = torch.randn(2*2, self.batch, self.hidden_dim1 // 2)    #4*B*(h1/2)\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_hidden2(self):       \n",
    "        hidden = torch.randn(self.batch, self.hidden_dim2)              #B*h2\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_label_embed(self):\n",
    "        hidden = torch.zeros(self.batch, self.label_embed_dim)          #B*LE\n",
    "        return hidden.cuda()if USE_CUDA else hidden\n",
    "    \n",
    "    def create_entity(self):\n",
    "        output_tensor = torch.zeros(self.batch, MAX_LEN, self.ent_size)  #B*ML*es\n",
    "        return output_tensor.cuda()if USE_CUDA else output_tensor\n",
    "    \n",
    "    def create_rel_matrix(self):\n",
    "        rel_tensor = torch.zeros(self.batch, MAX_LEN, MAX_LEN, self.rel_size)  #B*ML*ML*rs\n",
    "        return rel_tensor.cuda()if USE_CUDA else rel_tensor\n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        self.batch = sentence.size(0)\n",
    "#         self.hidden1 = self.init_hidden1()                      #4*B*(h1/2)\n",
    "        entity_tensor = self.create_entity()                    #B*ML*es\n",
    "        rel_tensor = self.create_rel_matrix()                   #B*ML*ML*rs\n",
    "        \n",
    "        \n",
    "\n",
    "        embeds = self.word_embeds(sentence)                     #B*ML*E,[128, 100, 20]\n",
    "        \n",
    "#         bilstm_out, self.hidden1 = self.bilstm(embeds, self.hidden1)\n",
    "        bilstm_out, hidden1 = self.bilstm(embeds)\n",
    "        # bilstm_out -> B*ML*h1,[128, 100, 10]\n",
    "        # self.hidden1 -> ( 4*B*(h1/2), 4*B*(h1/2) )\n",
    "        \n",
    "        # bn\n",
    "        bilstm_out = self.bn(bilstm_out)\n",
    "        dense_out = self.dense(bilstm_out)                      #B*ML*DENSE_OUT,[128, 100, 100]\n",
    "        \n",
    "        \n",
    "        encoder_sequence_l = [] \n",
    "\n",
    "        for length in range(MAX_LEN):\n",
    "            now_token = dense_out[:,length,:]\n",
    "            now_token = torch.squeeze(now_token, 1)\n",
    "            if length==0:\n",
    "                \n",
    "#                 fake_hidden=(100)\n",
    "#                 noise_x = random(100)\n",
    "                self.hidden2 = self.init_hidden2()\n",
    "                self.zero_label_embed = self.init_label_embed()\n",
    "                combine_x = torch.cat((now_token, self.zero_label_embed),1)  #B*(DENSE_OUT+LE),[128, 103]\n",
    "                \n",
    "            else:\n",
    "#                 fake_hidden=h\n",
    "                self.hidden2 = (h_next, c_next)\n",
    "                combine_x = torch.cat((now_token, label),1)\n",
    "\n",
    "            h_next, c_next = self.top_hidden(combine_x, self.hidden2)    #B*h2,[128, 8]           \n",
    "            to_tags = self.hidden2tag(h_next)                            #B*es,[128, 5]            \n",
    "            ent_output = self.softmax(to_tags)                               #B*es,[128, 5]             \n",
    "            label = self.label_embed(ent_output)                             #B*LE,[128, 3]\n",
    "            \n",
    "            s_ent_output = self.softmax_entity(ent_output)\n",
    "            \n",
    "            \n",
    "            # Assignments to Variables are in-place operations.\n",
    "            # Use that variable in lots of other contexts \n",
    "            # and some of the functions require it to not change. \n",
    "            to_tags_clone = to_tags.clone()\n",
    "            label_clone = label.clone()\n",
    "            \n",
    "            \n",
    "#             for i, tag in enumerate(s_ent_output):\n",
    "#                 if tag==ent_tag_to_ix['O']:\n",
    "#                     to_tags_clone[i] = torch.FloatTensor([-999999 * self.ent_size])\n",
    "#                     label_clone[i] = torch.FloatTensor([-999999 * self.ent_size])\n",
    "                    \n",
    "            # relation layer\n",
    "#             encoder_sequence_l.append(torch.cat((to_tags,label),1)) \n",
    "            encoder_sequence_l.append(torch.cat((h_next,label),1))  \n",
    "            encoder_sequence = torch.stack(encoder_sequence_l).t()     #B*len*(es+LE), [128,1,8]          \n",
    "\n",
    "            # Calculate attention weights \n",
    "            attn_weights = self.attn(encoder_sequence)\n",
    "\n",
    "        \n",
    "            entity_tensor[:,length,:] = ent_output\n",
    "            \n",
    "            # rel_tensor[:,length, 頭~當前 ,:]\n",
    "            rel_tensor[:,length,:length+1,:] = attn_weights\n",
    "\n",
    "        \n",
    "        \n",
    "        '''NLLLoss input: Input: (N,C) where C = number of classes'''\n",
    "        return entity_tensor.view(self.batch*MAX_LEN, self.ent_size), \\\n",
    "               rel_tensor.view(self.batch*MAX_LEN*MAX_LEN, self.rel_size)\n",
    "        \n",
    "        \n",
    "    def softmax_entity(self, entity):\n",
    "        entity = entity.view(self.batch,ent_size).argmax(1)\n",
    "        return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/notebooks/sinica/dataset/'\n",
    "train_data = root+'facial.train'\n",
    "dev_data = root+'facial.dev'\n",
    "test_data = root+'facial.test'\n",
    "\n",
    "relation_data_old = root+'facial_r.old.train'\n",
    "# relation_data = root+'facial_r.train'\n",
    "relation_data = root+'facial_r2.train'\n",
    "schema_root = root+'schema.txt'\n",
    "dev_data = root+'facial_r2.dev'\n",
    "test_data = root+'facial_r2.test'\n",
    "\n",
    "\n",
    "UNKOWN_TAG = \"<UNKNOWN>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "REL_NONE = 'Rel-None'\n",
    "REL_PAD = 'Rel-Pad'\n",
    "rule = ('FUNC', 'ApplyTo', 'STAT')\n",
    "\n",
    "schema = schema_load(schema_root)\n",
    "ENT_TAG = define_entity(schema)\n",
    "REL_TAG = define_relation(schema)\n",
    "ent_tag_to_ix = tag2ix(ENT_TAG)\n",
    "'''{'<PAD>': 1,\n",
    " '<UNKNOWN>': 0,\n",
    " 'B-FUNC': 2,\n",
    " 'B-STAT': 3,\n",
    " 'I-FUNC': 4,\n",
    " 'I-STAT': 5,\n",
    " 'O': 6}'''\n",
    "rel_tag_to_ix = tag2ix(REL_TAG)\n",
    "'''{'ApplyTo': 2, 'Rel-None': 1, 'Rel-Pad': 0}'''\n",
    "\n",
    "# ========hyper-parameter-set==========\n",
    "\n",
    "ent_size = len(ent_tag_to_ix)\n",
    "rel_size = len(rel_tag_to_ix)\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 18\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM1 = 10\n",
    "HIDDEN_DIM2 = 8\n",
    "LABEL_EMBED_DIM = ent_size\n",
    "DENSE_OUT = 100\n",
    "\n",
    "# ATTN_IN = ent_size+LABEL_EMBED_DIM\n",
    "ATTN_IN = HIDDEN_DIM2+LABEL_EMBED_DIM\n",
    "ATTN_OUT = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    content = readfile(data)\n",
    "    word_list, ent_list, rel_list = split_to_list(content)\n",
    "    word_to_ix = word2index(word_list)\n",
    "    reserved_index = filter_len(word_list)\n",
    "    filter_word, filter_ent, filter_rel = filter_sentence(reserved_index, word_list, ent_list, rel_list)\n",
    "    f_w, f_e, f_r = deep_copy_lists(filter_word, filter_ent, filter_rel)\n",
    "    input_padded, ent_padded, rel_padded = pad_all(f_w, f_e, f_r)\n",
    "    #================================================\n",
    "    input_var = prepare_all(input_padded, word_to_ix)\n",
    "    ent_var = prepare_all(ent_padded, ent_tag_to_ix)\n",
    "    rel_var = prepare_rel(rel_padded, rel_tag_to_ix)\n",
    "    #================================================\n",
    "    vocab_size = len(word_to_ix)\n",
    "    \n",
    "    reserved_index = torch.from_numpy(np.asarray(reserved_index))\n",
    "    \n",
    "    return input_var, ent_var, rel_var, vocab_size, word_to_ix, reserved_index, word_list\n",
    "\n",
    "def dev_preprocess(dev_data):\n",
    "    dev_content = readfile(dev_data)\n",
    "    word_list, ent_list, rel_list = split_to_list(dev_content)\n",
    "    reserved_index = filter_len(word_list)\n",
    "    filter_word, filter_ent, filter_rel = filter_sentence(reserved_index, word_list, ent_list, rel_list)\n",
    "    f_w, f_e, f_r = deep_copy_lists(filter_word, filter_ent, filter_rel)\n",
    "    input_padded, ent_padded, rel_padded = pad_all(f_w, f_e, f_r)\n",
    "    #================================================\n",
    "    input_var = prepare_all(input_padded, word_to_ix)\n",
    "    ent_var = prepare_all(ent_padded, ent_tag_to_ix)\n",
    "    rel_var = prepare_rel(rel_padded, rel_tag_to_ix)\n",
    "    \n",
    "    reserved_index = torch.from_numpy(np.asarray(reserved_index))\n",
    "    \n",
    "    return input_var, ent_var, rel_var, reserved_index, word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_ent_tag = dict_inverse(ent_tag_to_ix)\n",
    "ix_to_rel_tag = dict_inverse(rel_tag_to_ix)\n",
    "#===============================================\n",
    "input_var, ent_var, rel_var, vocab_size, word_to_ix, raw_index, raw_input = preprocess(relation_data)\n",
    "loader = dataload(input_var, ent_var, rel_var, raw_index)\n",
    "\n",
    "input_dev, ent_dev, rel_dev, raw_index_dev, raw_input_dev = dev_preprocess(dev_data)\n",
    "dev_loader = dataload(input_dev, ent_dev, rel_dev, raw_index_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Entity_Typing(vocab_size, ent_tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, \\\n",
    "              LABEL_EMBED_DIM, rel_tag_to_ix).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "criterion_tag = nn.NLLLoss()\n",
    "criterion_rel = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/70 [00:07<08:40,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | ent loss 0.6564 | rel loss 0.0758 | total loss 0.7322\n",
      "         | val ent loss 0.6174 | val rel loss 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 2/70 [00:15<08:41,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | ent loss 0.4045 | rel loss 0.0227 | total loss 0.4273\n",
      "         | val ent loss 0.3455 | val rel loss 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 3/70 [00:22<08:20,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | ent loss 0.3261 | rel loss 0.0137 | total loss 0.3398\n",
      "         | val ent loss 0.2872 | val rel loss 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 4/70 [00:30<08:15,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | ent loss 0.3271 | rel loss 0.0108 | total loss 0.3379\n",
      "         | val ent loss 0.2436 | val rel loss 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 5/70 [00:37<08:11,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | ent loss 0.2861 | rel loss 0.0090 | total loss 0.2952\n",
      "         | val ent loss 0.2684 | val rel loss 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▊         | 6/70 [00:44<07:56,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | ent loss 0.2235 | rel loss 0.0080 | total loss 0.2315\n",
      "         | val ent loss 0.2173 | val rel loss 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 7/70 [00:52<07:48,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | ent loss 0.1818 | rel loss 0.0067 | total loss 0.1885\n",
      "         | val ent loss 0.1673 | val rel loss 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█▏        | 8/70 [00:59<07:40,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | ent loss 0.1713 | rel loss 0.0077 | total loss 0.1789\n",
      "         | val ent loss 0.1583 | val rel loss 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 9/70 [01:06<07:33,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | ent loss 0.1324 | rel loss 0.0053 | total loss 0.1377\n",
      "         | val ent loss 0.1516 | val rel loss 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 10/70 [01:14<07:27,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | ent loss 0.1627 | rel loss 0.0052 | total loss 0.1679\n",
      "         | val ent loss 0.1239 | val rel loss 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 11/70 [01:22<07:20,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | ent loss 0.1111 | rel loss 0.0039 | total loss 0.1150\n",
      "          | val ent loss 0.1153 | val rel loss 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 12/70 [01:30<07:15,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 | ent loss 0.1225 | rel loss 0.0042 | total loss 0.1267\n",
      "          | val ent loss 0.1266 | val rel loss 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 13/70 [01:37<07:09,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 | ent loss 0.1093 | rel loss 0.0036 | total loss 0.1130\n",
      "          | val ent loss 0.1230 | val rel loss 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 14/70 [01:44<06:59,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 | ent loss 0.1103 | rel loss 0.0037 | total loss 0.1140\n",
      "          | val ent loss 0.1067 | val rel loss 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 15/70 [01:52<06:51,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 | ent loss 0.0983 | rel loss 0.0031 | total loss 0.1014\n",
      "          | val ent loss 0.0974 | val rel loss 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 16/70 [01:59<06:44,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 | ent loss 0.0778 | rel loss 0.0027 | total loss 0.0805\n",
      "          | val ent loss 0.1021 | val rel loss 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 17/70 [02:07<06:38,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | ent loss 0.0865 | rel loss 0.0032 | total loss 0.0897\n",
      "          | val ent loss 0.0928 | val rel loss 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 18/70 [02:15<06:30,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 | ent loss 0.0715 | rel loss 0.0026 | total loss 0.0741\n",
      "          | val ent loss 0.0995 | val rel loss 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 19/70 [02:23<06:25,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 | ent loss 0.0809 | rel loss 0.0031 | total loss 0.0840\n",
      "          | val ent loss 0.0797 | val rel loss 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 20/70 [02:30<06:16,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | ent loss 0.0639 | rel loss 0.0025 | total loss 0.0664\n",
      "          | val ent loss 0.0772 | val rel loss 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 21/70 [02:38<06:09,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 | ent loss 0.0621 | rel loss 0.0028 | total loss 0.0648\n",
      "          | val ent loss 0.0787 | val rel loss 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███▏      | 22/70 [02:45<06:01,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 | ent loss 0.0536 | rel loss 0.0024 | total loss 0.0560\n",
      "          | val ent loss 0.0809 | val rel loss 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 23/70 [02:53<05:53,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 | ent loss 0.0570 | rel loss 0.0024 | total loss 0.0594\n",
      "          | val ent loss 0.0597 | val rel loss 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 24/70 [03:00<05:46,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 | ent loss 0.0508 | rel loss 0.0023 | total loss 0.0531\n",
      "          | val ent loss 0.1069 | val rel loss 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 25/70 [03:08<05:39,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 | ent loss 0.0508 | rel loss 0.0027 | total loss 0.0535\n",
      "          | val ent loss 0.0724 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 26/70 [03:16<05:31,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 | ent loss 0.0485 | rel loss 0.0028 | total loss 0.0513\n",
      "          | val ent loss 0.0774 | val rel loss 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 27/70 [03:23<05:24,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 | ent loss 0.0385 | rel loss 0.0021 | total loss 0.0406\n",
      "          | val ent loss 0.0674 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 28/70 [03:31<05:17,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 | ent loss 0.0520 | rel loss 0.0024 | total loss 0.0544\n",
      "          | val ent loss 0.0557 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 29/70 [03:39<05:09,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 | ent loss 0.0333 | rel loss 0.0021 | total loss 0.0355\n",
      "          | val ent loss 0.0647 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 30/70 [03:47<05:03,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 | ent loss 0.0330 | rel loss 0.0020 | total loss 0.0350\n",
      "          | val ent loss 0.0803 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 31/70 [03:56<04:57,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 | ent loss 0.0287 | rel loss 0.0018 | total loss 0.0305\n",
      "          | val ent loss 0.0367 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 32/70 [04:03<04:49,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 | ent loss 0.0333 | rel loss 0.0021 | total loss 0.0355\n",
      "          | val ent loss 0.0511 | val rel loss 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 33/70 [04:11<04:41,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 | ent loss 0.0259 | rel loss 0.0018 | total loss 0.0277\n",
      "          | val ent loss 0.0620 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▊     | 34/70 [04:19<04:34,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 | ent loss 0.0307 | rel loss 0.0022 | total loss 0.0329\n",
      "          | val ent loss 0.0496 | val rel loss 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 35/70 [04:25<04:25,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 | ent loss 0.0233 | rel loss 0.0022 | total loss 0.0255\n",
      "          | val ent loss 0.0409 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████▏    | 36/70 [04:32<04:17,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 | ent loss 0.0173 | rel loss 0.0018 | total loss 0.0191\n",
      "          | val ent loss 0.0518 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 37/70 [04:39<04:09,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 | ent loss 0.0224 | rel loss 0.0019 | total loss 0.0244\n",
      "          | val ent loss 0.0649 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 38/70 [04:46<04:01,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 | ent loss 0.0189 | rel loss 0.0016 | total loss 0.0205\n",
      "          | val ent loss 0.0757 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 39/70 [04:53<03:53,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 | ent loss 0.0192 | rel loss 0.0016 | total loss 0.0208\n",
      "          | val ent loss 0.0299 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 40/70 [05:00<03:45,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 | ent loss 0.0134 | rel loss 0.0018 | total loss 0.0152\n",
      "          | val ent loss 0.0423 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 41/70 [05:08<03:37,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 | ent loss 0.0165 | rel loss 0.0019 | total loss 0.0184\n",
      "          | val ent loss 0.0486 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 42/70 [05:15<03:30,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 | ent loss 0.0096 | rel loss 0.0019 | total loss 0.0116\n",
      "          | val ent loss 0.0427 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████▏   | 43/70 [05:22<03:22,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 | ent loss 0.0101 | rel loss 0.0017 | total loss 0.0119\n",
      "          | val ent loss 0.0241 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 44/70 [05:30<03:15,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 | ent loss 0.0113 | rel loss 0.0013 | total loss 0.0126\n",
      "          | val ent loss 0.0371 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 45/70 [05:37<03:07,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 | ent loss 0.0127 | rel loss 0.0018 | total loss 0.0145\n",
      "          | val ent loss 0.0736 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 46/70 [05:44<02:59,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 | ent loss 0.0105 | rel loss 0.0017 | total loss 0.0122\n",
      "          | val ent loss 0.0658 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 47/70 [05:51<02:51,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 | ent loss 0.0193 | rel loss 0.0014 | total loss 0.0206\n",
      "          | val ent loss 0.0163 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 48/70 [05:58<02:44,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 | ent loss 0.0255 | rel loss 0.0018 | total loss 0.0273\n",
      "          | val ent loss 0.0653 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 49/70 [06:05<02:36,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 | ent loss 0.0161 | rel loss 0.0015 | total loss 0.0175\n",
      "          | val ent loss 0.0552 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 50/70 [06:13<02:29,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 | ent loss 0.0110 | rel loss 0.0015 | total loss 0.0125\n",
      "          | val ent loss 0.0457 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 51/70 [06:21<02:21,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 | ent loss 0.0095 | rel loss 0.0017 | total loss 0.0112\n",
      "          | val ent loss 0.0680 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 52/70 [06:28<02:14,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 | ent loss 0.0077 | rel loss 0.0012 | total loss 0.0090\n",
      "          | val ent loss 0.0451 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 53/70 [06:36<02:07,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 | ent loss 0.0077 | rel loss 0.0015 | total loss 0.0092\n",
      "          | val ent loss 0.0388 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 54/70 [06:43<01:59,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 | ent loss 0.0071 | rel loss 0.0013 | total loss 0.0084\n",
      "          | val ent loss 0.0678 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 55/70 [06:48<01:51,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54 | ent loss 0.0080 | rel loss 0.0015 | total loss 0.0095\n",
      "          | val ent loss 0.0714 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 56/70 [06:55<01:43,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55 | ent loss 0.0151 | rel loss 0.0015 | total loss 0.0166\n",
      "          | val ent loss 0.0576 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 57/70 [07:03<01:36,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 56 | ent loss 0.0113 | rel loss 0.0014 | total loss 0.0127\n",
      "          | val ent loss 0.0449 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 58/70 [07:11<01:29,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57 | ent loss 0.0050 | rel loss 0.0012 | total loss 0.0062\n",
      "          | val ent loss 0.0500 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 59/70 [07:19<01:21,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58 | ent loss 0.0060 | rel loss 0.0011 | total loss 0.0070\n",
      "          | val ent loss 0.0586 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 60/70 [07:25<01:14,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 59 | ent loss 0.0051 | rel loss 0.0014 | total loss 0.0064\n",
      "          | val ent loss 0.0269 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 61/70 [07:32<01:06,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 | ent loss 0.0042 | rel loss 0.0011 | total loss 0.0053\n",
      "          | val ent loss 0.0620 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 62/70 [07:40<00:59,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61 | ent loss 0.0091 | rel loss 0.0012 | total loss 0.0104\n",
      "          | val ent loss 0.0503 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 63/70 [07:47<00:51,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 62 | ent loss 0.0049 | rel loss 0.0013 | total loss 0.0063\n",
      "          | val ent loss 0.0277 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████▏| 64/70 [07:55<00:44,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 63 | ent loss 0.0127 | rel loss 0.0015 | total loss 0.0142\n",
      "          | val ent loss 0.0394 | val rel loss 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 65/70 [08:03<00:37,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 64 | ent loss 0.0063 | rel loss 0.0012 | total loss 0.0074\n",
      "          | val ent loss 0.0627 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 66/70 [08:10<00:29,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65 | ent loss 0.0055 | rel loss 0.0011 | total loss 0.0066\n",
      "          | val ent loss 0.0707 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 67/70 [08:17<00:22,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 66 | ent loss 0.0050 | rel loss 0.0011 | total loss 0.0060\n",
      "          | val ent loss 0.0780 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 68/70 [08:25<00:14,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 67 | ent loss 0.0063 | rel loss 0.0011 | total loss 0.0074\n",
      "          | val ent loss 0.0509 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▊| 69/70 [08:33<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68 | ent loss 0.0042 | rel loss 0.0012 | total loss 0.0055\n",
      "          | val ent loss 0.0598 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 70/70 [08:40<00:00,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69 | ent loss 0.0057 | rel loss 0.0010 | total loss 0.0067\n",
      "          | val ent loss 0.0388 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 70\n",
    "print_every = 12\n",
    "all_ent_loss = []\n",
    "all_rel_loss = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "loss = 0\n",
    "ent_loss = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(range(n_iters)):  \n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        batchsize = batch_x.size(0)\n",
    "        \n",
    "        batch_ent = batch_ent.view(batchsize*MAX_LEN)\n",
    "        batch_rel = batch_rel.view(batchsize*MAX_LEN*MAX_LEN)\n",
    "        \n",
    "        loss_ent = criterion_tag(ent_output, batch_ent.cuda() if USE_CUDA else batch_ent)\n",
    "        loss_rel = criterion_rel(rel_output, batch_rel.cuda() if USE_CUDA else batch_rel)\n",
    "        loss = loss_ent+loss_rel\n",
    "        \n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % print_every == 1:\n",
    "            all_ent_loss.append(loss_ent.cpu())\n",
    "            all_rel_loss.append(loss_rel.cpu())\n",
    "        #    print('%.4f| epoch: %d| step: %d| %s' % (loss, epoch, step, timeSince(start)))\n",
    "        \n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(dev_loader):\n",
    "        model.eval()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        batchsize = batch_x.size(0)\n",
    "        \n",
    "        val_loss_ent = criterion_tag(ent_output.cpu(), batch_ent.view(batchsize*MAX_LEN)) \n",
    "        val_loss_rel = criterion_rel(rel_output.cpu(), batch_rel.view(batchsize*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    \n",
    "    print(\"epoch: %d | ent loss %.4f | rel loss %.4f | total loss %.4f\" \\\n",
    "          % (epoch, loss_ent, loss_rel, loss))\n",
    "    print(\"      %s  | val ent loss %.4f | val rel loss %.4f\"\n",
    "          % (\" \"*len(str(epoch)), val_loss_ent, val_loss_rel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_ent_loss[:100], label='ent_loss')\n",
    "plt.plot(all_rel_loss[:100], label='rel_loss')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_choose(input_var):\n",
    "    r_choose = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        r_choose.append(random.randint(0,len(input_var)))\n",
    "    return r_choose\n",
    "        \n",
    "def ent_argmax(output, batchsize):\n",
    "    output = output.view(batchsize,MAX_LEN,ent_size).argmax(2)\n",
    "    return output\n",
    "\n",
    "def rel_argmax(output, batchsize):\n",
    "    output = output.view(batchsize,MAX_LEN,MAX_LEN,rel_size).argmax(3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "true : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "===================================================\n",
      "\n",
      "\n",
      "predict : ['O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "true : ['O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Entity loss : 0.0052\n",
      "Relation loss : 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    r_choose = random_choose(input_var)\n",
    "    model.eval()\n",
    "    ent_output, rel_output = model(input_var[r_choose].cuda() if USE_CUDA else input_var)\n",
    "    \n",
    "    batchsize = input_var[r_choose].size(0)\n",
    "    \n",
    "    ent_loss = criterion_tag(ent_output.cpu(), ent_var[r_choose].view(BATCH_SIZE*MAX_LEN))\n",
    "    ent_output = ent_argmax(ent_output, batchsize)\n",
    "    \n",
    "    rel_loss = criterion_rel(rel_output.cpu(), rel_var[r_choose].view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    \n",
    "#     print('predict :', ent_output[0])\n",
    "#     print('true :', ent_var[r_choose[0]])\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[0], ix_to_ent_tag))\n",
    "    print('true :', index2tag(ent_var[r_choose[0]], ix_to_ent_tag))\n",
    "    print()\n",
    "    print('===================================================')\n",
    "    print()\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[1], ix_to_ent_tag))\n",
    "    print('true :', index2tag(ent_var[r_choose[1]], ix_to_ent_tag))\n",
    "    \n",
    "    print()\n",
    "    print(\"Entity loss : %.4f\" % ent_loss)\n",
    "    print(\"Relation loss : %.4f\" % rel_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "\n",
      "predict : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "true : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Entity loss : 0.0691\n",
      "Relation loss : 0.0059\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    r_choose = random_choose(input_dev)\n",
    "\n",
    "    model.eval()\n",
    "    ent_output, rel_output = model(input_dev[r_choose].cuda() if USE_CUDA else input_dev)\n",
    "    \n",
    "    batchsize = input_dev[r_choose].size(0)\n",
    "    \n",
    "    ent_loss = criterion_tag(ent_output.cpu(), ent_dev[r_choose].view(batchsize*MAX_LEN))\n",
    "    ent_output = ent_argmax(ent_output, batchsize)\n",
    "    \n",
    "    rel_loss = criterion_rel(rel_output.cpu(), rel_var[r_choose].view(batchsize*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    print(r_choose[0])\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[0], ix_to_ent_tag))\n",
    "    print()\n",
    "    print('true :', index2tag(ent_dev[r_choose[0]], ix_to_ent_tag))\n",
    "    print()\n",
    "\n",
    "    print(\"Entity loss : %.4f\" % ent_loss)\n",
    "    print(\"Relation loss : %.4f\" % rel_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# *****測多關係\n",
    "# 每個epoch print recall validation \n",
    "# 處理單句\n",
    "# 之後可做，自動標rel\n",
    "\n",
    "def batch_decode(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel):\n",
    "    rel_lists = []\n",
    "    true_rel_lists = []\n",
    "    combine_outputs = []\n",
    "    combine_trues = []\n",
    "    \n",
    "    #================\n",
    "    \n",
    "    \n",
    "    for e,r,i,te,tr in zip(ent_output, rel_output, batch_index, true_ent, true_rel):\n",
    "        \n",
    "        # 單句decode關係\n",
    "        true_r_list, appear_error = decode_rel(te, tr)   # true\n",
    "        r_list, appear_error = decode_rel(e, r)          # predict\n",
    "        \n",
    "        # 出現error，跳過這句\n",
    "        if appear_error:\n",
    "            continue\n",
    "            \n",
    "        true_rel_lists.append(true_r_list)\n",
    "        rel_lists.append(r_list)\n",
    "        \n",
    "        # 算句子長度\n",
    "        len_of_list = len(word_lists[i])\n",
    "        word_list = word_lists[i]\n",
    "        \n",
    "        true_ent = index2tag(te, ix_to_ent_tag)\n",
    "        pridict_ent = index2tag(e, ix_to_ent_tag)\n",
    "        \n",
    "        print(word_list)\n",
    "        print(true_ent[:len_of_list])\n",
    "        print(true_r_list[:len_of_list])\n",
    "\n",
    "        print()\n",
    "        print('Predict output')\n",
    "        print(pridict_ent[:len_of_list])\n",
    "        print(r_list[:len_of_list])\n",
    "        print()\n",
    "        print(\"=====================================\")\n",
    "        \n",
    "        # 重新組成跟input data同樣的形式\n",
    "        for word in range(len_of_list):\n",
    "            # ========predict======================\n",
    "            add_r = ''\n",
    "            if type(r_list[word]) is list: \n",
    "                for single_r in r_list[word]:\n",
    "                    add_r = add_r+single_r+' '\n",
    "            combine_outputs.append(word_list[word]+' '+pridict_ent[word]+' '+add_r)\n",
    "            \n",
    "            # ==================true===================\n",
    "            add_r = ''\n",
    "            if type(true_r_list[word]) is list:     \n",
    "                for single_r in true_r_list[word]:\n",
    "                    try :\n",
    "                        add_r = add_r+single_r+' '\n",
    "                    except TypeError:\n",
    "                        add_r = add_r+' '\n",
    "                    \n",
    "            combine_trues.append(word_list[word]+' '+true_ent[word]+' '+add_r)\n",
    "                  \n",
    "            \n",
    "        combine_outputs.append(\" \")\n",
    "        combine_trues.append(\" \")\n",
    "      \n",
    "    \n",
    "    # 過濾 O tag的 index\n",
    "    f_ix = filter_otag(combine_trues)\n",
    "    \n",
    "    filter_true = list(combine_trues[i] for i in f_ix)\n",
    "    filter_output = list(combine_outputs[i] for i in f_ix)\n",
    "\n",
    "#     print(np.array(filter_output))\n",
    "#     print(np.array(filter_true))\n",
    "    print()\n",
    "    print(\"%s \\t %s \\t %s \\t\" % ('precision ', 'recall ', 'fbeta_score '))\n",
    "    print(precision_recall_fscore_support(filter_true, filter_output, average='micro', \\\n",
    "                                          labels=np.unique(filter_output)))        \n",
    "    \n",
    "    return filter_true, filter_output\n",
    "    \n",
    "\n",
    "    \n",
    "def create_rel_info(ent_output):\n",
    "    r_list = []     # 存放完整關係\n",
    "    r_dict = {}     # 記錄關係資訊\n",
    "    appear_error = False\n",
    "    pred_ent = index2tag(ent_output, ix_to_ent_tag)\n",
    "\n",
    "    e_loc = 0       # 當前遇到的entity的位置\n",
    "    for loc, e in enumerate(pred_ent):\n",
    "        if e[0]=='B':\n",
    "            e_loc = loc\n",
    "            r_dict[loc] = {\n",
    "                '_2ndtag':e[2:],\n",
    "                'end':loc,\n",
    "            }\n",
    "            r_list.append([])\n",
    "                   \n",
    "        elif e[0]=='I':\n",
    "            \n",
    "            # 錯誤來自於，entity預測錯誤，沒有預測到B tag，直接跳到I tag\n",
    "            # 所以沒有紀錄e_loc\n",
    "            try: \n",
    "                r_dict[e_loc]['end'] = loc\n",
    "            except KeyError:\n",
    "                appear_error = True\n",
    "                break\n",
    "                \n",
    "            r_list.append([])\n",
    "            \n",
    "        else:\n",
    "            r_list.append(\"\")\n",
    "              \n",
    "    return r_list, r_dict, appear_error\n",
    "\n",
    "\n",
    "\n",
    "def decode_rel(ent_output, rel_output):\n",
    "\n",
    "    r_list, r_dict, appear_error = create_rel_info(ent_output)\n",
    "    \n",
    "    \n",
    "    IsB = False           # 是否遇到B tag的lock\n",
    "    IsNext = False        # 是否為B tag後面的tag 的lock\n",
    "    num_reocrd = -1       # 紀錄pair數\n",
    "    now_loc = 0\n",
    "    end_loc = 0\n",
    "    rel_keyerror = False\n",
    "    \n",
    "    for now in range(len(rel_output)):\n",
    "        for loc, rel in enumerate(rel_output[now][:now+1]):\n",
    "            rel = rel.cpu().numpy()\n",
    "            \n",
    "            # 有關係存在，且為B tag \n",
    "            if rel!=rel_tag_to_ix[REL_NONE] and IsB==False and IsNext==False:\n",
    "\n",
    "                IsB = True\n",
    "                IsNext = True\n",
    "                tag = ix_to_rel_tag[int(rel)]\n",
    "                num_reocrd+=1\n",
    "                now_loc = loc\n",
    "                \n",
    "                # 錯誤來自於，now_loc找不到，也就是說，rel預測出來是有關係存在\n",
    "                # 但預測是'O'\n",
    "                # 而在entity中卻沒有預測出來，所以r_dict中沒有紀錄\n",
    "                try:\n",
    "                    end_loc = r_dict[now_loc]['end']\n",
    "                except KeyError:\n",
    "                    rel_keyerror = True\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                second_tag = r_dict[now_loc]['_2ndtag']\n",
    "                # check_loc 之後有多關係出現時要更改\n",
    "                preAorB = check_loc(second_tag)\n",
    "                nowAorB = 'B' if preAorB=='A' else 'A'\n",
    "                \n",
    "                pre_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+preAorB\n",
    "                now_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+nowAorB\n",
    "                \n",
    "                # 將以前的token填上關係\n",
    "                for token in range(now_loc, end_loc+1):\n",
    "                    \n",
    "                    '''AttributeError: 'str' object has no attribute 'append'''\n",
    "                    r_list[token].append(pre_complete_rel)\n",
    "                \n",
    "                # 當前token填上關係\n",
    "                r_list[now].append(now_complete_rel)\n",
    "\n",
    "            \n",
    "            # B tag後面的tag\n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsB:\n",
    "                # 如果還在這個entity的範圍內\n",
    "                if loc<=end_loc:\n",
    "                    pass\n",
    "                \n",
    "                # 超出現在這個entity的範圍，改lock\n",
    "                else:\n",
    "                    IsB = False\n",
    "            \n",
    "            # B tag後面的tag的關係，依照前面的關係複製\n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsNext:\n",
    "                r_list[now] = r_list[now-1]\n",
    "                \n",
    "            else:\n",
    "                IsB = False\n",
    "        \n",
    "        if rel_keyerror:\n",
    "            rel_keyerror = False\n",
    "            break\n",
    "                \n",
    "                \n",
    "    return r_list, appear_error\n",
    "                \n",
    "\n",
    "# 是三元關係中的前者還是後者                \n",
    "# 有多種關係資料後這段要改\n",
    "def check_loc(second_tag):\n",
    "    if second_tag in rule[0]:\n",
    "        return 'A'\n",
    "    elif second_tag in rule[2]:\n",
    "        return 'B'\n",
    "    \n",
    "    \n",
    "def filter_otag(combine_trues):\n",
    "    ix2eval = []\n",
    "    \n",
    "    for i, token in enumerate(combine_trues):\n",
    "        # error : string index out of range\n",
    "        # 出現空白\n",
    "        try :\n",
    "            token[2]!='O'\n",
    "        except IndexError:\n",
    "            continue\n",
    "        else:\n",
    "            if token[2]!='O':\n",
    "                ix2eval.append(i)\n",
    "            \n",
    "    return ix2eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is version 2\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# *****測多關係\n",
    "# 處裡奇怪的BUG\n",
    "# 每個epoch print recall validation \n",
    "# 之後可做，自動標rel\n",
    "\n",
    "def batch_decode(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel):\n",
    "    rel_lists = []\n",
    "    true_rel_lists = []\n",
    "    combine_outputs = []\n",
    "    combine_trues = []\n",
    "    \n",
    "    #================\n",
    "    \n",
    "    \n",
    "    for e,r,i,te,tr in zip(ent_output, rel_output, batch_index, true_ent, true_rel):\n",
    "        \n",
    "        # 單句decode關係\n",
    "        true_r_list, appear_error = decode_rel(te, tr)   # true\n",
    "        r_list, appear_error = decode_rel(e, r)          # predict\n",
    "        \n",
    "        true_r_list = [list(set(i)) if type(i) is list else i for i in true_r_list]\n",
    "        r_list = [list(set(i)) if type(i) is list else i for i in r_list]\n",
    "        \n",
    "        # 出現error，跳過這句\n",
    "        if appear_error:\n",
    "            continue\n",
    "            \n",
    "        true_rel_lists.append(true_r_list)\n",
    "        rel_lists.append(r_list)\n",
    "        \n",
    "        # 算句子長度\n",
    "        len_of_list = len(word_lists[i])\n",
    "        word_list = word_lists[i]\n",
    "        \n",
    "        true_ent = index2tag(te, ix_to_ent_tag)\n",
    "        pridict_ent = index2tag(e, ix_to_ent_tag)\n",
    "        \n",
    "        print(word_list)\n",
    "        print(true_ent[:len_of_list])\n",
    "        print(true_r_list[:len_of_list])\n",
    "\n",
    "        print()\n",
    "        print('Predict output')\n",
    "        print(pridict_ent[:len_of_list])\n",
    "        print(r_list[:len_of_list])\n",
    "        print()\n",
    "        print(\"=====================================\")\n",
    "        \n",
    "        # 重新組成跟input data同樣的形式\n",
    "        for word in range(len_of_list):\n",
    "            # ========predict======================\n",
    "            add_r = ''\n",
    "            if type(r_list[word]) is list: \n",
    "                for single_r in r_list[word]:\n",
    "                    add_r = add_r+single_r+' '\n",
    "            combine_outputs.append(word_list[word]+' '+pridict_ent[word]+' '+add_r)\n",
    "            \n",
    "            # ==================true===================\n",
    "            add_r = ''\n",
    "            if type(true_r_list[word]) is list:     \n",
    "                for single_r in true_r_list[word]:\n",
    "                    try :\n",
    "                        add_r = add_r+single_r+' '\n",
    "                    except TypeError:\n",
    "                        add_r = add_r+' '\n",
    "                    \n",
    "            combine_trues.append(word_list[word]+' '+true_ent[word]+' '+add_r)\n",
    "                  \n",
    "            \n",
    "        combine_outputs.append(\" \")\n",
    "        combine_trues.append(\" \")\n",
    "      \n",
    "    \n",
    "    # 過濾 O tag的 index\n",
    "    f_ix = filter_otag(combine_trues)\n",
    "    \n",
    "    filter_true = list(combine_trues[i] for i in f_ix)\n",
    "    filter_output = list(combine_outputs[i] for i in f_ix)\n",
    "\n",
    "#     print(np.array(filter_output))\n",
    "#     print(np.array(filter_true))\n",
    "    print()\n",
    "    print(\"%s \\t %s \\t %s \\t\" % ('precision ', 'recall ', 'fbeta_score '))\n",
    "    print(precision_recall_fscore_support(filter_true, filter_output, average='micro', \\\n",
    "                                          labels=np.unique(filter_output)))        \n",
    "    \n",
    "    return filter_true, filter_output\n",
    "    \n",
    "\n",
    "    \n",
    "def create_rel_info(ent_output):\n",
    "    r_list = []     # 存放完整關係\n",
    "    r_dict = {}     # 記錄關係資訊\n",
    "    appear_error = False\n",
    "\n",
    "    pred_ent = index2tag(ent_output, ix_to_ent_tag)\n",
    "    \n",
    "\n",
    "    e_loc = 0       # 當前遇到的entity的位置\n",
    "    for loc, e in enumerate(pred_ent):\n",
    "        if e[0]=='B':\n",
    "            e_loc = loc\n",
    "            r_dict[loc] = {\n",
    "                '_2ndtag':e[2:],\n",
    "                'end':loc,\n",
    "            }\n",
    "            r_list.append([])\n",
    "                   \n",
    "        elif e[0]=='I':\n",
    "            # 錯誤來自於，entity預測錯誤，沒有預測到B tag，直接跳到I tag\n",
    "            # 所以沒有紀錄e_loc\n",
    "            try: \n",
    "                r_dict[e_loc]['end'] = loc\n",
    "            except KeyError:\n",
    "                appear_error = True\n",
    "                break\n",
    "            \n",
    "            r_list.append([])\n",
    "            \n",
    "        else:\n",
    "            r_list.append(\"\")\n",
    "              \n",
    "    return r_list, r_dict, appear_error\n",
    "\n",
    "\n",
    "\n",
    "def decode_rel(ent_output, rel_output):\n",
    "\n",
    "    r_list, r_dict, appear_error = create_rel_info(ent_output) \n",
    "    \n",
    "    '''{9: {'end': 10, '_2ndtag': 'FUNC'}, 11: {'end': 14, '_2ndtag': 'STAT'}, 20: {'end': 21, '_2ndtag': 'FUNC'}}\n",
    "['', '', '', '', '', '', '', '', '', [], [], [], [], [], [], '', '', '', '', '', [], [], '', ...'''\n",
    "    \n",
    "    IsB = False           # 是否遇到B tag的lock\n",
    "    IsNext = False        # 是否為B tag後面的tag 的lock\n",
    "    num_reocrd = -1       # 紀錄pair數\n",
    "    now_loc = 0\n",
    "    pre_rel_end_loc = 0\n",
    "    now_rel_end_loc = 0\n",
    "    \n",
    "    rel_keyerror = False\n",
    "    \n",
    "    for now in range(len(rel_output)):\n",
    "        for loc, rel in enumerate(rel_output[now][:now+1]):\n",
    "            rel = rel.cpu().numpy()\n",
    "            \n",
    "#             print(rel, IsB, IsNext)\n",
    "            \n",
    "            # 有關係存在，且為B tag \n",
    "            if rel!=rel_tag_to_ix[REL_NONE] and IsB==False and IsNext==False:\n",
    "\n",
    "                IsB = True\n",
    "                IsNext = True\n",
    "                tag = ix_to_rel_tag[int(rel)]\n",
    "                num_reocrd+=1\n",
    "                now_loc = loc\n",
    "                \n",
    "                # 錯誤來自於，now_loc找不到，也就是說，rel預測出來是有關係存在\n",
    "                # 但預測是'O'\n",
    "                # 而在entity中卻沒有預測出來，所以r_dict中沒有紀錄\n",
    "                try:\n",
    "                    pre_rel_end_loc = r_dict[now_loc]['end']\n",
    "\n",
    "                except KeyError:\n",
    "                    rel_keyerror = True\n",
    "                    break\n",
    "                    \n",
    "                \n",
    "                try:\n",
    "                    now_rel_end_loc = r_dict[now]['end']\n",
    "                    \n",
    "                except KeyError:\n",
    "                    rel_keyerror = True\n",
    "                    break\n",
    "            \n",
    "             \n",
    "                second_tag = r_dict[now_loc]['_2ndtag']\n",
    "                # check_loc 之後有多關係出現時要更改\n",
    "                preAorB = check_loc(second_tag)\n",
    "                nowAorB = 'B' if preAorB=='A' else 'A'\n",
    "                \n",
    "                pre_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+preAorB\n",
    "                now_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+nowAorB\n",
    "                \n",
    "                # 將以前的token填上關係\n",
    "                for token in range(now_loc, pre_rel_end_loc+1):\n",
    "                    \n",
    "                    '''AttributeError: 'str' object has no attribute 'append'''\n",
    "                    r_list[token].append(pre_complete_rel)\n",
    "                \n",
    "                # 當前token填上關係\n",
    "                r_list[now].append(now_complete_rel)\n",
    "                \n",
    "\n",
    "            \n",
    "            # 關係前位中B tag後面的tag\n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsB:\n",
    "                # 如果還在這個entity的範圍內\n",
    "                if loc<=pre_rel_end_loc:\n",
    "#                     print('loc', loc)\n",
    "#                     print('pre_rel_end_loc', pre_rel_end_loc)\n",
    "                    pass\n",
    "                \n",
    "                # 超出現在這個entity的範圍，改lock\n",
    "                else:\n",
    "#                     print('loc', loc)\n",
    "#                     print('pre_rel_end_loc', pre_rel_end_loc)\n",
    "                    IsB = False\n",
    "\n",
    "            \n",
    "            # B tag後面的tag的關係，依照前面的關係複製\n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsNext:\n",
    "                r_list[now] = r_list[now-1]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                if now<=now_rel_end_loc:\n",
    "                    IsB = False\n",
    "                else:\n",
    "                    IsB = False\n",
    "                    IsNext = False\n",
    "\n",
    "        \n",
    "        if rel_keyerror:\n",
    "            rel_keyerror = False\n",
    "            break\n",
    "                \n",
    "                \n",
    "    return r_list, appear_error\n",
    "                \n",
    "\n",
    "# 是三元關係中的前者還是後者                \n",
    "# 有多種關係資料後這段要改\n",
    "def check_loc(second_tag):\n",
    "    if second_tag in rule[0]:\n",
    "        return 'A'\n",
    "    elif second_tag in rule[2]:\n",
    "        return 'B'\n",
    "    \n",
    "    \n",
    "def filter_otag(combine_trues):\n",
    "    ix2eval = []\n",
    "    \n",
    "    for i, token in enumerate(combine_trues):\n",
    "        # error : string index out of range\n",
    "        # 出現空白\n",
    "        try :\n",
    "            token[2]!='O'\n",
    "        except IndexError:\n",
    "            continue\n",
    "        else:\n",
    "            if token[2]!='O':\n",
    "                ix2eval.append(i)\n",
    "            \n",
    "    return ix2eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity loss : 0.0451\n",
      "Relation loss : 0.0012\n",
      "\n",
      "['當', '天', '擦', '就', '馬', '上', '覺', '得', '皮', '膚', '乾', '燥', '感', '得', '到', '舒', '緩']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['增', '加', '肌', '膚', '彈', '性', '及', '含', '水', '量', '，', '提', '高', '保', '濕', '效', '果', '，', '防', '止', '肌', '膚', '乾', '燥', '，', '去', '除', '細', '小', '皺', '紋', '，']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['而', '面', '膜', '的', '精', '華', '液', '不', '算', '多', '，', '但', '也', '很', '夠', '用', '了', '，', '是', '敷', '上', '之', '後', '不', '會', '滴', '下', '的', '量', '，', '剛', '剛', '好', '，', '精', '華', '液', '本', '身', '沒', '什', '麼', '香', '味', '，', '我', '這', '個', '小', '敏', '感', '的', '油', '性', '肌', '老', '實', '說', '敷', '完', '之', '後', '卻', '什', '麼', '感', '覺', '都', '沒', '有', '，', '沒', '有', '過', '敏', '、', '沒', '有', '感', '覺', '特', '別', '保', '水', '、', '緊', '緻', '.', '.', '.', '等']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['這', '次', '試', '用', '的', '是', '\"', '南', '極', '冰', '河', '醣', '蛋', '白', '滲', '透', '保', '濕', '面', '膜', '\"', '，', '對', '於', '乾', '肌', '的', '我', '來', '說', '，', '再', '適', '合', '不', '過', '了']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['這', '款', '面', '膜', '它', '強', '調', '乾', '肌', '及', '肌', '膚', '鬆', '弛', '.', '.', '.', '等', ',', '但', '我', '發', '現', '緊', '緻', '的', '部', '份', '不', '是', '那', '麼', '明', '顯', ',', '但', '保', '濕', '部', '分', '我', '倒', '是', '很']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A'], '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['使', '用', '了', '幾', '次', '，', '效', '果', '都', '不', '太', '令', '人', '滿', '意', '，', '也', '許', '是', '這', '款', '面', '膜', '主', '要', '強', '調', '的', '是', '緊', '緻', '功', '能', '，', '我', '的', '超', '乾', '肌', '得', '先', '把', '缺', '水', '問', '題', '解', '決', '了', '才', '能', '看', '出', '效', '果', '吧', '∼']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['高', '效', '保', '濕', '，', '舒', '緩', '肌', '膚', '，', '溫', '和', '修', '復', '，', '重', '塑', '晶', '透', '的', '的', '亮', '顏', '全', '效', '保', '濕', '，', '改', '善', '乾', '性', '肌', '膚', '問', '題', '，', '使', '肌', '膚', '亮', '透', '有', '光', '澤']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['可', '視', '膚', '況', '/', '氣', '候', '選', '擇', '厚', '敷', 'o', 'r', '薄', '敷', '，', '不', '論', '是', '救', '急', '保', '養', '或', '是', '乾', '肌', '日', '常', '補', '水', '，', '保', '濕', '效', '果', '皆', '相', '當', '顯', '著', '，', '簡', '直', '宜', '家', '宜', '室', '四', '季', '皆', '宜', '齁', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['極', '效', '肌', '因', '修', '護', '巨', '藻', '、', '齒', '缘', '墨', '角', '藻', '複', '合', '水', '解', '酵', '母', '提', '取', '物', '，', '浸', '透', '濕', '潤', '乾', '燥', '肌', '膚', '，', '增', '強', '防', '護', '屏', '障', '，', '協', '同', '蘆', '薈', '、', '山', '金', '車', '及', '光', '果', '甘', '草', '植', '萃', '精', '華', '，', '深', '度', '安', '撫', '同', '時', '舒', '緩', '肌', '膚', '的', '不', '適', '，', '拋', '開', '惱', '人', '的', '乾', '燥', '缺', '水', '不', '安', '讓', '肌', '膚', '穩', '定']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['“', '酵', '母', '萃', '取', '”', '搭', '配', '“', '小', '分', '子', '玻', '尿', '酸', '”', '及', '“', '牛', '奶', '蛋', '白', '”', '使', '乾', '燥', '肌', '膚', '充', '滿', '水', '分', '，', '長', '效', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['洗', '完', '臉', '後', '，', '我', '將', '「', '我', '的', '美', '麗', '日', '記', '」', '南', '極', '冰', '河', '醣', '蛋', '白', '滲', '透', '保', '濕', '面', '膜', '敷', '在', '臉', '上', '，', '經', '過', '2', '0', '分', '鐘', '後', '，', '將', '面', '膜', '從', '臉', '部', '由', '下', '往', '上', '撕', '下', '，', '再', '用', '清', '水', '將', '面', '膜', '清', '洗', '乾', '淨', '後', '，', '從', '鏡', '子', '中', '我', '看', '到', '臉', '部', '肌', '膚', '乾', '燥', '缺', '水', '的', '現', '象', '消', '失', '了']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['皮', '珂', '兒', '是', '韓', '國', '製', '造', '的', '保', '養', '品', '，', '有', '可', '愛', '的', '火', '山', '插', '圖', '，', '鐵', '罐', '包', '裝', '，', '是', '很', '特', '殊', '的', '慕', '絲', '質', '地', '，', '需', '沖', '洗', '的', '清', '潔', '面', '膜', '，', '很', '適', '合', '油', '性', '肌', '膚', '來', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '']\n",
      "\n",
      "=====================================\n",
      "['補', '充', '說', '明', ',', '因', '為', '我', '本', '身', '肌', '膚', '屬', '於', '敏', '感', '性', '肌', '膚', ',', '用', '此', '款', '面', '膜', '不', '會', '過', '敏', '喔', '~', '~', '而', '且', '保', '濕', '效', '果', '很', '強', '喔', '~', '^', '^']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 't', 'e', 'p', '2', '.', '強', '化', '補', '水', '-', '嫩', '白', '蠶', '絲', '面', '膜', '-', '面', '膜', '非', '常', '薄', '貼', '，', '記', '得', '要', '把', '蠶', '絲', '面', '膜', '朝', '臉', '敷', '上', '後', '再', '把', '珍', '珠', '膜', '撕', '下', '布', '料', '採', '用', '凝', '水', '蠶', '絲', '，', '清', '透', '，', '用', '天', '然', '草', '本', '植', '物', '萃', '取', '的', '精', '華', '，', '幫', '助', '保', '濕', '，', '也', '可', '以', '舒', '緩', '一', '些', '夏', '季', '保', '養', '時', '乾', '燥', '肌', '的', '狀', '況']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', [], [], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', [], [], '', '', '', '', [], [], '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['溫', '和', '的', '成', '份', '讓', '敏', '感', '的', '肌', '膚', '也', '能', '輕', '鬆', '的', '美', '白']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['“', '酵', '母', '萃', '取', '”', '搭', '配', '“', '小', '分', '子', '玻', '尿', '酸', '”', '及', '“', '牛', '奶', '蛋', '白', '”', '使', '乾', '燥', '肌', '膚', '充', '滿', '水', '分', '，', '長', '效', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['不', '過', '乾', '性', '肌', '膚', '敷', '完', '後', '可', '能', '要', '馬', '上', '擦', '點', '保', '濕', '的', '保', '養', '品', '不', '然', '皮', '膚', '會', '緊', '緊', '繃', '繃', '的', '喔']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', [], [], '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['但', '對', '皮', '膚', '乾', '癢', '保', '濕', '及', '透', '亮', '很', '有', '幫', '助', '的', '喲']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "\n",
      "precision  \t recall  \t fbeta_score  \t\n",
      "(0.5050505050505051, 0.5154639175257731, 0.510204081632653, None)\n",
      "\n",
      "==========================\n",
      "==========================\n",
      "\n",
      "\n",
      "Entity loss : 0.0663\n",
      "Relation loss : 0.0012\n",
      "\n",
      "['我', '的', '皮', '膚', '是', '屬', '於', '油', '性', '敏', '感', '肌', '，', '用', '太', '刺', '激', '的', '保', '養', '品', '會', '有', '側', '痛', '和', '肌', '膚', '發', '紅', '的', '感', '覺']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['看', '品', '名', '就', '知', '道', '是', '首', '重', '保', '濕', '效', '果', '的', '面', '膜', ',', '對', '於', '我', '這', '缺', '水', '易', '出', '油', '的', '肌', '膚', ',', '當', '然', '是', '迫', '不', '及', '待', '地', '使', '用', '啦', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['若', '乾', '性', '肌', '膚', '使', '用', '會', '保', '濕', '度', '不', '足']\n",
      "['O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', [], [], [], [], '', '', '', [], [], [], '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以', '上', '是', '我', '不', '會', '再', '使', '用', '此', '產', '品', '，', '也', '不', '會', '推', '薦', '親', '友', '使', '用', '的', '原', '因', '，', '最', '大', '原', '因', '是', '因', '為', '；', '居', '然', '在', '使', '用', '後', '，', '肌', '膚', '反', '而', '變', '得', '乾', '到', '發', '癢', '，', '可', '見', '產', '品', '若', '無', '法', '真', '正', '提', '供', '乾', '性', '肌', '膚', '滋', '潤', '和', '保', '濕', '的', '話', '，', '添', '加', '再', '多', '吸', '引', '消', '費', '者', '的', '成', '分', '也', '不', '見', '得', '能', '得', '到', '消', '費', '者', '的', '心']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['顧', '名', '思', '義', '這', '是', '一', '款', '晚', '安', '凍', '膜', '，', '乾', '肌', '我', '本', '人', '於', '晚', '上', '卸', '妝', '洗', '臉', '洗', '澡', '後', '睡', '前', '保', '養', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "=====================================\n",
      "['保', '濕', '效', '果', '超', '優', '的', '，', '暗', '沉', '和', '乾', '燥', '肌', '也', '統', '統', 'O', 'U', 'T', '了', '，']\n",
      "['B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['不', '過', '每', '次', '敷', '完', '臉', ',', '保', '濕', '的', '效', '果', '非', '常', '好', ',', '因', '為', '是', '敏', '感', '膚', '質', ',', '有', '時', '候', '臉', '上', '會', '有', '過', '敏', '現', '象', ',']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['建', '議', '乾', '肌', '的', '朋', '友', '們', '還', '是', '要', '勤', '做', '後', '續', '的', '保', '養', '動', '作']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "=====================================\n",
      "['本', '產', '品', '適', '合', '一', '般', '膚', '質', '及', '敏', '感', '性', '膚', '質', '的', '日', '常', '保', '養', '，', '也', '很', '適', '合', '急', '救', '乾', '荒', '缺', '水', '的', '肌', '膚', '，', '保', '濕', '效', '果', '能', '立', '即', '見', '效']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A'], '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['以', '前', '我', '有', '敷', '過', '別', '款', '的', '美', '白', '面', '膜', '~', '很', '容', '易', '就', '會', '讓', '我', '的', '臉', '部', '肌', '膚', '過', '敏', '，', '會', '有', '紅', '紅', '的', '臉', '出', '現']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['露', '珠', '草', '萃', '取', '液', '晶', '華', '能', '舒', '緩', '肌', '膚', '乾', '燥', '不', '適', '並', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A']]\n",
      "\n",
      "=====================================\n",
      "['很', '適', '合', '油', '性', '膚', '質', '使', '用', '，', '但', '對', '於', '乾', '性', '肌', '膚', '就', '要', '加', '強', '後', '續', '的', '保', '濕']\n",
      "['O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', [], [], [], [], '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['我', '的', '美', '麗', '日', '記', '一', '直', '是', '我', '的', '愛', '用', '品', '之', '一', '，', '對', '於', '健', '康', '膚', '色', '油', '性', '混', '合', '肌', '的', '我', '，', '挑', '選', '面', '膜', '的', '重', '點', '就', '是', '-', '-', '-', '可', '以', '變', '白', '、', '保', '濕', '、', '不', '會', '黏', '黏', '的', '(', '很', '重', '要', ')']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['夏', '日', '將', '到', '了', '，', '保', '濕', '不', '是', '只', '有', '在', '冬', '天', '，', '夏', '天', '會', '常', '常', '待', '在', '冷', '氣', '房', '，', '所', '以', '保', '濕', '還', '是', '很', '重', '要', '，', '而', '這', '面', '膜', '對', '於', '乾', '燥', '的', '肌', '膚', '急', '救', '補', '水', '效', '果', '還', '挺', '讓', '人', '滿', '意', '的']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['溫', '柔', '而', '堅', '定', '的', '修', '護', '力', 'Z', 'e', 'p', 'h', 'y', 'r', 'i', 'n', 'e', '數', '字', '保', '養', ',', '非', '常', '溫', '和', '的', '產', '品', '即', '使', '是', '敏', '感', '肌', '也', '可', '以', '輕', '鬆', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['有', '些', '保', '濕', '面', '膜', '，', '我', '的', '敏', '感', '性', '膚', '質', '如', '果', '敷', '太', '久', '，', '就', '會', '開', '始', '發', '癢', '，', '這', '款', '一', '直', '敷', '到', '最', '後', '都', '很', '舒', '服', '呢']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['第', '三', '種', '方', '式', '是', '每', '週', '1', '-', '2', '次', '，', '夜', '間', '所', '有', '保', '養', '程', '序', '後', '，', '以', '厚', '敷', '來', '加', '強', '肌', '膚', '乾', '燥', '部', '位', '的', '照', '顧']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['在', '這', '一', '下', '冷', '一', '下', '又', '熱', '的', '氣', '候', ',', '對', '於', '乾', '性', '肌', '膚', '的', '我', '來', '說', ',', '真', '是', '相', '當', '需', '要', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "\n",
      "precision  \t recall  \t fbeta_score  \t\n",
      "(0.37362637362637363, 0.4788732394366197, 0.419753086419753, None)\n",
      "\n",
      "==========================\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity loss : 0.0483\n",
      "Relation loss : 0.0011\n",
      "\n",
      "['我', '覺', '得', '這', '面', '膜', '在', '這', '價', '位', '還', '可', '以', '接', '受', '，', '尤', '其', '是', '很', '會', '過', '敏', '的', '人', '來', '說', '，', '能', '使', '用', '的', '產', '品', '不', '多', '，', '保', '濕', '度', '對', '乾', '性', '肌', '的', '我', '來', '說', '也', '還', '可', '以', '，', '這', '款', '我', '覺', '得', '可', '以', '再', '回', '購']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['本', '身', '是', '極', '乾', '的', '膚', '質', ',', '常', '需', '要', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['使', '用', '完', '後', '肌', '膚', '乾', '淨', '舒', '服', ',', '休', '息', '一', '下', '後', '就', '可', '以', '上', '平', '日', '保', '養', '步', '驟', '~']\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O']\n",
      "['', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9f9709605917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfilter_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_output\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_input_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# total score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-7fe50c12add8>\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 單句decode關係\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrue_r_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappear_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mr_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappear_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrue_r_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_r_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-7fe50c12add8>\u001b[0m in \u001b[0;36mdecode_rel\u001b[0;34m(ent_output, rel_output)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;34m'''AttributeError: 'str' object has no attribute 'append'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                     \u001b[0mr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_complete_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# 當前token填上關係\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always') \n",
    "\n",
    "score_predict = []\n",
    "score_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(dev_loader):\n",
    "        model.eval()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        batchsize = batch_x.size(0)\n",
    "        \n",
    "        ent_loss = criterion_tag(ent_output.cpu(), batch_ent.view(batchsize*MAX_LEN))\n",
    "        ent_output = ent_argmax(ent_output, batchsize)\n",
    "        \n",
    "        rel_loss = criterion_rel(rel_output.cpu(), batch_rel.view(batchsize*MAX_LEN*MAX_LEN))\n",
    "        rel_output = rel_argmax(rel_output, batchsize)\n",
    "    \n",
    "        print()    \n",
    "        print(\"Entity loss : %.4f\" % ent_loss)\n",
    "        print(\"Relation loss : %.4f\" % rel_loss)\n",
    "        print()\n",
    "        \n",
    "        filter_true, filter_output = batch_decode(ent_output, rel_output, \\\n",
    "                                    batch_index, raw_input_dev, batch_ent, batch_rel)\n",
    "        \n",
    "        # total score\n",
    "        score_predict.extend(filter_output)\n",
    "        score_true.extend(filter_true)\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        print('==========================')\n",
    "        print('==========================')\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    print(\"Total score\")\n",
    "    print(\"%s \\t %s \\t %s \\t\" % ('precision ', 'recall ', 'fbeta_score '))\n",
    "    print(precision_recall_fscore_support(score_true, score_predict, average='micro', \\\n",
    "                                          labels=np.unique(score_predict)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = root+'facial_r2.test'\n",
    "input_test, ent_test, rel_test, raw_index_test, raw_input_test = dev_preprocess(test_data)\n",
    "test_loader = dataload(input_test, ent_test, rel_test, raw_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity loss : 0.0527\n",
      "Relation loss : 0.0012\n",
      "\n",
      "['優', '點', '：', '很', '好', '推', '，', '延', '展', '性', '也', '很', '好', '，', '也', '不', '黏', '膩', '，', '吸', '收', '程', '度', '很', '好', '，', '很', '快', '就', '推', '進', '去', '的', '感', '覺', '，', '擦', '完', '後', '，', '摸', '起', '來', '很', '好', '摸', '，', '很', '柔', '嫩', '，', '很', '平', '滑', '，', '沒', '有', '剌', '激', '感', '，', '即', '使', '是', '敏', '感', '肌', '也', '可', '以', '使', '用', '喔']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['●', '做', '完', '雷', '射', '後', '覺', '得', '皮', '膚', '敏', '感', '，', '不', '知', '道', '要', '擦', '什', '麼', '美', '白', '產', '品', '，', '但', '又', '怕', '反', '黑']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['收', '到', 'L', 'A', 'N', 'E', 'I', 'G', 'E', '蘭', '芝', '水', '酷', '肌', '因', '智', '慧', '保', '濕', '凝', '凍', '的', '試', '用', '通', '知', '，', '其', '特', '色', '是', '超', '導', '離', '子', '礦', '物', '水', '能', '深', '層', '滲', '透', '2', '0', '層', '肌', '膚', '，', '活', '化', '保', '濕', '基', '因', '修', '護', '受', '損', '肌', '膚', '，', '並', '且', '紓', '解', '乾', '燥', '肌', '膚', '的', '不', '適', '感']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['使', '用', '後', '肌', '膚', '水', '潤', '保', '濕', '，', '由', '於', '我', '是', '混', '合', '性', '偏', '油', '肌', '，', '會', '視', '當', '日', '氣', '候', '及', '肌', '膚', '狀', '況', '再', '決', '定', '是', '否', '補', '充', '乳', '液', '，', '上', '圖', '未', '使', '用', '乳', '液', '，', '肌', '膚', '就', '很', '水', '透', '潤', '澤']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['推', '薦', '給', '喜', '歡', '簡', '單', '保', '養', '的', '敏', '感', '肌', '朋', '友', ':', ')']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['我', '本', '身', '是', '混', '合', '偏', '油', '肌', '，', '平', '時', '若', '保', '養', '不', '夠', '或', '用', '到', '不', '合', '適', '的', '保', '養', '品', '，', '容', '易', '導', '致', '出', '油', '量', '大', '增', '、', '暗', '沉', '甚', '至', '長', '粉', '刺', '痘', '痘']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', [], [], '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['有', '達', '到', '平', '衡', '過', '度', '出', '油', '肌', '膚', '的', '效', '果', '，', '對', '於', '容', '易', '長', '痘', '痘', '者', '，', '可', '以', '舒', '緩', '肌', '膚', '，']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O']\n",
      "['', '', '', '', '', [], [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '']\n",
      "\n",
      "=====================================\n",
      "['我', '是', '屬', '於', '很', '容', '易', '泛', '紅', '的', '膚', '質', '，', '這', '款', '是', '專', '門', '敏', '感', '肌', '設', '計', '的', '保', '濕', '精', '華']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['真', '心', '覺', '得', '這', '是', '一', '罐', '適', '合', '像', '我', '這', '樣', '敏', '感', '肌', '的', '水', '水', '使', '用', '的', '產', '品', '~', '質', '地', '溫', '和', '而', '且', '保', '濕', '度', '優', '到', '爆', '炸']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['這', '產', '品', '滿', '適', '合', '冷', '氣', '房', '使', '用', '~', '因', '為', '真', '的', '滿', '保', '濕', '的', '~', '可', '以', '擦', '厚', '一', '些', '當', '面', '膜', '~', '也', '可', '以', '當', '眼', '膠', '~', '不', '會', '刺', '激', '敏', '感', '性', '肌', '膚', '~', '不', '致', '痘', '~', '味', '道', '不', '重', '讓', '人', '覺', '得', '滿', '放', '鬆', '的', '~', '好', '推', '好', '吸', '收', '~', '擦', '完', '我', '的', '肌', '膚', '變', '明', '亮', '~', '改', '善', '膚', '色', '暗', '沉', '喔', '~', '會', '繼', '續', '使', '用', '的', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['保', '濕', '修', '護', '，', '適', '用', '於', '一', '般', '、', '敏', '感', '或', '乾', '性', '肌', '膚', '使', '用']\n",
      "['B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O']\n",
      "[['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '']\n",
      "\n",
      "Predict output\n",
      "['B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O']\n",
      "[['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '']\n",
      "\n",
      "=====================================\n",
      "['建', '議', '過', '敏', '膚', '質', '的', '朋', '友', '對', '於', '美', '白', '產', '品', '的', '選', '擇', '要', '特', '別', '小', '心']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', [], [], [], [], '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['因', '為', '加', '保', '濕', '配', '方', '~', '所', '以', '混', '合', '性', '肌', '膚', '，', '雙', '頰', '容', '易', '乾', '燥', '的', '我', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', [], [], [], [], [], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['怕', '油', '肌', '膚', '可', '酌', '量', '減', '少', '使', '用', '2', '~', '3', '滴', '調', '整', '用', '量', '，', '再', '以', '精', '華', '液', '與', '乳', '霜', '加', '強', '保', '養']\n",
      "['O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], []]\n",
      "\n",
      "Predict output\n",
      "['O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['對', '於', '乾', '性', '肌', '膚', '保', '濕', '度', '略', '嫌', '不', '足']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', [], [], [], [], [], [], [], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['所', '以', '，', '可', '能', '偏', '熟', '零', '肌', '的', '人', '要', '非', '常', '加', '強', '妝', '前', '的', '保', '濕', '度', '，', '它', '的', '質', '地', '應', '該', '比', '較', '適', '合', '年', '輕', '肌', '膚', '在', '夏', '天', '或', '有', '點', '偏', '油', '性', '肌', '膚', '的', '人', '來', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['看', '來', '「', 'P', 'U', 'R', 'U', 'N', 'N', 'U', 'R', 'I', '韓', '方', '去', '角', '質', '手', '工', '皂', '﹙', '艾', '草', '﹚', '」', '果', '真', '很', '適', '合', '舒', '緩', '乾', '癢', '肌', '膚', '啊']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B'], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['在', '這', '秋', '冬', '交', '替', '的', '季', '節', '，', '天', '氣', '陰', '晴', '不', '定', '，', '使', '得', '肌', '膚', '常', '出', '現', '缺', '水', '的', '乾', '燥', '狀', '態', '，', '使', '用', '專', '科', '完', '美', '多', '效', '晚', '安', '水', '凝', '霜', '之', '後', '，', '肌', '膚', '乾', '燥', '問', '題', '明', '顯', '獲', '得', '改', '善', '，', '肌', '膚', '變', '得', '緊', '緻', '有', '光', '澤', '，', '光', '滑', '好', '撫', '摸']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "\n",
      "precision  \t recall  \t fbeta_score  \t\n",
      "(0.4945054945054945, 0.703125, 0.5806451612903226, None)\n",
      "\n",
      "==========================\n",
      "\n",
      "\n",
      "Entity loss : 0.0338\n",
      "Relation loss : 0.0013\n",
      "\n",
      "['此', '款', '精', '華', '液', '強', '調', '舒', '緩', '功', '能', '，', '所', '以', '對', '於', '較', '敏', '感', '膚', '質', '的', '我', '是', '在', '適', '合', '不', '過', '的', '，']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['能', '夠', '平', '撫', '、', '舒', '緩', '一', '天', '工', '作', '的', '壓', '力', '，', '放', '鬆', '緊', '張', '的', '心', '情', '，', '產', '品', '天', '然', '溫', '和', '無', '刺', '激', '，', '任', '何', '肌', '膚', '（', '含', '敏', '感', '性', '肌', '膚', '）', '均', '可', '適', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['非', '常', '推', '薦', '秋', '冬', '急', '需', '保', '濕', '的', '乾', '燥', '肌', '膚', '使', '用', '喔']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '']\n",
      "\n",
      "=====================================\n",
      "['且', '因', '他', '的', '保', '濕', '成', '分', '，', '所', '以', '像', '我', '這', '樣', '乾', '燥', '的', '肌', '膚', '，', '都', '覺', '得', '還', '挺', '保', '濕', '，', '不', '像', '以', '前', '用', '完', '遮', '瑕', '產', '品', '後', '，', '會', '覺', '得', '乾', '乾', '的', '，', '然', '後', '細', '紋', '變', '得', '更', '明', '顯']\n",
      "['O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', [], [], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['不', '過', '整', '體', '而', '言', ',', '新', '升', '級', '版', '的', '保', '濕', '霜', ',', '它', '的', '保', '濕', '度', '及', '滋', '潤', '度', ',', '彈', '力', '度', '真', '的', '很', '不', '錯', ',', '不', '油', '也', '不', '會', '讓', '肌', '膚', '乾', ',', '解', '決', '了', '不', '少', '冬', '天', '肌', '膚', '的', '問', '題']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['珞', '瑜', '認', '為', '「', 'X', 'a', 'n', 't', 'i', 'a', '桑', '緹', '亞', '微', '導', '修', '護', '新', '生', '精', '華', '液', '」', '可', '讓', '肌', '膚', '保', '濕', '、', '舒', '緩', '肌', '膚', '乾', '燥', '、', '補', '充', '及', '鎖', '住', '水', '分', '、', '滋', '潤', '臉', '上', '的', '肌', '膚', '，', '使', '「', '顏', '肌', '」', '（', '指', '臉', '上', '肌', '膚', '）', '呈', '現', '透', '亮', '光', '澤']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', [], [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', [], [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['專', '為', '乾', '性', '與', '敏', '感', '性', '肌', '膚', '所', '設', '計', '的', '修', '護', '乳', '霜', '、', '長', '效', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', [], [], [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "=====================================\n",
      "['潔', '美', '淨', '層', '脂', '質', '保', '濕', '調', '理', '霜', '主', '要', '是', '針', '對', '3', '5', '-', '4', '9', '歲', '有', '多', '年', '乾', '性', '肌', '膚', '以', '及', '敏', '感', '問', '題', '的', '女', '性']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['粉', '底', '液', '我', '一', '定', '會', '找', '有', '保', '濕', '成', '分', '在', '裡', '面', ',', '因', '為', '我', '的', '皮', '膚', '是', '乾', '肌']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], []]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B']]\n",
      "\n",
      "=====================================\n",
      "['使', '用', '後', '很', '快', '就', '吸', '收', '了', ',', '清', '爽', '不', '黏', '膩', ',', '使', '用', '後', '肌', '膚', '柔', '滑', '有', '光', '澤', ',', '保', '濕', '度', '很', '持', '久', ',', '有', '淡', '淡', '的', '香', '氣', '我', '喜', '歡', ',', '大', '致', '上', '來', '說', '是', '不', '錯', '用', '的', '商', '品', ',', '敏', '感', '肌', '也', '適', '用', '喔']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['對', '於', '敏', '感', '性', '肌', '膚', '亦', '有', '舒', '緩', '和', '改', '善', '的', '作', '用']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n",
      "['使', '用', '皇', '家', '蜂', '王', '乳', '修', '護', '精', '華', '期', '間', '，', '早', '晚', '各', '一', '次', '使', '用', '，', '肌', '膚', '一', '切', '正', '常', '，', '完', '完', '全', '全', '沒', '有', '過', '敏', '現', '象', '~', '不', '但', '沒', '有', '過', '敏', '，', '還', '有', '舒', '緩', '的', '感', '覺', '~', '敏', '感', '肌', '的', '水', '水', '放', '心', '吧', '^', '_', '^']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['質', '地', '偏', '乳', '狀', '~', '~', '很', '保', '濕', '~', '~', '我', '覺', '得', '很', '適', '合', '待', '在', '冷', '氣', '房', '和', '偏', '乾', '性', '肌', '的', '水', '水', '使', '用', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '']\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '']\n",
      "\n",
      "=====================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d1649e0b6519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_input_test\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0mbatch_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=========================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-7fe50c12add8>\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 單句decode關係\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrue_r_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappear_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mr_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappear_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrue_r_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_r_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-7fe50c12add8>\u001b[0m in \u001b[0;36mdecode_rel\u001b[0;34m(ent_output, rel_output)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;34m'''AttributeError: 'str' object has no attribute 'append'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                     \u001b[0mr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_complete_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# 當前token填上關係\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        batchsize = batch_x.size(0)\n",
    "        \n",
    "        ent_loss = criterion_tag(ent_output.cpu(), batch_ent.view(BATCH_SIZE*MAX_LEN))\n",
    "        ent_output = ent_argmax(ent_output, batchsize)\n",
    "        \n",
    "        rel_loss = criterion_rel(rel_output.cpu(), batch_rel.view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "        rel_output = rel_argmax(rel_output, batchsize)\n",
    "    \n",
    "        print()\n",
    "        \n",
    "        print(\"Entity loss : %.4f\" % ent_loss)\n",
    "        print(\"Relation loss : %.4f\" % rel_loss)\n",
    "        print()\n",
    "        \n",
    "        batch_decode(ent_output, rel_output, batch_index, raw_input_test, \\\n",
    "                     batch_ent, batch_rel)\n",
    "        print()\n",
    "        print('==========================')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_content = readfile(dev_data)\n",
    "word_list, ent_list, rel_list = split_to_list(dev_content)\n",
    "reserved_index = filter_len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['露',\n",
       " '珠',\n",
       " '草',\n",
       " '萃',\n",
       " '取',\n",
       " '液',\n",
       " '晶',\n",
       " '華',\n",
       " '能',\n",
       " '舒',\n",
       " '緩',\n",
       " '肌',\n",
       " '膚',\n",
       " '乾',\n",
       " '燥',\n",
       " '不',\n",
       " '適',\n",
       " '並',\n",
       " '加',\n",
       " '強',\n",
       " '保',\n",
       " '濕']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[-15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 802,  745,  610,  598,  259,  294,  113,  293,  127,  245,\n",
      "         246,   52,   70,  137,  138,   37,  281,  385,  150,   96,\n",
      "          23,   24,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
      "tensor([[ 6,  6,  6,  6,  6,  6,  6,  6,  6,  3,  5,  2,  4,  4,\n",
      "          4,  6,  6,  6,  6,  6,  3,  5,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1]], device='cuda:0')\n",
      "tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
      "         2,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0], device='cuda:0')\n",
      "tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0], device='cuda:0')\n",
      "['露', '珠', '草', '萃', '取', '液', '晶', '華', '能', '舒', '緩', '肌', '膚', '乾', '燥', '不', '適', '並', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A']]\n",
      "\n",
      "Predict output\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], ['ApplyTo-0-B', 'ApplyTo-1-B'], '', '', '', '', '', ['ApplyTo-1-A'], ['ApplyTo-1-A']]\n",
      "\n",
      "=====================================\n",
      "\n",
      "precision  \t recall  \t fbeta_score  \t\n",
      "(1.0, 1.0, 1.0, None)\n"
     ]
    }
   ],
   "source": [
    "#### \n",
    "with torch.no_grad():\n",
    "#     r_choose = random_choose(input_dev)\n",
    "    input_dev, ent_dev, rel_dev, raw_index_dev, raw_input_dev = dev_preprocess(dev_data)\n",
    "    \n",
    "    print(input_dev[-12])\n",
    "    model.eval()\n",
    "    ii = torch.unsqueeze(input_dev[-12],0)\n",
    "    ent_output, rel_output = model(ii.cuda() if USE_CUDA else input_dev)\n",
    "    \n",
    "    batchsize = ii.size(0)\n",
    "    \n",
    "    ent_loss = criterion_tag(ent_output.cpu(), ent_dev[-12].view(batchsize*MAX_LEN))\n",
    "    ent_output = ent_argmax(ent_output, batchsize)\n",
    "    \n",
    "    rel_loss = criterion_rel(rel_output.cpu(), rel_var[-12].view(batchsize*MAX_LEN*MAX_LEN))\n",
    "    rel_output = rel_argmax(rel_output, batchsize)\n",
    "    \n",
    "    print(ent_output)\n",
    "    print(rel_output[0, 21])\n",
    "    print(rel_output[0, 11])\n",
    "    rrr = torch.unsqueeze(rel_dev[-12], 0)\n",
    "    \n",
    "    lll = []\n",
    "    lll.append(raw_input_dev[-15])\n",
    "\n",
    "    \n",
    "    filter_true, filter_output = batch_decode(ent_output, rel_output, \\\n",
    "                            raw_index_dev, lll, ent_dev[-12].view(1, -1), rrr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
