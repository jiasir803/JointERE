{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(data):\n",
    "    with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().splitlines()\n",
    "        \n",
    "    return content\n",
    "\n",
    "def schema_load(schema_root):\n",
    "    raw_dict = \"\".join(readfile(schema_root))\n",
    "    dict2json = \"\".join(raw_dict.split()[2:])\n",
    "\n",
    "    json_acceptable_string = dict2json.replace(\"'\", \"\\\"\")\n",
    "    schema = json.loads(json_acceptable_string)\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def define_entity(schema):\n",
    "    tag_type = list(schema['tagging'])\n",
    "    \n",
    "    entity_tag = []\n",
    "    for k in list(schema['entity'].keys()):\n",
    "        entity_tag.append(schema['entity'][k]['tag'])\n",
    "        \n",
    "    TAG = []\n",
    "    for t in tag_type:\n",
    "        for e in entity_tag:\n",
    "            if t!='O':\n",
    "                TAG.append(t+'-'+e)  \n",
    "                \n",
    "    TAG = [UNKOWN_TAG, PAD_TAG] + TAG + ['O']   \n",
    "\n",
    "    return TAG\n",
    "\n",
    "def tag2ix(TAG):\n",
    "    tag_to_ix={t:i for i,t in enumerate(TAG)}\n",
    "    return tag_to_ix\n",
    "\n",
    "def define_relation(schema):\n",
    "    relation_type = list(schema['relation'])\n",
    "    \n",
    "    relation_tag = []\n",
    "    for k in list(schema['relation'].keys()):\n",
    "        relation_tag.append(schema['relation'][k]['tag'])\n",
    "    \n",
    "    relation_tag = [REL_PAD] + [REL_NONE] + relation_tag\n",
    "        \n",
    "    return relation_tag\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def get_word_and_label(_content, start_w, end_w):\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "    \n",
    "    for word_set in _content[start_w:end_w]:\n",
    "        word_set = word_set.split()\n",
    "        if len(word_set)==1:\n",
    "            word_list.append(' ')\n",
    "            ent_list.append('O')\n",
    "            rel_list.append(REL_NONE)\n",
    "        \n",
    "        else:\n",
    "            word_list.append(word_set[0])\n",
    "            ent_list.append(word_set[1])\n",
    "\n",
    "            try:\n",
    "                testerror = word_set[2]\n",
    "            except IndexError:\n",
    "                rel_list.append(REL_NONE)\n",
    "            else:\n",
    "                rel_list.append(word_set[2:])\n",
    "    \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "def split_to_list(content):\n",
    "    init = 0\n",
    "    word_list = []\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "\n",
    "    for now_token, c in enumerate(content):\n",
    "        if c=='':\n",
    "            words, ents, rels = get_word_and_label(content, init, now_token)\n",
    "            init = now_token+1\n",
    "            word_list.append(words)\n",
    "            ent_list.append(ents)\n",
    "            rel_list.append(rels)\n",
    "            \n",
    "    return word_list, ent_list, rel_list\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def word2index(word_list):\n",
    "    word_to_ix = {\"<UNKNOWN>\":0, \"<PAD>\":1}\n",
    "    for sentence in word_list:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "                \n",
    "    return word_to_ix\n",
    "\n",
    "def dict_inverse(tag_to_ix):\n",
    "    ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "    return ix_to_tag\n",
    "\n",
    "def index2tag(indexs, ix_to):\n",
    "    to_tags = [ix_to[i] for i in indexs.cpu().numpy()]\n",
    "    return to_tags\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def find_max_len(word_list):\n",
    "    max_len = 0\n",
    "    for i in range(len(word_list)):\n",
    "        if max_len<len(word_list[i]):\n",
    "            max_len=len(word_list[i])\n",
    "            \n",
    "    return max_len\n",
    "\n",
    "# ====== filter the length of sentence more than MAX_LEN =======\n",
    "\n",
    "def filter_len(word_list):\n",
    "    reserved_index = []\n",
    "    for i in range(len(word_list)):\n",
    "        if len(word_list[i])<MAX_LEN:\n",
    "            reserved_index.append(i)\n",
    "            \n",
    "    return reserved_index\n",
    "\n",
    "\n",
    "def filter_sentence(reserved_index, word_list, ent_list, rel_list):\n",
    "    filter_word = list(word_list[i] for i in reserved_index)\n",
    "    filter_ent = list(ent_list[i] for i in reserved_index)\n",
    "    filter_rel = list(rel_list[i] for i in reserved_index)\n",
    "    return filter_word, filter_ent, filter_rel\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def pad_seq(seq, isrel):\n",
    "    if isrel:\n",
    "        seq += [REL_NONE for i in range(MAX_LEN-len(seq))]\n",
    "    else:\n",
    "        seq += [PAD_TAG for i in range(MAX_LEN-len(seq))]\n",
    "    return seq\n",
    "\n",
    "def pad_all(filter_word, filter_ent, filter_rel):\n",
    "    input_padded = [pad_seq(s, False) for s in filter_word]\n",
    "    ent_padded = [pad_seq(s, False) for s in filter_ent]\n",
    "    rel_padded = [pad_seq(s, True) for s in filter_rel]\n",
    "    \n",
    "    return input_padded, ent_padded, rel_padded\n",
    "\n",
    "def deep_copy_lists(filter_word, filter_ent, filter_rel):\n",
    "    f_w = copy.deepcopy(filter_word)\n",
    "    f_e = copy.deepcopy(filter_ent)\n",
    "    f_r = copy.deepcopy(filter_rel)\n",
    "    \n",
    "    return f_w, f_e, f_r\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if w not in to_ix:\n",
    "            idxs.append(to_ix[UNKOWN_TAG])\n",
    "        else:\n",
    "            idxs.append(to_ix[w])\n",
    "    \n",
    "#     idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_all(seqs, to_ix):\n",
    "    seq_list = []\n",
    "    for i in range(len(seqs)):\n",
    "        seq_list.append(prepare_sequence(seqs[i], to_ix))\n",
    "        \n",
    "    seq_list = torch.stack(seq_list)\n",
    "        \n",
    "    return seq_list\n",
    "\n",
    "\n",
    "\n",
    "def prepare_rel(rel_padded, to_ix):\n",
    "    \n",
    "    rel_ptr = torch.zeros(len(rel_padded), MAX_LEN, MAX_LEN, dtype=torch.long) \n",
    "    \n",
    "    # 對當前的token，去比較之前所有出現過的entity，是否有關係，建成矩陣\n",
    "    # [B*ML*ML]，第二維ML是當前token，第三維ML是根據當前token對之前出現過的entity紀錄關係，以index紀錄\n",
    "    for i, rel_seq in enumerate(rel_padded):\n",
    "        rel_dict = {}\n",
    "        for j, token_seq in enumerate(rel_seq):\n",
    "            rel_ptr[i][j][:j+1] = 1\n",
    "            if token_seq != REL_NONE:\n",
    "                for k, rel in enumerate(token_seq):\n",
    "\n",
    "                    # if 是第一次出現，紀錄後面數字(標第幾對)和關係位置(A OR B)\n",
    "                    # 假如下次出現又是同個關係位置(A)，依然紀錄\n",
    "                    # 直到下次出現關係位置B，依照之前紀錄的A位置的字，然後在第三維去標關係\n",
    "\n",
    "                    rel_token = rel.split('-')\n",
    "                    if rel_token[1] not in rel_dict:\n",
    "                        rel_dict[rel_token[1]] = {'rel':rel_token[0], 'loc':rel_token[2], 'idx':[j]}\n",
    "\n",
    "                    elif rel_token[1] in rel_dict and rel_dict[rel_token[1]]['loc']==rel_token[2]:\n",
    "                        rel_dict[rel_token[1]]['idx'].append(j)\n",
    "\n",
    "                    else:\n",
    "                        record_loc = rel_dict[rel_token[1]]['idx']\n",
    "                        for idxx in record_loc:\n",
    "                            rel_ptr[i][j][idxx] = to_ix[rel_token[0]]\n",
    "                            \n",
    "    return rel_ptr\n",
    "                \n",
    "\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "def dataload(input_var, ent_var, rel_var, raw_input):\n",
    "    torch_dataset = Data.TensorDataset(input_var, ent_var, rel_var, raw_input)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=2,       \n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# ==================================================\n",
    "def softmax_entity(entity):\n",
    "    entity = entity.view(BATCH_SIZE,ent_size).argmax(1)\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, attn_input, attn_output, rel_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.attn_input = attn_input\n",
    "        self.attn_output = attn_output\n",
    "        self.rel_size = rel_size\n",
    "        \n",
    "        self.w1 = nn.Linear(self.attn_input, self.attn_output)\n",
    "        self.w2 = nn.Linear(self.attn_input, self.attn_output)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.v = nn.Linear(self.attn_output, self.rel_size, bias=False)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        \n",
    "        decoder = encoder_outputs[:,-1,:].unsqueeze(1)                       #B*1*(ts+LE) [128,1,8]\n",
    "        encoder_score = self.w1(encoder_outputs)                             #B*now len*ATTN_OUT\n",
    "        decoder_score = self.w2(decoder)                                     #B*1*ATTN_OUT\n",
    "        energy = self.tanh(encoder_score+decoder_score)                      #B*now len*ATTN_OUT            \n",
    "        \n",
    "        energy = self.v(energy)                                              #B*now len*rel_size\n",
    "        \n",
    "        \n",
    "        # 針對每個entity做softmax，去顯示他們的關係權重\n",
    "        # 主要都會是rel_none\n",
    "        # 對第二維(rel)做softmax\n",
    "        p = self.softmax(energy)                                         #B*now len*rel_size\n",
    "        \n",
    "        return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity_Typing(nn.Module):\n",
    "    def __init__(self, vocab_size, ent_tag_to_ix, embedding_dim, hidden_dim1, hidden_dim2, \\\n",
    "                 label_embed_dim, rel_tag_to_ix):\n",
    "        \n",
    "        super(Entity_Typing, self).__init__()\n",
    "        self.embedding_dim = embedding_dim                   #E\n",
    "        self.hidden_dim1 = hidden_dim1                       #h1\n",
    "        self.hidden_dim2 = hidden_dim2                       #h2\n",
    "        self.label_embed_dim = label_embed_dim               #LE\n",
    "        self.vocab_size = vocab_size                         #vs\n",
    "        self.ent_to_ix = ent_tag_to_ix\n",
    "        self.ent_size = len(ent_tag_to_ix)                   #es\n",
    "        self.rel_to_ix = rel_tag_to_ix\n",
    "        self.rel_size = len(rel_tag_to_ix)                   #rs           \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn = nn.BatchNorm1d(DENSE_OUT, momentum=0.5, affine=False)\n",
    "        \n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         self.bilstm = nn.LSTM(embedding_dim, hidden_dim1 // 2,\n",
    "#                             num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)        \n",
    "        self.bilstm = nn.GRU(embedding_dim, hidden_dim1 // 2,\n",
    "                            num_layers=2, bidirectional=True, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        \n",
    "        self.dense = nn.Linear(hidden_dim1, DENSE_OUT)\n",
    "        self.top_hidden = nn.LSTMCell(DENSE_OUT+label_embed_dim, hidden_dim2)          \n",
    "        \n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim2, self.ent_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.label_embed = nn.Linear(self.ent_size, self.label_embed_dim)\n",
    "        \n",
    "        self.attn = Attn(ATTN_IN, ATTN_OUT, self.rel_size)\n",
    "        \n",
    "        \n",
    "    def init_hidden1(self):       \n",
    "        hidden = torch.randn(2*2, BATCH_SIZE, self.hidden_dim1 // 2)    #4*B*(h1/2)\n",
    "#         hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_hidden2(self):       \n",
    "        hidden = torch.randn(BATCH_SIZE, self.hidden_dim2)              #B*h2\n",
    "#         hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_label_embed(self):\n",
    "        hidden = torch.zeros(BATCH_SIZE, self.label_embed_dim)          #B*LE\n",
    "        return hidden.cuda()if USE_CUDA else hidden\n",
    "    \n",
    "    def create_entity(self):\n",
    "        output_tensor = torch.zeros(BATCH_SIZE, MAX_LEN, self.ent_size)  #B*ML*es\n",
    "        return output_tensor.cuda()if USE_CUDA else output_tensor\n",
    "    \n",
    "    def create_rel_matrix(self):\n",
    "        rel_tensor = torch.zeros(BATCH_SIZE, MAX_LEN, MAX_LEN, self.rel_size)  #B*ML*ML*rs\n",
    "        return rel_tensor.cuda()if USE_CUDA else rel_tensor\n",
    "    \n",
    "    \n",
    "        \n",
    "    def forward(self, sentence):\n",
    "#         self.hidden1 = self.init_hidden1()                      #4*B*(h1/2)\n",
    "        entity_tensor = self.create_entity()                    #B*ML*es\n",
    "        rel_tensor = self.create_rel_matrix()                   #B*ML*ML*rs\n",
    "        \n",
    "        embeds = self.word_embeds(sentence)                     #B*ML*E,[128, 100, 20]\n",
    "        \n",
    "#         bilstm_out, self.hidden1 = self.bilstm(embeds, self.hidden1)\n",
    "        bilstm_out, hidden1 = self.bilstm(embeds)\n",
    "        # bilstm_out -> B*ML*h1,[128, 100, 10]\n",
    "        # self.hidden1 -> ( 4*B*(h1/2), 4*B*(h1/2) )\n",
    "        \n",
    "        # bn\n",
    "        bilstm_out = self.bn(bilstm_out)\n",
    "        dense_out = self.dense(bilstm_out)                      #B*ML*DENSE_OUT,[128, 100, 100]\n",
    "        \n",
    "        \n",
    "        encoder_sequence_l = [] \n",
    "\n",
    "        for length in range(MAX_LEN):\n",
    "            now_token = dense_out[:,length,:]\n",
    "            now_token = torch.squeeze(now_token, 1)\n",
    "            if length==0:\n",
    "                \n",
    "#                 fake_hidden=(100)\n",
    "#                 noise_x = random(100)\n",
    "                self.hidden2 = self.init_hidden2()\n",
    "                self.zero_label_embed = self.init_label_embed()\n",
    "                combine_x = torch.cat((now_token, self.zero_label_embed),1)  #B*(DENSE_OUT+LE),[128, 103]\n",
    "                \n",
    "            else:\n",
    "#                 fake_hidden=h\n",
    "                self.hidden2 = (h_next, c_next)\n",
    "                combine_x = torch.cat((now_token, label),1)\n",
    "\n",
    "            h_next, c_next = self.top_hidden(combine_x, self.hidden2)    #B*h2,[128, 8]           \n",
    "            to_tags = self.hidden2tag(h_next)                            #B*es,[128, 5]            \n",
    "            ent_output = self.softmax(to_tags)                               #B*es,[128, 5]             \n",
    "            label = self.label_embed(ent_output)                             #B*LE,[128, 3]\n",
    "            \n",
    "            s_ent_output = softmax_entity(ent_output)\n",
    "            \n",
    "            \n",
    "            # Assignments to Variables are in-place operations.\n",
    "            # Use that variable in lots of other contexts \n",
    "            # and some of the functions require it to not change. \n",
    "            to_tags_clone = to_tags.clone()\n",
    "            label_clone = label.clone()\n",
    "            \n",
    "            \n",
    "#             for i, tag in enumerate(s_ent_output):\n",
    "#                 if tag==ent_tag_to_ix['O']:\n",
    "#                     to_tags_clone[i] = torch.FloatTensor([-999999 * self.ent_size])\n",
    "#                     label_clone[i] = torch.FloatTensor([-999999 * self.ent_size])\n",
    "                    \n",
    "            # relation layer\n",
    "            encoder_sequence_l.append(torch.cat((to_tags,label),1))          \n",
    "            encoder_sequence = torch.stack(encoder_sequence_l).t()     #B*len*(es+LE), [128,1,8]          \n",
    "\n",
    "            # Calculate attention weights \n",
    "            attn_weights = self.attn(encoder_sequence)\n",
    "\n",
    "        \n",
    "            entity_tensor[:,length,:] = ent_output\n",
    "            \n",
    "            # rel_tensor[:,length, 頭~當前 ,:]\n",
    "            rel_tensor[:,length,:length+1,:] = attn_weights\n",
    "\n",
    "        \n",
    "        \n",
    "        '''NLLLoss input: Input: (N,C) where C = number of classes'''\n",
    "        return entity_tensor.view(BATCH_SIZE*MAX_LEN, self.ent_size), \\\n",
    "               rel_tensor.view(BATCH_SIZE*MAX_LEN*MAX_LEN, self.rel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/notebooks/sinica/dataset/'\n",
    "train_data = root+'facial.train'\n",
    "dev_data = root+'facial.dev'\n",
    "test_data = root+'facial.test'\n",
    "\n",
    "relation_data_old = root+'facial_r.old.train'\n",
    "# relation_data = root+'facial_r.train'\n",
    "relation_data = root+'facial_r2.train'\n",
    "schema_root = root+'schema.txt'\n",
    "dev_data = root+'facial_r2.dev'\n",
    "\n",
    "\n",
    "UNKOWN_TAG = \"<UNKNOWN>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "REL_NONE = 'Rel-None'\n",
    "REL_PAD = 'Rel-Pad'\n",
    "rule = ('FUNC', 'ApplyTo', 'STAT')\n",
    "\n",
    "schema = schema_load(schema_root)\n",
    "ENT_TAG = define_entity(schema)\n",
    "REL_TAG = define_relation(schema)\n",
    "ent_tag_to_ix = tag2ix(ENT_TAG)\n",
    "'''{'<PAD>': 1,\n",
    " '<UNKNOWN>': 0,\n",
    " 'B-FUNC': 2,\n",
    " 'B-STAT': 3,\n",
    " 'I-FUNC': 4,\n",
    " 'I-STAT': 5,\n",
    " 'O': 6}'''\n",
    "rel_tag_to_ix = tag2ix(REL_TAG)\n",
    "'''{'ApplyTo': 2, 'Rel-None': 1, 'Rel-Pad': 0}'''\n",
    "\n",
    "# ========hyper-parameter-set==========\n",
    "\n",
    "ent_size = len(ent_tag_to_ix)\n",
    "rel_size = len(rel_tag_to_ix)\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 18\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM1 = 10\n",
    "HIDDEN_DIM2 = 8\n",
    "LABEL_EMBED_DIM = ent_size\n",
    "DENSE_OUT = 100\n",
    "\n",
    "ATTN_IN = ent_size+LABEL_EMBED_DIM\n",
    "ATTN_OUT = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    content = readfile(data)\n",
    "    word_list, ent_list, rel_list = split_to_list(content)\n",
    "    word_to_ix = word2index(word_list)\n",
    "    reserved_index = filter_len(word_list)\n",
    "    filter_word, filter_ent, filter_rel = filter_sentence(reserved_index, word_list, ent_list, rel_list)\n",
    "    f_w, f_e, f_r = deep_copy_lists(filter_word, filter_ent, filter_rel)\n",
    "    input_padded, ent_padded, rel_padded = pad_all(f_w, f_e, f_r)\n",
    "    #================================================\n",
    "    input_var = prepare_all(input_padded, word_to_ix)\n",
    "    ent_var = prepare_all(ent_padded, ent_tag_to_ix)\n",
    "    rel_var = prepare_rel(rel_padded, rel_tag_to_ix)\n",
    "    #================================================\n",
    "    vocab_size = len(word_to_ix)\n",
    "    \n",
    "    reserved_index = torch.from_numpy(np.asarray(reserved_index))\n",
    "    \n",
    "    return input_var, ent_var, rel_var, vocab_size, word_to_ix, reserved_index, word_list\n",
    "\n",
    "def dev_preprocess(dev_data):\n",
    "    dev_content = readfile(dev_data)\n",
    "    word_list, ent_list, rel_list = split_to_list(dev_content)\n",
    "    reserved_index = filter_len(word_list)\n",
    "    filter_word, filter_ent, filter_rel = filter_sentence(reserved_index, word_list, ent_list, rel_list)\n",
    "    f_w, f_e, f_r = deep_copy_lists(filter_word, filter_ent, filter_rel)\n",
    "    input_padded, ent_padded, rel_padded = pad_all(f_w, f_e, f_r)\n",
    "    #================================================\n",
    "    input_var = prepare_all(input_padded, word_to_ix)\n",
    "    ent_var = prepare_all(ent_padded, ent_tag_to_ix)\n",
    "    rel_var = prepare_rel(rel_padded, rel_tag_to_ix)\n",
    "    \n",
    "    reserved_index = torch.from_numpy(np.asarray(reserved_index))\n",
    "    \n",
    "    return input_var, ent_var, rel_var, reserved_index, word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_ent_tag = dict_inverse(ent_tag_to_ix)\n",
    "ix_to_rel_tag = dict_inverse(rel_tag_to_ix)\n",
    "#===============================================\n",
    "input_var, ent_var, rel_var, vocab_size, word_to_ix, raw_index, raw_input = preprocess(relation_data)\n",
    "loader = dataload(input_var, ent_var, rel_var, raw_index)\n",
    "\n",
    "input_dev, ent_dev, rel_dev, raw_index_dev, raw_input_dev = dev_preprocess(dev_data)\n",
    "dev_loader = dataload(input_dev, ent_dev, rel_dev, raw_index_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Entity_Typing(vocab_size, ent_tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, \\\n",
    "              LABEL_EMBED_DIM, rel_tag_to_ix).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "criterion_tag = nn.NLLLoss()\n",
    "# criterion_rel = nn.CrossEntropyLoss()\n",
    "criterion_rel = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/70 [00:07<08:31,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | ent loss 0.4766 | rel loss 0.0417 | total loss 0.5184\n",
      "         | val ent loss 0.4090 | val rel loss 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 2/70 [00:15<08:33,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | ent loss 0.3403 | rel loss 0.0187 | total loss 0.3590\n",
      "         | val ent loss 0.2943 | val rel loss 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 3/70 [00:22<08:33,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | ent loss 0.2997 | rel loss 0.0129 | total loss 0.3126\n",
      "         | val ent loss 0.2694 | val rel loss 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 4/70 [00:30<08:24,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | ent loss 0.3096 | rel loss 0.0108 | total loss 0.3204\n",
      "         | val ent loss 0.2450 | val rel loss 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 5/70 [00:38<08:16,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | ent loss 0.2927 | rel loss 0.0094 | total loss 0.3020\n",
      "         | val ent loss 0.2547 | val rel loss 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▊         | 6/70 [00:45<08:05,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | ent loss 0.2756 | rel loss 0.0072 | total loss 0.2829\n",
      "         | val ent loss 0.2458 | val rel loss 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 7/70 [00:52<07:56,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | ent loss 0.2439 | rel loss 0.0087 | total loss 0.2526\n",
      "         | val ent loss 0.1840 | val rel loss 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█▏        | 8/70 [01:00<07:48,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | ent loss 0.1481 | rel loss 0.0067 | total loss 0.1547\n",
      "         | val ent loss 0.1511 | val rel loss 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 9/70 [01:07<07:40,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | ent loss 0.1300 | rel loss 0.0058 | total loss 0.1358\n",
      "         | val ent loss 0.1327 | val rel loss 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 10/70 [01:15<07:33,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | ent loss 0.1175 | rel loss 0.0044 | total loss 0.1218\n",
      "         | val ent loss 0.1341 | val rel loss 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 11/70 [01:21<07:17,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | ent loss 0.1127 | rel loss 0.0042 | total loss 0.1169\n",
      "          | val ent loss 0.1309 | val rel loss 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 12/70 [01:29<07:10,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 | ent loss 0.1056 | rel loss 0.0039 | total loss 0.1094\n",
      "          | val ent loss 0.1074 | val rel loss 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 13/70 [01:36<07:03,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 | ent loss 0.0927 | rel loss 0.0033 | total loss 0.0960\n",
      "          | val ent loss 0.0922 | val rel loss 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 14/70 [01:42<06:49,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 | ent loss 0.1068 | rel loss 0.0038 | total loss 0.1106\n",
      "          | val ent loss 0.0895 | val rel loss 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 15/70 [01:49<06:41,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 | ent loss 0.0535 | rel loss 0.0024 | total loss 0.0559\n",
      "          | val ent loss 0.0985 | val rel loss 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 16/70 [01:56<06:34,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 | ent loss 0.0582 | rel loss 0.0028 | total loss 0.0610\n",
      "          | val ent loss 0.0787 | val rel loss 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 17/70 [02:04<06:27,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | ent loss 0.0536 | rel loss 0.0029 | total loss 0.0565\n",
      "          | val ent loss 0.0696 | val rel loss 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 18/70 [02:11<06:21,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 | ent loss 0.0443 | rel loss 0.0026 | total loss 0.0469\n",
      "          | val ent loss 0.0704 | val rel loss 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 19/70 [02:19<06:14,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 | ent loss 0.0325 | rel loss 0.0026 | total loss 0.0351\n",
      "          | val ent loss 0.0624 | val rel loss 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 20/70 [02:27<06:07,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | ent loss 0.0333 | rel loss 0.0022 | total loss 0.0355\n",
      "          | val ent loss 0.0729 | val rel loss 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 21/70 [02:34<06:01,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 | ent loss 0.0230 | rel loss 0.0019 | total loss 0.0249\n",
      "          | val ent loss 0.0744 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███▏      | 22/70 [02:42<05:53,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 | ent loss 0.0219 | rel loss 0.0022 | total loss 0.0241\n",
      "          | val ent loss 0.0456 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 23/70 [02:49<05:47,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 | ent loss 0.0249 | rel loss 0.0019 | total loss 0.0269\n",
      "          | val ent loss 0.0583 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 24/70 [02:57<05:40,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 | ent loss 0.0224 | rel loss 0.0021 | total loss 0.0245\n",
      "          | val ent loss 0.0660 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 25/70 [03:05<05:33,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 | ent loss 0.0184 | rel loss 0.0017 | total loss 0.0201\n",
      "          | val ent loss 0.0799 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 26/70 [03:13<05:27,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 | ent loss 0.0280 | rel loss 0.0019 | total loss 0.0298\n",
      "          | val ent loss 0.0632 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 27/70 [03:21<05:20,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 | ent loss 0.0276 | rel loss 0.0024 | total loss 0.0300\n",
      "          | val ent loss 0.0504 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 28/70 [03:28<05:13,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 | ent loss 0.0130 | rel loss 0.0014 | total loss 0.0145\n",
      "          | val ent loss 0.0646 | val rel loss 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████▏     | 29/70 [03:36<05:06,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 | ent loss 0.0139 | rel loss 0.0015 | total loss 0.0155\n",
      "          | val ent loss 0.0631 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 30/70 [03:44<04:58,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 | ent loss 0.0144 | rel loss 0.0015 | total loss 0.0160\n",
      "          | val ent loss 0.0532 | val rel loss 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 31/70 [03:50<04:50,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 | ent loss 0.0191 | rel loss 0.0017 | total loss 0.0207\n",
      "          | val ent loss 0.0451 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 32/70 [03:58<04:42,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 | ent loss 0.0107 | rel loss 0.0013 | total loss 0.0120\n",
      "          | val ent loss 0.0496 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 33/70 [04:04<04:34,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 | ent loss 0.0148 | rel loss 0.0018 | total loss 0.0166\n",
      "          | val ent loss 0.0642 | val rel loss 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▊     | 34/70 [04:12<04:26,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 | ent loss 0.0158 | rel loss 0.0017 | total loss 0.0174\n",
      "          | val ent loss 0.0574 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 35/70 [04:19<04:19,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 | ent loss 0.0138 | rel loss 0.0013 | total loss 0.0151\n",
      "          | val ent loss 0.0578 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████▏    | 36/70 [04:27<04:12,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 | ent loss 0.0112 | rel loss 0.0014 | total loss 0.0126\n",
      "          | val ent loss 0.0563 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 37/70 [04:35<04:05,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 | ent loss 0.0126 | rel loss 0.0016 | total loss 0.0142\n",
      "          | val ent loss 0.0591 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 38/70 [04:42<03:58,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 | ent loss 0.0184 | rel loss 0.0014 | total loss 0.0198\n",
      "          | val ent loss 0.0548 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 39/70 [04:50<03:50,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 | ent loss 0.0169 | rel loss 0.0018 | total loss 0.0187\n",
      "          | val ent loss 0.0606 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 40/70 [04:57<03:42,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 | ent loss 0.0246 | rel loss 0.0013 | total loss 0.0260\n",
      "          | val ent loss 0.0619 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 41/70 [05:03<03:34,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 | ent loss 0.0164 | rel loss 0.0014 | total loss 0.0178\n",
      "          | val ent loss 0.0671 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 42/70 [05:10<03:27,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 | ent loss 0.0110 | rel loss 0.0013 | total loss 0.0123\n",
      "          | val ent loss 0.0646 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████▏   | 43/70 [05:17<03:19,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 | ent loss 0.0063 | rel loss 0.0012 | total loss 0.0074\n",
      "          | val ent loss 0.0629 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 44/70 [05:25<03:12,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 | ent loss 0.0134 | rel loss 0.0012 | total loss 0.0147\n",
      "          | val ent loss 0.0786 | val rel loss 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 45/70 [05:33<03:05,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 | ent loss 0.0055 | rel loss 0.0012 | total loss 0.0066\n",
      "          | val ent loss 0.0633 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 46/70 [05:39<02:57,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 | ent loss 0.0130 | rel loss 0.0014 | total loss 0.0144\n",
      "          | val ent loss 0.0493 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 47/70 [05:47<02:50,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 | ent loss 0.0087 | rel loss 0.0013 | total loss 0.0100\n",
      "          | val ent loss 0.0533 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 48/70 [05:55<02:42,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 | ent loss 0.0058 | rel loss 0.0011 | total loss 0.0069\n",
      "          | val ent loss 0.0444 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 49/70 [06:02<02:35,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 | ent loss 0.0054 | rel loss 0.0009 | total loss 0.0063\n",
      "          | val ent loss 0.0469 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 50/70 [06:09<02:27,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 | ent loss 0.0072 | rel loss 0.0011 | total loss 0.0083\n",
      "          | val ent loss 0.0348 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 51/70 [06:17<02:20,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 | ent loss 0.0096 | rel loss 0.0010 | total loss 0.0106\n",
      "          | val ent loss 0.0678 | val rel loss 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 52/70 [06:24<02:13,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 | ent loss 0.0061 | rel loss 0.0011 | total loss 0.0072\n",
      "          | val ent loss 0.0760 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 53/70 [06:32<02:05,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 52 | ent loss 0.0122 | rel loss 0.0014 | total loss 0.0136\n",
      "          | val ent loss 0.0424 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 54/70 [06:40<01:58,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53 | ent loss 0.0096 | rel loss 0.0010 | total loss 0.0106\n",
      "          | val ent loss 0.0379 | val rel loss 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 55/70 [06:46<01:50,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 54 | ent loss 0.0082 | rel loss 0.0010 | total loss 0.0092\n",
      "          | val ent loss 0.0639 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 56/70 [06:54<01:43,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55 | ent loss 0.0040 | rel loss 0.0009 | total loss 0.0049\n",
      "          | val ent loss 0.0564 | val rel loss 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 57/70 [07:02<01:36,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 56 | ent loss 0.0039 | rel loss 0.0009 | total loss 0.0049\n",
      "          | val ent loss 0.0472 | val rel loss 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 58/70 [07:10<01:29,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57 | ent loss 0.0100 | rel loss 0.0011 | total loss 0.0111\n",
      "          | val ent loss 0.0548 | val rel loss 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 59/70 [07:17<01:21,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 58 | ent loss 0.0085 | rel loss 0.0013 | total loss 0.0097\n",
      "          | val ent loss 0.0732 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 60/70 [07:25<01:14,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 59 | ent loss 0.0033 | rel loss 0.0010 | total loss 0.0043\n",
      "          | val ent loss 0.0575 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 61/70 [07:33<01:06,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 | ent loss 0.0049 | rel loss 0.0013 | total loss 0.0062\n",
      "          | val ent loss 0.0530 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 62/70 [07:41<00:59,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61 | ent loss 0.0074 | rel loss 0.0010 | total loss 0.0084\n",
      "          | val ent loss 0.0440 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 63/70 [07:48<00:52,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 62 | ent loss 0.0053 | rel loss 0.0011 | total loss 0.0064\n",
      "          | val ent loss 0.0715 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████▏| 64/70 [07:55<00:44,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 63 | ent loss 0.0119 | rel loss 0.0010 | total loss 0.0130\n",
      "          | val ent loss 0.0665 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 65/70 [08:02<00:37,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 64 | ent loss 0.0070 | rel loss 0.0009 | total loss 0.0079\n",
      "          | val ent loss 0.0248 | val rel loss 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 66/70 [08:09<00:29,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 65 | ent loss 0.0060 | rel loss 0.0013 | total loss 0.0073\n",
      "          | val ent loss 0.0429 | val rel loss 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 67/70 [08:16<00:22,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 66 | ent loss 0.0078 | rel loss 0.0010 | total loss 0.0088\n",
      "          | val ent loss 0.0585 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 68/70 [08:24<00:14,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 67 | ent loss 0.0032 | rel loss 0.0010 | total loss 0.0041\n",
      "          | val ent loss 0.0634 | val rel loss 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▊| 69/70 [08:31<00:07,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68 | ent loss 0.0034 | rel loss 0.0011 | total loss 0.0045\n",
      "          | val ent loss 0.0798 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 70/70 [08:40<00:00,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 69 | ent loss 0.0099 | rel loss 0.0010 | total loss 0.0109\n",
      "          | val ent loss 0.0577 | val rel loss 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 70\n",
    "print_every = 12\n",
    "all_ent_loss = []\n",
    "all_rel_loss = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "loss = 0\n",
    "ent_loss = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(range(n_iters)):  \n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        batch_ent = batch_ent.view(BATCH_SIZE*MAX_LEN)\n",
    "        batch_rel = batch_rel.view(BATCH_SIZE*MAX_LEN*MAX_LEN)\n",
    "        \n",
    "        loss_ent = criterion_tag(ent_output, batch_ent.cuda() if USE_CUDA else batch_ent)\n",
    "        loss_rel = criterion_rel(rel_output, batch_rel.cuda() if USE_CUDA else batch_rel)\n",
    "        loss = loss_ent+loss_rel\n",
    "        \n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % print_every == 1:\n",
    "            all_ent_loss.append(loss_ent.cpu())\n",
    "            all_rel_loss.append(loss_rel.cpu())\n",
    "        #    print('%.4f| epoch: %d| step: %d| %s' % (loss, epoch, step, timeSince(start)))\n",
    "        \n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(dev_loader):\n",
    "        model.eval()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        val_loss_ent = criterion_tag(ent_output.cpu(), batch_ent.view(BATCH_SIZE*MAX_LEN)) \n",
    "        val_loss_rel = criterion_rel(rel_output.cpu(), batch_rel.view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    \n",
    "    print(\"epoch: %d | ent loss %.4f | rel loss %.4f | total loss %.4f\" \\\n",
    "          % (epoch, loss_ent, loss_rel, loss))\n",
    "    print(\"      %s  | val ent loss %.4f | val rel loss %.4f\"\n",
    "          % (\" \"*len(str(epoch)), val_loss_ent, val_loss_rel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_ent_loss[:100], label='ent_loss')\n",
    "plt.plot(all_rel_loss[:100], label='rel_loss')\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\") \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_choose(input_var):\n",
    "    r_choose = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        r_choose.append(random.randint(0,len(input_var)))\n",
    "    return r_choose\n",
    "        \n",
    "def ent_argmax(output):\n",
    "    output = output.view(BATCH_SIZE,MAX_LEN,ent_size).argmax(2)\n",
    "    return output\n",
    "\n",
    "def rel_argmax(output):\n",
    "    output = output.view(BATCH_SIZE,MAX_LEN,MAX_LEN,rel_size).argmax(3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "predict : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "true : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "===================================================\n",
      "\n",
      "\n",
      "predict : ['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "true : ['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Entity loss : 0.0064\n",
      "Relation loss : 0.0009\n"
     ]
    }
   ],
   "source": [
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    r_choose = random_choose(input_var)\n",
    "    model.eval()\n",
    "    ent_output, rel_output = model(input_var[r_choose].cuda() if USE_CUDA else input_var)\n",
    "    \n",
    "    ent_loss = criterion_tag(ent_output.cpu(), ent_var[r_choose].view(BATCH_SIZE*MAX_LEN))\n",
    "    ent_output = ent_argmax(ent_output)\n",
    "    \n",
    "    rel_loss = criterion_rel(rel_output.cpu(), rel_var[r_choose].view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    \n",
    "#     print('predict :', ent_output[0])\n",
    "#     print('true :', ent_var[r_choose[0]])\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[0], ix_to_ent_tag))\n",
    "    print('true :', index2tag(ent_var[r_choose[0]], ix_to_ent_tag))\n",
    "    print()\n",
    "    print('===================================================')\n",
    "    print()\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[1], ix_to_ent_tag))\n",
    "    print('true :', index2tag(ent_var[r_choose[1]], ix_to_ent_tag))\n",
    "    \n",
    "    print()\n",
    "    print(\"Entity loss : %.4f\" % ent_loss)\n",
    "    print(\"Relation loss : %.4f\" % rel_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "\n",
      "predict : ['O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "true : ['O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "\n",
      "Entity loss : 0.0552\n",
      "Relation loss : 0.0056\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    r_choose = random_choose(input_dev)\n",
    "    model.eval()\n",
    "    ent_output, rel_output = model(input_dev[r_choose].cuda() if USE_CUDA else input_dev)\n",
    "    \n",
    "    ent_loss = criterion_tag(ent_output.cpu(), ent_dev[r_choose].view(BATCH_SIZE*MAX_LEN))\n",
    "    ent_output = ent_argmax(ent_output)\n",
    "    \n",
    "    rel_loss = criterion_rel(rel_output.cpu(), rel_var[r_choose].view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "    \n",
    "    print(r_choose[0])\n",
    "    print()\n",
    "    print('predict :', index2tag(ent_output[0], ix_to_ent_tag))\n",
    "    print()\n",
    "    print('true :', index2tag(ent_dev[r_choose[0]], ix_to_ent_tag))\n",
    "    print()\n",
    "\n",
    "    print(\"Entity loss : %.4f\" % ent_loss)\n",
    "    print(\"Relation loss : %.4f\" % rel_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entity loss : 0.0774\n",
      "Relation loss : 0.0012\n",
      "\n",
      "['我', '的', '皮', '膚', '是', '乾', '性', '肌', ',', '雖', '然', '在', '夏', '天', '不', '會', '乾', '到', '緊', '繃', ',', '但', '每', '天', '還', '是', '要', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['極', '效', '肌', '因', '修', '護', '巨', '藻', '、', '齒', '缘', '墨', '角', '藻', '複', '合', '水', '解', '酵', '母', '提', '取', '物', '，', '浸', '透', '濕', '潤', '乾', '燥', '肌', '膚', '，', '增', '強', '防', '護', '屏', '障', '，', '協', '同', '蘆', '薈', '、', '山', '金', '車', '及', '光', '果', '甘', '草', '植', '萃', '精', '華', '，', '深', '度', '安', '撫', '同', '時', '舒', '緩', '肌', '膚', '的', '不', '適', '，', '拋', '開', '惱', '人', '的', '乾', '燥', '缺', '水', '不', '安', '讓', '肌', '膚', '穩', '定']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['使', '用', '完', '後', '肌', '膚', '乾', '淨', '舒', '服', ',', '休', '息', '一', '下', '後', '就', '可', '以', '上', '平', '日', '保', '養', '步', '驟', '~']\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O']\n",
      "['', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['高', '效', '保', '濕', '，', '舒', '緩', '肌', '膚', '，', '溫', '和', '修', '復', '，', '重', '塑', '晶', '透', '的', '的', '亮', '顏', '全', '效', '保', '濕', '，', '改', '善', '乾', '性', '肌', '膚', '問', '題', '，', '使', '肌', '膚', '亮', '透', '有', '光', '澤']\n",
      "['O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['露', '珠', '草', '萃', '取', '液', '晶', '華', '能', '舒', '緩', '肌', '膚', '乾', '燥', '不', '適', '並', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['像', '我', '本', '身', '有', '輕', '微', '過', '敏', '膚', '質', '的', '情', '況', '，', '只', '要', '在', '悶', '熱', '的', '環', '境', '中', '肌', '膚', '就', '容', '易', '產', '生', '泛', '紅', '的', '情', '形', '，', '這', '幾', '天', '都', '靠', '蠟', '菊', '純', '露', '面', '膜', '來', '舒', '緩', '肌', '膚']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', [], [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['洗', '完', '臉', '後', '，', '我', '將', '「', '我', '的', '美', '麗', '日', '記', '」', '南', '極', '冰', '河', '醣', '蛋', '白', '滲', '透', '保', '濕', '面', '膜', '敷', '在', '臉', '上', '，', '經', '過', '2', '0', '分', '鐘', '後', '，', '將', '面', '膜', '從', '臉', '部', '由', '下', '往', '上', '撕', '下', '，', '再', '用', '清', '水', '將', '面', '膜', '清', '洗', '乾', '淨', '後', '，', '從', '鏡', '子', '中', '我', '看', '到', '臉', '部', '肌', '膚', '乾', '燥', '缺', '水', '的', '現', '象', '消', '失', '了']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['夏', '日', '將', '到', '了', '，', '保', '濕', '不', '是', '只', '有', '在', '冬', '天', '，', '夏', '天', '會', '常', '常', '待', '在', '冷', '氣', '房', '，', '所', '以', '保', '濕', '還', '是', '很', '重', '要', '，', '而', '這', '面', '膜', '對', '於', '乾', '燥', '的', '肌', '膚', '急', '救', '補', '水', '效', '果', '還', '挺', '讓', '人', '滿', '意', '的']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', [], [], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['這', '款', '面', '膜', '它', '強', '調', '乾', '肌', '及', '肌', '膚', '鬆', '弛', '.', '.', '.', '等', ',', '但', '我', '發', '現', '緊', '緻', '的', '部', '份', '不', '是', '那', '麼', '明', '顯', ',', '但', '保', '濕', '部', '分', '我', '倒', '是', '很']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['保', '濕', '效', '果', '超', '優', '的', '，', '暗', '沉', '和', '乾', '燥', '肌', '也', '統', '統', 'O', 'U', 'T', '了', '，']\n",
      "['B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['而', '面', '膜', '的', '精', '華', '液', '不', '算', '多', '，', '但', '也', '很', '夠', '用', '了', '，', '是', '敷', '上', '之', '後', '不', '會', '滴', '下', '的', '量', '，', '剛', '剛', '好', '，', '精', '華', '液', '本', '身', '沒', '什', '麼', '香', '味', '，', '我', '這', '個', '小', '敏', '感', '的', '油', '性', '肌', '老', '實', '說', '敷', '完', '之', '後', '卻', '什', '麼', '感', '覺', '都', '沒', '有', '，', '沒', '有', '過', '敏', '、', '沒', '有', '感', '覺', '特', '別', '保', '水', '、', '緊', '緻', '.', '.', '.', '等']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['這', '次', '試', '用', '的', '是', '\"', '南', '極', '冰', '河', '醣', '蛋', '白', '滲', '透', '保', '濕', '面', '膜', '\"', '，', '對', '於', '乾', '肌', '的', '我', '來', '說', '，', '再', '適', '合', '不', '過', '了']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', '酵', '母', '萃', '取', '”', '搭', '配', '“', '小', '分', '子', '玻', '尿', '酸', '”', '及', '“', '牛', '奶', '蛋', '白', '”', '使', '乾', '燥', '肌', '膚', '充', '滿', '水', '分', '，', '長', '效', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['當', '天', '擦', '就', '馬', '上', '覺', '得', '皮', '膚', '乾', '燥', '感', '得', '到', '舒', '緩']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['增', '加', '肌', '膚', '彈', '性', '及', '含', '水', '量', '，', '提', '高', '保', '濕', '效', '果', '，', '防', '止', '肌', '膚', '乾', '燥', '，', '去', '除', '細', '小', '皺', '紋', '，']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['“', '酵', '母', '萃', '取', '”', '搭', '配', '“', '小', '分', '子', '玻', '尿', '酸', '”', '及', '“', '牛', '奶', '蛋', '白', '”', '使', '乾', '燥', '肌', '膚', '充', '滿', '水', '分', '，', '長', '效', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['不', '過', '每', '次', '敷', '完', '臉', ',', '保', '濕', '的', '效', '果', '非', '常', '好', ',', '因', '為', '是', '敏', '感', '膚', '質', ',', '有', '時', '候', '臉', '上', '會', '有', '過', '敏', '現', '象', ',']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['第', '三', '種', '方', '式', '是', '每', '週', '1', '-', '2', '次', '，', '夜', '間', '所', '有', '保', '養', '程', '序', '後', '，', '以', '厚', '敷', '來', '加', '強', '肌', '膚', '乾', '燥', '部', '位', '的', '照', '顧']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['我 O ', '的 O ', '皮 O ', '膚 O ', '是 O ', '乾 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', ', O ', '雖 O ', '然 O ', '在 O ', '夏 O ', '天 O ', '不 O ', '會 O ', '乾 O ', '到 O ', '緊 O ', '繃 O ', ', O ', '但 O ', '每 O ', '天 O ', '還 O ', '是 O ', '要 O ', '加 O ', '強 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', ' ', '極 O ', '效 O ', '肌 O ', '因 O ', '修 O ', '護 O ', '巨 O ', '藻 O ', '、 O ', '齒 O ', '缘 O ', '墨 O ', '角 O ', '藻 O ', '複 O ', '合 O ', '水 O ', '解 O ', '酵 O ', '母 O ', '提 O ', '取 O ', '物 O ', '， O ', '浸 O ', '透 O ', '濕 O ', '潤 O ', '乾 B-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '， O ', '增 O ', '強 O ', '防 O ', '護 O ', '屏 O ', '障 O ', '， O ', '協 O ', '同 O ', '蘆 O ', '薈 O ', '、 O ', '山 O ', '金 O ', '車 O ', '及 O ', '光 O ', '果 O ', '甘 O ', '草 O ', '植 O ', '萃 O ', '精 O ', '華 O ', '， O ', '深 O ', '度 O ', '安 O ', '撫 O ', '同 O ', '時 O ', '舒 B-FUNC ApplyTo-0-A ', '緩 I-FUNC ApplyTo-0-A ', '肌 O ', '膚 O ', '的 O ', '不 O ', '適 O ', '， O ', '拋 O ', '開 O ', '惱 O ', '人 O ', '的 O ', '乾 B-STAT ', '燥 I-STAT ', '缺 O ', '水 O ', '不 O ', '安 O ', '讓 O ', '肌 O ', '膚 O ', '穩 O ', '定 O ', ' ', '使 O ', '用 O ', '完 O ', '後 O ', '肌 B-STAT ', '膚 I-STAT ', '乾 I-STAT ', '淨 O ', '舒 O ', '服 O ', ', O ', '休 O ', '息 O ', '一 O ', '下 O ', '後 O ', '就 O ', '可 O ', '以 O ', '上 O ', '平 O ', '日 O ', '保 B-FUNC ', '養 I-FUNC ', '步 O ', '驟 O ', '~ O ', ' ', '高 O ', '效 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '， O ', '舒 B-FUNC ', '緩 I-FUNC ', '肌 O ', '膚 O ', '， O ', '溫 O ', '和 O ', '修 O ', '復 O ', '， O ', '重 O ', '塑 O ', '晶 O ', '透 O ', '的 O ', '的 O ', '亮 O ', '顏 O ', '全 O ', '效 O ', '保 B-FUNC ', '濕 I-FUNC ', '， O ', '改 O ', '善 O ', '乾 B-STAT ', '性 I-STAT ', '肌 I-STAT ', '膚 I-STAT ', '問 O ', '題 O ', '， O ', '使 O ', '肌 O ', '膚 O ', '亮 O ', '透 O ', '有 O ', '光 O ', '澤 O ', ' ', '露 O ', '珠 O ', '草 O ', '萃 O ', '取 O ', '液 O ', '晶 O ', '華 O ', '能 O ', '舒 B-FUNC ApplyTo-0-A ', '緩 I-FUNC ApplyTo-0-A ', '肌 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '不 O ', '適 O ', '並 O ', '加 O ', '強 O ', '保 B-FUNC ', '濕 I-FUNC ', ' ', '像 O ', '我 O ', '本 O ', '身 O ', '有 O ', '輕 O ', '微 O ', '過 B-STAT ApplyTo-0-B ', '敏 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '質 I-STAT ApplyTo-0-B ', '的 O ', '情 O ', '況 O ', '， O ', '只 O ', '要 O ', '在 O ', '悶 O ', '熱 O ', '的 O ', '環 O ', '境 O ', '中 B-STAT ', '肌 I-STAT ', '膚 I-STAT ', '就 O ', '容 O ', '易 O ', '產 O ', '生 O ', '泛 O ', '紅 O ', '的 O ', '情 O ', '形 O ', '， O ', '這 O ', '幾 O ', '天 O ', '都 O ', '靠 O ', '蠟 O ', '菊 O ', '純 O ', '露 O ', '面 O ', '膜 O ', '來 O ', '舒 B-FUNC ', '緩 I-FUNC ', '肌 I-FUNC ', '膚 O ', ' ', '洗 O ', '完 O ', '臉 O ', '後 O ', '， O ', '我 O ', '將 O ', '「 O ', '我 O ', '的 O ', '美 O ', '麗 O ', '日 O ', '記 O ', '」 O ', '南 O ', '極 O ', '冰 O ', '河 O ', '醣 O ', '蛋 O ', '白 O ', '滲 O ', '透 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '面 O ', '膜 O ', '敷 O ', '在 O ', '臉 O ', '上 O ', '， O ', '經 O ', '過 O ', '2 O ', '0 O ', '分 O ', '鐘 O ', '後 O ', '， O ', '將 O ', '面 O ', '膜 O ', '從 O ', '臉 O ', '部 O ', '由 O ', '下 O ', '往 O ', '上 O ', '撕 O ', '下 O ', '， O ', '再 O ', '用 O ', '清 O ', '水 O ', '將 O ', '面 O ', '膜 O ', '清 O ', '洗 O ', '乾 O ', '淨 O ', '後 O ', '， O ', '從 O ', '鏡 O ', '子 O ', '中 O ', '我 O ', '看 O ', '到 O ', '臉 O ', '部 O ', '肌 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '缺 O ', '水 O ', '的 O ', '現 O ', '象 O ', '消 O ', '失 O ', '了 O ', ' ', '夏 O ', '日 O ', '將 O ', '到 O ', '了 O ', '， O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '不 O ', '是 O ', '只 O ', '有 O ', '在 O ', '冬 O ', '天 O ', '， O ', '夏 O ', '天 O ', '會 O ', '常 O ', '常 O ', '待 O ', '在 O ', '冷 O ', '氣 O ', '房 O ', '， O ', '所 O ', '以 O ', '保 B-FUNC ', '濕 I-FUNC ', '還 O ', '是 O ', '很 O ', '重 O ', '要 O ', '， O ', '而 O ', '這 O ', '面 O ', '膜 O ', '對 O ', '於 O ', '乾 B-STAT ', '燥 I-STAT ', '的 I-STAT ', '肌 I-STAT ', '膚 I-STAT ', '急 O ', '救 O ', '補 O ', '水 O ', '效 O ', '果 O ', '還 O ', '挺 O ', '讓 O ', '人 O ', '滿 O ', '意 O ', '的 O ', ' ', '這 O ', '款 O ', '面 O ', '膜 O ', '它 O ', '強 O ', '調 O ', '乾 B-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '及 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '鬆 O ', '弛 O ', '. O ', '. O ', '. O ', '等 O ', ', O ', '但 O ', '我 O ', '發 O ', '現 O ', '緊 B-FUNC ApplyTo-0-A ', '緻 I-FUNC ApplyTo-0-A ', '的 O ', '部 O ', '份 O ', '不 O ', '是 O ', '那 O ', '麼 O ', '明 O ', '顯 O ', ', O ', '但 O ', '保 B-FUNC ', '濕 I-FUNC ', '部 O ', '分 O ', '我 O ', '倒 O ', '是 O ', '很 O ', ' ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '效 O ', '果 O ', '超 O ', '優 O ', '的 O ', '， O ', '暗 O ', '沉 O ', '和 O ', '乾 B-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '也 O ', '統 O ', '統 O ', 'O O ', 'U O ', 'T O ', '了 O ', '， O ', ' ', '而 O ', '面 O ', '膜 O ', '的 O ', '精 O ', '華 O ', '液 O ', '不 O ', '算 O ', '多 O ', '， O ', '但 O ', '也 O ', '很 O ', '夠 O ', '用 O ', '了 O ', '， O ', '是 O ', '敷 O ', '上 O ', '之 O ', '後 O ', '不 O ', '會 O ', '滴 O ', '下 O ', '的 O ', '量 O ', '， O ', '剛 O ', '剛 O ', '好 O ', '， O ', '精 O ', '華 O ', '液 O ', '本 O ', '身 O ', '沒 O ', '什 O ', '麼 O ', '香 O ', '味 O ', '， O ', '我 O ', '這 O ', '個 O ', '小 O ', '敏 O ', '感 O ', '的 O ', '油 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '老 O ', '實 O ', '說 O ', '敷 O ', '完 O ', '之 O ', '後 O ', '卻 O ', '什 O ', '麼 O ', '感 O ', '覺 O ', '都 O ', '沒 O ', '有 O ', '， O ', '沒 O ', '有 O ', '過 O ', '敏 O ', '、 O ', '沒 O ', '有 O ', '感 O ', '覺 O ', '特 O ', '別 O ', '保 O ', '水 O ', '、 O ', '緊 B-FUNC ApplyTo-0-A ', '緻 I-FUNC ApplyTo-0-A ', '. O ', '. O ', '. O ', '等 O ', ' ', '這 O ', '次 O ', '試 O ', '用 O ', '的 O ', '是 O ', '\" O ', '南 O ', '極 O ', '冰 O ', '河 O ', '醣 O ', '蛋 O ', '白 O ', '滲 O ', '透 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '面 O ', '膜 O ', '\" O ', '， O ', '對 O ', '於 O ', '乾 B-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '的 O ', '我 O ', '來 O ', '說 O ', '， O ', '再 O ', '適 O ', '合 O ', '不 O ', '過 O ', '了 O ', ' ', '“ O ', '酵 O ', '母 O ', '萃 O ', '取 O ', '” O ', '搭 O ', '配 O ', '“ O ', '小 O ', '分 O ', '子 O ', '玻 O ', '尿 O ', '酸 O ', '” O ', '及 O ', '“ O ', '牛 O ', '奶 O ', '蛋 O ', '白 O ', '” O ', '使 O ', '乾 B-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '充 O ', '滿 O ', '水 O ', '分 O ', '， O ', '長 O ', '效 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', ' ', '當 O ', '天 O ', '擦 O ', '就 O ', '馬 O ', '上 O ', '覺 O ', '得 O ', '皮 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '得 O ', '到 O ', '舒 B-FUNC ApplyTo-0-A ', '緩 I-FUNC ApplyTo-0-A ', ' ', '增 O ', '加 O ', '肌 O ', '膚 O ', '彈 O ', '性 O ', '及 O ', '含 O ', '水 O ', '量 O ', '， O ', '提 O ', '高 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '效 O ', '果 O ', '， O ', '防 O ', '止 O ', '肌 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '， O ', '去 O ', '除 O ', '細 O ', '小 O ', '皺 O ', '紋 O ', '， O ', ' ', '“ O ', '酵 O ', '母 O ', '萃 O ', '取 O ', '” O ', '搭 O ', '配 O ', '“ O ', '小 O ', '分 O ', '子 O ', '玻 O ', '尿 O ', '酸 O ', '” O ', '及 O ', '“ O ', '牛 O ', '奶 O ', '蛋 O ', '白 O ', '” O ', '使 O ', '乾 B-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '充 O ', '滿 O ', '水 O ', '分 O ', '， O ', '長 O ', '效 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', ' ', '不 O ', '過 O ', '每 O ', '次 O ', '敷 O ', '完 O ', '臉 O ', ', O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '的 O ', '效 O ', '果 O ', '非 O ', '常 O ', '好 O ', ', O ', '因 O ', '為 O ', '是 O ', '敏 B-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '質 I-STAT ApplyTo-0-B ', ', O ', '有 O ', '時 O ', '候 O ', '臉 O ', '上 O ', '會 O ', '有 O ', '過 O ', '敏 O ', '現 O ', '象 O ', ', O ', ' ', '第 O ', '三 O ', '種 O ', '方 O ', '式 O ', '是 O ', '每 O ', '週 O ', '1 O ', '- O ', '2 O ', '次 O ', '， O ', '夜 O ', '間 O ', '所 O ', '有 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '程 O ', '序 O ', '後 O ', '， O ', '以 O ', '厚 O ', '敷 O ', '來 O ', '加 O ', '強 O ', '肌 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '部 O ', '位 O ', '的 O ', '照 O ', '顧 O ', ' ']\n",
      "['我 O ' '的 O ' '皮 O ' '膚 O ' '是 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '性 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' ', O ' '雖 O ' '然 O '\n",
      " '在 O ' '夏 O ' '天 O ' '不 O ' '會 O ' '乾 O ' '到 O ' '緊 O ' '繃 O ' ', O '\n",
      " '但 O ' '每 O ' '天 O ' '還 O ' '是 O ' '要 O ' '加 O ' '強 O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' ' ' '極 O ' '效 O ' '肌 O '\n",
      " '因 O ' '修 O ' '護 O ' '巨 O ' '藻 O ' '、 O ' '齒 O ' '缘 O ' '墨 O ' '角 O '\n",
      " '藻 O ' '複 O ' '合 O ' '水 O ' '解 O ' '酵 O ' '母 O ' '提 O ' '取 O ' '物 O '\n",
      " '， O ' '浸 O ' '透 O ' '濕 O ' '潤 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '燥 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B '\n",
      " '， O ' '增 O ' '強 O ' '防 O ' '護 O ' '屏 O ' '障 O ' '， O ' '協 O ' '同 O '\n",
      " '蘆 O ' '薈 O ' '、 O ' '山 O ' '金 O ' '車 O ' '及 O ' '光 O ' '果 O ' '甘 O '\n",
      " '草 O ' '植 O ' '萃 O ' '精 O ' '華 O ' '， O ' '深 O ' '度 O ' '安 O ' '撫 O '\n",
      " '同 O ' '時 O ' '舒 B-FUNC ApplyTo-0-A ' '緩 I-FUNC ApplyTo-0-A ' '肌 O '\n",
      " '膚 O ' '的 O ' '不 O ' '適 O ' '， O ' '拋 O ' '開 O ' '惱 O ' '人 O ' '的 O '\n",
      " '乾 B-STAT ' '燥 I-STAT ' '缺 O ' '水 O ' '不 O ' '安 O ' '讓 O ' '肌 O ' '膚 O '\n",
      " '穩 O ' '定 O ' ' ' '使 O ' '用 O ' '完 O ' '後 O ' '肌 B-STAT ' '膚 I-STAT '\n",
      " '乾 I-STAT ' '淨 O ' '舒 O ' '服 O ' ', O ' '休 O ' '息 O ' '一 O ' '下 O '\n",
      " '後 O ' '就 O ' '可 O ' '以 O ' '上 O ' '平 O ' '日 O ' '保 B-FUNC ' '養 I-FUNC '\n",
      " '步 O ' '驟 O ' '~ O ' ' ' '高 O ' '效 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' '， O ' '舒 B-FUNC ' '緩 I-FUNC ' '肌 O ' '膚 O '\n",
      " '， O ' '溫 O ' '和 O ' '修 O ' '復 O ' '， O ' '重 O ' '塑 O ' '晶 O ' '透 O '\n",
      " '的 O ' '的 O ' '亮 O ' '顏 O ' '全 O ' '效 O ' '保 B-FUNC ' '濕 I-FUNC ' '， O '\n",
      " '改 O ' '善 O ' '乾 B-STAT ' '性 I-STAT ' '肌 I-STAT ' '膚 I-STAT ' '問 O '\n",
      " '題 O ' '， O ' '使 O ' '肌 O ' '膚 O ' '亮 O ' '透 O ' '有 O ' '光 O ' '澤 O ' ' '\n",
      " '露 O ' '珠 O ' '草 O ' '萃 O ' '取 O ' '液 O ' '晶 O ' '華 O ' '能 O '\n",
      " '舒 B-FUNC ApplyTo-0-A ' '緩 I-FUNC ApplyTo-0-A ' '肌 B-STAT ApplyTo-0-B '\n",
      " '膚 I-STAT ApplyTo-0-B ' '乾 I-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B '\n",
      " '不 O ' '適 O ' '並 O ' '加 O ' '強 O ' '保 B-FUNC ' '濕 I-FUNC ' ' ' '像 O '\n",
      " '我 O ' '本 O ' '身 O ' '有 O ' '輕 O ' '微 O ' '過 B-STAT ApplyTo-0-B '\n",
      " '敏 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '質 I-STAT ApplyTo-0-B '\n",
      " '的 O ' '情 O ' '況 O ' '， O ' '只 O ' '要 O ' '在 O ' '悶 O ' '熱 O ' '的 O '\n",
      " '環 O ' '境 O ' '中 B-STAT ' '肌 I-STAT ' '膚 I-STAT ' '就 O ' '容 O ' '易 O '\n",
      " '產 O ' '生 O ' '泛 O ' '紅 O ' '的 O ' '情 O ' '形 O ' '， O ' '這 O ' '幾 O '\n",
      " '天 O ' '都 O ' '靠 O ' '蠟 O ' '菊 O ' '純 O ' '露 O ' '面 O ' '膜 O ' '來 O '\n",
      " '舒 B-FUNC ' '緩 I-FUNC ' '肌 I-FUNC ' '膚 O ' ' ' '洗 O ' '完 O ' '臉 O '\n",
      " '後 O ' '， O ' '我 O ' '將 O ' '「 O ' '我 O ' '的 O ' '美 O ' '麗 O ' '日 O '\n",
      " '記 O ' '」 O ' '南 O ' '極 O ' '冰 O ' '河 O ' '醣 O ' '蛋 O ' '白 O ' '滲 O '\n",
      " '透 O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '面 O ' '膜 O '\n",
      " '敷 O ' '在 O ' '臉 O ' '上 O ' '， O ' '經 O ' '過 O ' '2 O ' '0 O ' '分 O '\n",
      " '鐘 O ' '後 O ' '， O ' '將 O ' '面 O ' '膜 O ' '從 O ' '臉 O ' '部 O ' '由 O '\n",
      " '下 O ' '往 O ' '上 O ' '撕 O ' '下 O ' '， O ' '再 O ' '用 O ' '清 O ' '水 O '\n",
      " '將 O ' '面 O ' '膜 O ' '清 O ' '洗 O ' '乾 O ' '淨 O ' '後 O ' '， O ' '從 O '\n",
      " '鏡 O ' '子 O ' '中 O ' '我 O ' '看 O ' '到 O ' '臉 O ' '部 O '\n",
      " '肌 B-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '乾 I-STAT ApplyTo-0-B '\n",
      " '燥 I-STAT ApplyTo-0-B ' '缺 O ' '水 O ' '的 O ' '現 O ' '象 O ' '消 O ' '失 O '\n",
      " '了 O ' ' ' '夏 O ' '日 O ' '將 O ' '到 O ' '了 O ' '， O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '不 O ' '是 O ' '只 O '\n",
      " '有 O ' '在 O ' '冬 O ' '天 O ' '， O ' '夏 O ' '天 O ' '會 O ' '常 O ' '常 O '\n",
      " '待 O ' '在 O ' '冷 O ' '氣 O ' '房 O ' '， O ' '所 O ' '以 O ' '保 B-FUNC '\n",
      " '濕 I-FUNC ' '還 O ' '是 O ' '很 O ' '重 O ' '要 O ' '， O ' '而 O ' '這 O '\n",
      " '面 O ' '膜 O ' '對 O ' '於 O ' '乾 B-STAT ' '燥 I-STAT ' '的 I-STAT '\n",
      " '肌 I-STAT ' '膚 I-STAT ' '急 O ' '救 O ' '補 O ' '水 O ' '效 O ' '果 O ' '還 O '\n",
      " '挺 O ' '讓 O ' '人 O ' '滿 O ' '意 O ' '的 O ' ' ' '這 O ' '款 O ' '面 O ' '膜 O '\n",
      " '它 O ' '強 O ' '調 O ' '乾 B-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B '\n",
      " '及 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B '\n",
      " '鬆 O ' '弛 O ' '. O ' '. O ' '. O ' '等 O ' ', O ' '但 O ' '我 O ' '發 O '\n",
      " '現 O ' '緊 B-FUNC ApplyTo-0-A ' '緻 I-FUNC ApplyTo-0-A ' '的 O ' '部 O '\n",
      " '份 O ' '不 O ' '是 O ' '那 O ' '麼 O ' '明 O ' '顯 O ' ', O ' '但 O '\n",
      " '保 B-FUNC ' '濕 I-FUNC ' '部 O ' '分 O ' '我 O ' '倒 O ' '是 O ' '很 O ' ' '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '效 O ' '果 O ' '超 O '\n",
      " '優 O ' '的 O ' '， O ' '暗 O ' '沉 O ' '和 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '燥 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '也 O ' '統 O ' '統 O '\n",
      " 'O O ' 'U O ' 'T O ' '了 O ' '， O ' ' ' '而 O ' '面 O ' '膜 O ' '的 O ' '精 O '\n",
      " '華 O ' '液 O ' '不 O ' '算 O ' '多 O ' '， O ' '但 O ' '也 O ' '很 O ' '夠 O '\n",
      " '用 O ' '了 O ' '， O ' '是 O ' '敷 O ' '上 O ' '之 O ' '後 O ' '不 O ' '會 O '\n",
      " '滴 O ' '下 O ' '的 O ' '量 O ' '， O ' '剛 O ' '剛 O ' '好 O ' '， O ' '精 O '\n",
      " '華 O ' '液 O ' '本 O ' '身 O ' '沒 O ' '什 O ' '麼 O ' '香 O ' '味 O ' '， O '\n",
      " '我 O ' '這 O ' '個 O ' '小 O ' '敏 O ' '感 O ' '的 O ' '油 B-STAT ApplyTo-0-B '\n",
      " '性 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '老 O ' '實 O ' '說 O '\n",
      " '敷 O ' '完 O ' '之 O ' '後 O ' '卻 O ' '什 O ' '麼 O ' '感 O ' '覺 O ' '都 O '\n",
      " '沒 O ' '有 O ' '， O ' '沒 O ' '有 O ' '過 O ' '敏 O ' '、 O ' '沒 O ' '有 O '\n",
      " '感 O ' '覺 O ' '特 O ' '別 O ' '保 O ' '水 O ' '、 O ' '緊 B-FUNC ApplyTo-0-A '\n",
      " '緻 I-FUNC ApplyTo-0-A ' '. O ' '. O ' '. O ' '等 O ' ' ' '這 O ' '次 O '\n",
      " '試 O ' '用 O ' '的 O ' '是 O ' '\" O ' '南 O ' '極 O ' '冰 O ' '河 O ' '醣 O '\n",
      " '蛋 O ' '白 O ' '滲 O ' '透 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' '面 O ' '膜 O ' '\" O ' '， O ' '對 O ' '於 O '\n",
      " '乾 B-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '的 O ' '我 O ' '來 O '\n",
      " '說 O ' '， O ' '再 O ' '適 O ' '合 O ' '不 O ' '過 O ' '了 O ' ' ' '“ O ' '酵 O '\n",
      " '母 O ' '萃 O ' '取 O ' '” O ' '搭 O ' '配 O ' '“ O ' '小 O ' '分 O ' '子 O '\n",
      " '玻 O ' '尿 O ' '酸 O ' '” O ' '及 O ' '“ O ' '牛 O ' '奶 O ' '蛋 O ' '白 O '\n",
      " '” O ' '使 O ' '乾 B-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '充 O ' '滿 O ' '水 O '\n",
      " '分 O ' '， O ' '長 O ' '效 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' ' ' '當 O ' '天 O ' '擦 O ' '就 O ' '馬 O ' '上 O '\n",
      " '覺 O ' '得 O ' '皮 B-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B '\n",
      " '乾 I-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B ' '感 I-STAT ApplyTo-0-B '\n",
      " '得 O ' '到 O ' '舒 B-FUNC ApplyTo-0-A ' '緩 I-FUNC ApplyTo-0-A ' ' ' '增 O '\n",
      " '加 O ' '肌 O ' '膚 O ' '彈 O ' '性 O ' '及 O ' '含 O ' '水 O ' '量 O ' '， O '\n",
      " '提 O ' '高 O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '效 O '\n",
      " '果 O ' '， O ' '防 O ' '止 O ' '肌 B-STAT ApplyTo-0-B '\n",
      " '膚 I-STAT ApplyTo-0-B ' '乾 I-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B '\n",
      " '， O ' '去 O ' '除 O ' '細 O ' '小 O ' '皺 O ' '紋 O ' '， O ' ' ' '“ O ' '酵 O '\n",
      " '母 O ' '萃 O ' '取 O ' '” O ' '搭 O ' '配 O ' '“ O ' '小 O ' '分 O ' '子 O '\n",
      " '玻 O ' '尿 O ' '酸 O ' '” O ' '及 O ' '“ O ' '牛 O ' '奶 O ' '蛋 O ' '白 O '\n",
      " '” O ' '使 O ' '乾 B-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '充 O ' '滿 O ' '水 O '\n",
      " '分 O ' '， O ' '長 O ' '效 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' ' ' '不 O ' '過 O ' '每 O ' '次 O ' '敷 O ' '完 O '\n",
      " '臉 O ' ', O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '的 O '\n",
      " '效 O ' '果 O ' '非 O ' '常 O ' '好 O ' ', O ' '因 O ' '為 O ' '是 O '\n",
      " '敏 B-STAT ApplyTo-0-B ' '感 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B '\n",
      " '質 I-STAT ApplyTo-0-B ' ', O ' '有 O ' '時 O ' '候 O ' '臉 O ' '上 O ' '會 O '\n",
      " '有 O ' '過 O ' '敏 O ' '現 O ' '象 O ' ', O ' ' ' '第 O ' '三 O ' '種 O ' '方 O '\n",
      " '式 O ' '是 O ' '每 O ' '週 O ' '1 O ' '- O ' '2 O ' '次 O ' '， O ' '夜 O '\n",
      " '間 O ' '所 O ' '有 O ' '保 B-FUNC ApplyTo-0-A ' '養 I-FUNC ApplyTo-0-A '\n",
      " '程 O ' '序 O ' '後 O ' '， O ' '以 O ' '厚 O ' '敷 O ' '來 O ' '加 O ' '強 O '\n",
      " '肌 B-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '乾 I-STAT ApplyTo-0-B '\n",
      " '燥 I-STAT ApplyTo-0-B ' '部 O ' '位 O ' '的 O ' '照 O ' '顧 O ' ' ']\n",
      "=================\n",
      "(0.8989284683570398, 0.8916269841269842, 0.8902825742982753, None)\n",
      "\n",
      "==========================\n",
      "\n",
      "\n",
      "Entity loss : 0.0621\n",
      "Relation loss : 0.0009\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以', '前', '我', '有', '敷', '過', '別', '款', '的', '美', '白', '面', '膜', '~', '很', '容', '易', '就', '會', '讓', '我', '的', '臉', '部', '肌', '膚', '過', '敏', '，', '會', '有', '紅', '紅', '的', '臉', '出', '現']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['以', '上', '是', '我', '不', '會', '再', '使', '用', '此', '產', '品', '，', '也', '不', '會', '推', '薦', '親', '友', '使', '用', '的', '原', '因', '，', '最', '大', '原', '因', '是', '因', '為', '；', '居', '然', '在', '使', '用', '後', '，', '肌', '膚', '反', '而', '變', '得', '乾', '到', '發', '癢', '，', '可', '見', '產', '品', '若', '無', '法', '真', '正', '提', '供', '乾', '性', '肌', '膚', '滋', '潤', '和', '保', '濕', '的', '話', '，', '添', '加', '再', '多', '吸', '引', '消', '費', '者', '的', '成', '分', '也', '不', '見', '得', '能', '得', '到', '消', '費', '者', '的', '心']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['適', '用', '族', '群', '為', '乾', '燥', '肌', '、', '缺', '水', '保', '濕', '、', '2', '5', '歲', '以', '上', '肌', '膚']\n",
      "['O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['我', '覺', '得', '這', '面', '膜', '在', '這', '價', '位', '還', '可', '以', '接', '受', '，', '尤', '其', '是', '很', '會', '過', '敏', '的', '人', '來', '說', '，', '能', '使', '用', '的', '產', '品', '不', '多', '，', '保', '濕', '度', '對', '乾', '性', '肌', '的', '我', '來', '說', '也', '還', '可', '以', '，', '這', '款', '我', '覺', '得', '可', '以', '再', '回', '購']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'I-FUNC', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['本', '身', '是', '極', '乾', '的', '膚', '質', ',', '常', '需', '要', '加', '強', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['皮', '珂', '兒', '是', '韓', '國', '製', '造', '的', '保', '養', '品', '，', '有', '可', '愛', '的', '火', '山', '插', '圖', '，', '鐵', '罐', '包', '裝', '，', '是', '很', '特', '殊', '的', '慕', '絲', '質', '地', '，', '需', '沖', '洗', '的', '清', '潔', '面', '膜', '，', '很', '適', '合', '油', '性', '肌', '膚', '來', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['有', '些', '保', '濕', '面', '膜', '，', '我', '的', '敏', '感', '性', '膚', '質', '如', '果', '敷', '太', '久', '，', '就', '會', '開', '始', '發', '癢', '，', '這', '款', '一', '直', '敷', '到', '最', '後', '都', '很', '舒', '服', '呢']\n",
      "['O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['但', '對', '皮', '膚', '乾', '癢', '保', '濕', '及', '透', '亮', '很', '有', '幫', '助', '的', '喲']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['我', '的', '皮', '膚', '是', '屬', '於', '油', '性', '敏', '感', '肌', '，', '用', '太', '刺', '激', '的', '保', '養', '品', '會', '有', '側', '痛', '和', '肌', '膚', '發', '紅', '的', '感', '覺']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['在', '這', '一', '下', '冷', '一', '下', '又', '熱', '的', '氣', '候', ',', '對', '於', '乾', '性', '肌', '膚', '的', '我', '來', '說', ',', '真', '是', '相', '當', '需', '要', '保', '濕']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n",
      "['效', '果', '方', '面', '我', '覺', '得', '還', '不', '錯', ',', '因', '為', '我', '是', '混', '合', '性', '肌', '膚', ',', '敷', '完', '了', '之', '後', '有', '覺', '得', '還', '滿', '保', '濕', '的', ',', '隔', '天', '早', '上', '起', '來', '比', '較', '會', '出', '油', '的', '地', '方', ',', '出', '油', '狀', '況', '就', '比', '較', '沒', '有', '那', '麼', '嚴', '重', '了', ',', '整', '題', '來', '說', '還', '滿', '推', '薦', '的', '喔']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['溫', '和', '的', '成', '份', '讓', '敏', '感', '的', '肌', '膚', '也', '能', '輕', '鬆', '的', '美', '白']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC']\n",
      "['', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A']]\n",
      "\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['溫', '柔', '而', '堅', '定', '的', '修', '護', '力', 'Z', 'e', 'p', 'h', 'y', 'r', 'i', 'n', 'e', '數', '字', '保', '養', ',', '非', '常', '溫', '和', '的', '產', '品', '即', '使', '是', '敏', '感', '肌', '也', '可', '以', '輕', '鬆', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['可', '視', '膚', '況', '/', '氣', '候', '選', '擇', '厚', '敷', 'o', 'r', '薄', '敷', '，', '不', '論', '是', '救', '急', '保', '養', '或', '是', '乾', '肌', '日', '常', '補', '水', '，', '保', '濕', '效', '果', '皆', '相', '當', '顯', '著', '，', '簡', '直', '宜', '家', '宜', '室', '四', '季', '皆', '宜', '齁', '~']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['建', '議', '乾', '肌', '的', '朋', '友', '們', '還', '是', '要', '勤', '做', '後', '續', '的', '保', '養', '動', '作']\n",
      "['O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['我', '的', '美', '麗', '日', '記', '一', '直', '是', '我', '的', '愛', '用', '品', '之', '一', '，', '對', '於', '健', '康', '膚', '色', '油', '性', '混', '合', '肌', '的', '我', '，', '挑', '選', '面', '膜', '的', '重', '點', '就', '是', '-', '-', '-', '可', '以', '變', '白', '、', '保', '濕', '、', '不', '會', '黏', '黏', '的', '(', '很', '重', '要', ')']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['顧', '名', '思', '義', '這', '是', '一', '款', '晚', '安', '凍', '膜', '，', '乾', '肌', '我', '本', '人', '於', '晚', '上', '卸', '妝', '洗', '臉', '洗', '澡', '後', '睡', '前', '保', '養', '使', '用']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STAT', 'I-STAT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FUNC', 'I-FUNC', 'O', 'O']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-B'], ['ApplyTo-0-B'], '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ['ApplyTo-0-A'], ['ApplyTo-0-A'], '', '']\n",
      "\n",
      "==========================\n",
      "\n",
      "['以 O ', '前 O ', '我 O ', '有 O ', '敷 O ', '過 O ', '別 O ', '款 O ', '的 O ', '美 B-FUNC ApplyTo-0-A ', '白 I-FUNC ApplyTo-0-A ', '面 O ', '膜 O ', '~ O ', '很 O ', '容 O ', '易 O ', '就 O ', '會 O ', '讓 O ', '我 O ', '的 O ', '臉 O ', '部 O ', '肌 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '過 I-STAT ApplyTo-0-B ', '敏 I-STAT ApplyTo-0-B ', '， O ', '會 O ', '有 O ', '紅 O ', '紅 O ', '的 O ', '臉 O ', '出 O ', '現 O ', ' ', '以 O ', '上 O ', '是 O ', '我 O ', '不 O ', '會 O ', '再 O ', '使 O ', '用 O ', '此 O ', '產 O ', '品 O ', '， O ', '也 O ', '不 O ', '會 O ', '推 O ', '薦 O ', '親 O ', '友 O ', '使 O ', '用 O ', '的 O ', '原 O ', '因 O ', '， O ', '最 O ', '大 O ', '原 O ', '因 O ', '是 O ', '因 O ', '為 O ', '； O ', '居 O ', '然 O ', '在 O ', '使 O ', '用 O ', '後 O ', '， O ', '肌 O ', '膚 O ', '反 O ', '而 O ', '變 O ', '得 O ', '乾 O ', '到 O ', '發 O ', '癢 O ', '， O ', '可 O ', '見 O ', '產 O ', '品 O ', '若 O ', '無 O ', '法 O ', '真 O ', '正 O ', '提 O ', '供 O ', '乾 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '滋 O ', '潤 O ', '和 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '的 O ', '話 O ', '， O ', '添 O ', '加 O ', '再 O ', '多 O ', '吸 O ', '引 O ', '消 O ', '費 O ', '者 O ', '的 O ', '成 O ', '分 O ', '也 O ', '不 O ', '見 O ', '得 O ', '能 O ', '得 O ', '到 O ', '消 O ', '費 O ', '者 O ', '的 O ', '心 O ', ' ', '適 O ', '用 O ', '族 O ', '群 O ', '為 O ', '乾 B-STAT ApplyTo-0-B ', '燥 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '、 O ', '缺 O ', '水 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '、 O ', '2 O ', '5 O ', '歲 O ', '以 O ', '上 O ', '肌 O ', '膚 O ', ' ', '我 O ', '覺 O ', '得 O ', '這 O ', '面 O ', '膜 O ', '在 O ', '這 O ', '價 O ', '位 O ', '還 O ', '可 O ', '以 O ', '接 O ', '受 O ', '， O ', '尤 O ', '其 O ', '是 O ', '很 O ', '會 O ', '過 O ', '敏 O ', '的 O ', '人 O ', '來 O ', '說 O ', '， O ', '能 O ', '使 O ', '用 O ', '的 O ', '產 O ', '品 O ', '不 O ', '多 O ', '， O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '度 I-FUNC ApplyTo-0-A ', '對 O ', '乾 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '的 O ', '我 O ', '來 O ', '說 O ', '也 O ', '還 O ', '可 O ', '以 O ', '， O ', '這 O ', '款 O ', '我 O ', '覺 O ', '得 O ', '可 O ', '以 O ', '再 O ', '回 O ', '購 O ', ' ', '本 O ', '身 O ', '是 O ', '極 O ', '乾 B-STAT ApplyTo-0-B ', '的 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '質 I-STAT ApplyTo-0-B ', ', O ', '常 O ', '需 O ', '要 O ', '加 O ', '強 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', ' ', '皮 O ', '珂 O ', '兒 O ', '是 O ', '韓 O ', '國 O ', '製 O ', '造 O ', '的 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '品 O ', '， O ', '有 O ', '可 O ', '愛 O ', '的 O ', '火 O ', '山 O ', '插 O ', '圖 O ', '， O ', '鐵 O ', '罐 O ', '包 O ', '裝 O ', '， O ', '是 O ', '很 O ', '特 O ', '殊 O ', '的 O ', '慕 O ', '絲 O ', '質 O ', '地 O ', '， O ', '需 O ', '沖 O ', '洗 O ', '的 O ', '清 O ', '潔 O ', '面 O ', '膜 O ', '， O ', '很 O ', '適 O ', '合 O ', '油 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '來 O ', '使 O ', '用 O ', ' ', '有 O ', '些 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '面 O ', '膜 O ', '， O ', '我 O ', '的 O ', '敏 B-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '質 I-STAT ApplyTo-0-B ', '如 O ', '果 O ', '敷 O ', '太 O ', '久 O ', '， O ', '就 O ', '會 O ', '開 O ', '始 O ', '發 O ', '癢 O ', '， O ', '這 O ', '款 O ', '一 O ', '直 O ', '敷 O ', '到 O ', '最 O ', '後 O ', '都 O ', '很 O ', '舒 O ', '服 O ', '呢 O ', ' ', '但 O ', '對 O ', '皮 B-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '乾 I-STAT ApplyTo-0-B ', '癢 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '及 O ', '透 O ', '亮 O ', '很 O ', '有 O ', '幫 O ', '助 O ', '的 O ', '喲 O ', ' ', '我 O ', '的 O ', '皮 O ', '膚 O ', '是 O ', '屬 O ', '於 O ', '油 O ', '性 O ', '敏 B-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '， O ', '用 O ', '太 O ', '刺 O ', '激 O ', '的 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '品 O ', '會 O ', '有 O ', '側 O ', '痛 O ', '和 O ', '肌 O ', '膚 O ', '發 O ', '紅 O ', '的 O ', '感 O ', '覺 O ', ' ', '在 O ', '這 O ', '一 O ', '下 O ', '冷 O ', '一 O ', '下 O ', '又 O ', '熱 O ', '的 O ', '氣 O ', '候 O ', ', O ', '對 O ', '於 O ', '乾 B-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '的 O ', '我 O ', '來 O ', '說 O ', ', O ', '真 O ', '是 O ', '相 O ', '當 O ', '需 O ', '要 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', ' ', '效 O ', '果 O ', '方 O ', '面 O ', '我 O ', '覺 O ', '得 O ', '還 O ', '不 O ', '錯 O ', ', O ', '因 O ', '為 O ', '我 O ', '是 O ', '混 B-STAT ApplyTo-0-B ', '合 I-STAT ApplyTo-0-B ', '性 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', ', O ', '敷 O ', '完 O ', '了 O ', '之 O ', '後 O ', '有 O ', '覺 O ', '得 O ', '還 O ', '滿 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '的 O ', ', O ', '隔 O ', '天 O ', '早 O ', '上 O ', '起 O ', '來 O ', '比 O ', '較 O ', '會 O ', '出 O ', '油 O ', '的 O ', '地 O ', '方 O ', ', O ', '出 O ', '油 O ', '狀 O ', '況 O ', '就 O ', '比 O ', '較 O ', '沒 O ', '有 O ', '那 O ', '麼 O ', '嚴 O ', '重 O ', '了 O ', ', O ', '整 O ', '題 O ', '來 O ', '說 O ', '還 O ', '滿 O ', '推 O ', '薦 O ', '的 O ', '喔 O ', ' ', '溫 O ', '和 O ', '的 O ', '成 O ', '份 O ', '讓 O ', '敏 B-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '的 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '膚 I-STAT ApplyTo-0-B ', '也 O ', '能 O ', '輕 O ', '鬆 O ', '的 O ', '美 B-FUNC ApplyTo-0-A ', '白 I-FUNC ApplyTo-0-A ', ' ', '溫 O ', '柔 O ', '而 O ', '堅 O ', '定 O ', '的 O ', '修 O ', '護 O ', '力 O ', 'Z O ', 'e O ', 'p O ', 'h O ', 'y O ', 'r O ', 'i O ', 'n O ', 'e O ', '數 O ', '字 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', ', O ', '非 O ', '常 O ', '溫 O ', '和 O ', '的 O ', '產 O ', '品 O ', '即 O ', '使 O ', '是 O ', '敏 B-STAT ApplyTo-0-B ', '感 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '也 O ', '可 O ', '以 O ', '輕 O ', '鬆 O ', '使 O ', '用 O ', ' ', '可 O ', '視 O ', '膚 O ', '況 O ', '/ O ', '氣 O ', '候 O ', '選 O ', '擇 O ', '厚 O ', '敷 O ', 'o O ', 'r O ', '薄 O ', '敷 O ', '， O ', '不 O ', '論 O ', '是 O ', '救 O ', '急 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '或 O ', '是 O ', '乾 B-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '日 O ', '常 O ', '補 O ', '水 O ', '， O ', '保 B-FUNC ', '濕 I-FUNC ', '效 O ', '果 O ', '皆 O ', '相 O ', '當 O ', '顯 O ', '著 O ', '， O ', '簡 O ', '直 O ', '宜 O ', '家 O ', '宜 O ', '室 O ', '四 O ', '季 O ', '皆 O ', '宜 O ', '齁 O ', '~ O ', ' ', '建 O ', '議 O ', '乾 B-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '的 O ', '朋 O ', '友 O ', '們 O ', '還 O ', '是 O ', '要 O ', '勤 O ', '做 O ', '後 O ', '續 O ', '的 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '動 O ', '作 O ', ' ', '我 O ', '的 O ', '美 O ', '麗 O ', '日 O ', '記 O ', '一 O ', '直 O ', '是 O ', '我 O ', '的 O ', '愛 O ', '用 O ', '品 O ', '之 O ', '一 O ', '， O ', '對 O ', '於 O ', '健 O ', '康 O ', '膚 O ', '色 O ', '油 O ', '性 O ', '混 B-STAT ApplyTo-0-B ', '合 I-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '的 O ', '我 O ', '， O ', '挑 O ', '選 O ', '面 O ', '膜 O ', '的 O ', '重 O ', '點 O ', '就 O ', '是 O ', '- O ', '- O ', '- O ', '可 O ', '以 O ', '變 O ', '白 O ', '、 O ', '保 B-FUNC ApplyTo-0-A ', '濕 I-FUNC ApplyTo-0-A ', '、 O ', '不 O ', '會 O ', '黏 O ', '黏 O ', '的 O ', '( O ', '很 O ', '重 O ', '要 O ', ') O ', ' ', '顧 O ', '名 O ', '思 O ', '義 O ', '這 O ', '是 O ', '一 O ', '款 O ', '晚 O ', '安 O ', '凍 O ', '膜 O ', '， O ', '乾 B-STAT ApplyTo-0-B ', '肌 I-STAT ApplyTo-0-B ', '我 O ', '本 O ', '人 O ', '於 O ', '晚 O ', '上 O ', '卸 O ', '妝 O ', '洗 O ', '臉 O ', '洗 O ', '澡 O ', '後 O ', '睡 O ', '前 O ', '保 B-FUNC ApplyTo-0-A ', '養 I-FUNC ApplyTo-0-A ', '使 O ', '用 O ', ' ']\n",
      "['以 O ' '前 O ' '我 O ' '有 O ' '敷 O ' '過 O ' '別 O ' '款 O ' '的 O '\n",
      " '美 B-FUNC ApplyTo-0-A ' '白 I-FUNC ApplyTo-0-A ' '面 O ' '膜 O ' '~ O '\n",
      " '很 O ' '容 O ' '易 O ' '就 O ' '會 O ' '讓 O ' '我 O ' '的 O ' '臉 O ' '部 O '\n",
      " '肌 B-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '過 I-STAT ApplyTo-0-B '\n",
      " '敏 I-STAT ApplyTo-0-B ' '， O ' '會 O ' '有 O ' '紅 O ' '紅 O ' '的 O ' '臉 O '\n",
      " '出 O ' '現 O ' ' ' '以 O ' '上 O ' '是 O ' '我 O ' '不 O ' '會 O ' '再 O ' '使 O '\n",
      " '用 O ' '此 O ' '產 O ' '品 O ' '， O ' '也 O ' '不 O ' '會 O ' '推 O ' '薦 O '\n",
      " '親 O ' '友 O ' '使 O ' '用 O ' '的 O ' '原 O ' '因 O ' '， O ' '最 O ' '大 O '\n",
      " '原 O ' '因 O ' '是 O ' '因 O ' '為 O ' '； O ' '居 O ' '然 O ' '在 O ' '使 O '\n",
      " '用 O ' '後 O ' '， O ' '肌 O ' '膚 O ' '反 O ' '而 O ' '變 O ' '得 O ' '乾 O '\n",
      " '到 O ' '發 O ' '癢 O ' '， O ' '可 O ' '見 O ' '產 O ' '品 O ' '若 O ' '無 O '\n",
      " '法 O ' '真 O ' '正 O ' '提 O ' '供 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '性 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B '\n",
      " '滋 O ' '潤 O ' '和 O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A '\n",
      " '的 O ' '話 O ' '， O ' '添 O ' '加 O ' '再 O ' '多 O ' '吸 O ' '引 O ' '消 O '\n",
      " '費 O ' '者 O ' '的 O ' '成 O ' '分 O ' '也 O ' '不 O ' '見 O ' '得 O ' '能 O '\n",
      " '得 O ' '到 O ' '消 O ' '費 O ' '者 O ' '的 O ' '心 O ' ' ' '適 O ' '用 O ' '族 O '\n",
      " '群 O ' '為 O ' '乾 B-STAT ApplyTo-0-B ' '燥 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '、 O ' '缺 O ' '水 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' '、 O ' '2 O ' '5 O ' '歲 O ' '以 O ' '上 O ' '肌 O '\n",
      " '膚 O ' ' ' '我 O ' '覺 O ' '得 O ' '這 O ' '面 O ' '膜 O ' '在 O ' '這 O ' '價 O '\n",
      " '位 O ' '還 O ' '可 O ' '以 O ' '接 O ' '受 O ' '， O ' '尤 O ' '其 O ' '是 O '\n",
      " '很 O ' '會 O ' '過 O ' '敏 O ' '的 O ' '人 O ' '來 O ' '說 O ' '， O ' '能 O '\n",
      " '使 O ' '用 O ' '的 O ' '產 O ' '品 O ' '不 O ' '多 O ' '， O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '度 I-FUNC ApplyTo-0-A '\n",
      " '對 O ' '乾 B-STAT ApplyTo-0-B ' '性 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '的 O ' '我 O ' '來 O ' '說 O ' '也 O ' '還 O ' '可 O '\n",
      " '以 O ' '， O ' '這 O ' '款 O ' '我 O ' '覺 O ' '得 O ' '可 O ' '以 O ' '再 O '\n",
      " '回 O ' '購 O ' ' ' '本 O ' '身 O ' '是 O ' '極 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '的 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '質 I-STAT ApplyTo-0-B '\n",
      " ', O ' '常 O ' '需 O ' '要 O ' '加 O ' '強 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' ' ' '皮 O ' '珂 O ' '兒 O ' '是 O ' '韓 O ' '國 O '\n",
      " '製 O ' '造 O ' '的 O ' '保 B-FUNC ApplyTo-0-A ' '養 I-FUNC ApplyTo-0-A '\n",
      " '品 O ' '， O ' '有 O ' '可 O ' '愛 O ' '的 O ' '火 O ' '山 O ' '插 O ' '圖 O '\n",
      " '， O ' '鐵 O ' '罐 O ' '包 O ' '裝 O ' '， O ' '是 O ' '很 O ' '特 O ' '殊 O '\n",
      " '的 O ' '慕 O ' '絲 O ' '質 O ' '地 O ' '， O ' '需 O ' '沖 O ' '洗 O ' '的 O '\n",
      " '清 O ' '潔 O ' '面 O ' '膜 O ' '， O ' '很 O ' '適 O ' '合 O '\n",
      " '油 B-STAT ApplyTo-0-B ' '性 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B '\n",
      " '膚 I-STAT ApplyTo-0-B ' '來 O ' '使 O ' '用 O ' ' ' '有 O ' '些 O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '面 O ' '膜 O ' '， O '\n",
      " '我 O ' '的 O ' '敏 B-STAT ApplyTo-0-B ' '感 I-STAT ApplyTo-0-B '\n",
      " '性 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '質 I-STAT ApplyTo-0-B '\n",
      " '如 O ' '果 O ' '敷 O ' '太 O ' '久 O ' '， O ' '就 O ' '會 O ' '開 O ' '始 O '\n",
      " '發 O ' '癢 O ' '， O ' '這 O ' '款 O ' '一 O ' '直 O ' '敷 O ' '到 O ' '最 O '\n",
      " '後 O ' '都 O ' '很 O ' '舒 O ' '服 O ' '呢 O ' ' ' '但 O ' '對 O '\n",
      " '皮 B-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' '乾 I-STAT ApplyTo-0-B '\n",
      " '癢 O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '及 O ' '透 O '\n",
      " '亮 O ' '很 O ' '有 O ' '幫 O ' '助 O ' '的 O ' '喲 O ' ' ' '我 O ' '的 O ' '皮 O '\n",
      " '膚 O ' '是 O ' '屬 O ' '於 O ' '油 O ' '性 O ' '敏 B-STAT ApplyTo-0-B '\n",
      " '感 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '， O ' '用 O ' '太 O '\n",
      " '刺 O ' '激 O ' '的 O ' '保 B-FUNC ApplyTo-0-A ' '養 I-FUNC ApplyTo-0-A '\n",
      " '品 O ' '會 O ' '有 O ' '側 O ' '痛 O ' '和 O ' '肌 O ' '膚 O ' '發 O ' '紅 O '\n",
      " '的 O ' '感 O ' '覺 O ' ' ' '在 O ' '這 O ' '一 O ' '下 O ' '冷 O ' '一 O ' '下 O '\n",
      " '又 O ' '熱 O ' '的 O ' '氣 O ' '候 O ' ', O ' '對 O ' '於 O '\n",
      " '乾 B-STAT ApplyTo-0-B ' '性 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B '\n",
      " '膚 I-STAT ApplyTo-0-B ' '的 O ' '我 O ' '來 O ' '說 O ' ', O ' '真 O ' '是 O '\n",
      " '相 O ' '當 O ' '需 O ' '要 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '濕 I-FUNC ApplyTo-0-A ' ' ' '效 O ' '果 O ' '方 O ' '面 O ' '我 O ' '覺 O '\n",
      " '得 O ' '還 O ' '不 O ' '錯 O ' ', O ' '因 O ' '為 O ' '我 O ' '是 O '\n",
      " '混 B-STAT ApplyTo-0-B ' '合 I-STAT ApplyTo-0-B ' '性 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '膚 I-STAT ApplyTo-0-B ' ', O ' '敷 O ' '完 O '\n",
      " '了 O ' '之 O ' '後 O ' '有 O ' '覺 O ' '得 O ' '還 O ' '滿 O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A ' '的 O ' ', O ' '隔 O '\n",
      " '天 O ' '早 O ' '上 O ' '起 O ' '來 O ' '比 O ' '較 O ' '會 O ' '出 O ' '油 O '\n",
      " '的 O ' '地 O ' '方 O ' ', O ' '出 O ' '油 O ' '狀 O ' '況 O ' '就 O ' '比 O '\n",
      " '較 O ' '沒 O ' '有 O ' '那 O ' '麼 O ' '嚴 O ' '重 O ' '了 O ' ', O ' '整 O '\n",
      " '題 O ' '來 O ' '說 O ' '還 O ' '滿 O ' '推 O ' '薦 O ' '的 O ' '喔 O ' ' ' '溫 O '\n",
      " '和 O ' '的 O ' '成 O ' '份 O ' '讓 O ' '敏 B-STAT ApplyTo-0-B '\n",
      " '感 I-STAT ApplyTo-0-B ' '的 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B '\n",
      " '膚 I-STAT ApplyTo-0-B ' '也 O ' '能 O ' '輕 O ' '鬆 O ' '的 O '\n",
      " '美 B-FUNC ApplyTo-0-A ' '白 I-FUNC ApplyTo-0-A ' ' ' '溫 O ' '柔 O ' '而 O '\n",
      " '堅 O ' '定 O ' '的 O ' '修 O ' '護 O ' '力 O ' 'Z O ' 'e O ' 'p O ' 'h O '\n",
      " 'y O ' 'r O ' 'i O ' 'n O ' 'e O ' '數 O ' '字 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '養 I-FUNC ApplyTo-0-A ' ', O ' '非 O ' '常 O ' '溫 O ' '和 O ' '的 O ' '產 O '\n",
      " '品 O ' '即 O ' '使 O ' '是 O ' '敏 B-STAT ApplyTo-0-B '\n",
      " '感 I-STAT ApplyTo-0-B ' '肌 I-STAT ApplyTo-0-B ' '也 O ' '可 O ' '以 O '\n",
      " '輕 O ' '鬆 O ' '使 O ' '用 O ' ' ' '可 O ' '視 O ' '膚 O ' '況 O ' '/ O ' '氣 O '\n",
      " '候 O ' '選 O ' '擇 O ' '厚 O ' '敷 O ' 'o O ' 'r O ' '薄 O ' '敷 O ' '， O '\n",
      " '不 O ' '論 O ' '是 O ' '救 O ' '急 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '養 I-FUNC ApplyTo-0-A ' '或 O ' '是 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '日 O ' '常 O ' '補 O ' '水 O ' '， O ' '保 B-FUNC '\n",
      " '濕 I-FUNC ' '效 O ' '果 O ' '皆 O ' '相 O ' '當 O ' '顯 O ' '著 O ' '， O '\n",
      " '簡 O ' '直 O ' '宜 O ' '家 O ' '宜 O ' '室 O ' '四 O ' '季 O ' '皆 O ' '宜 O '\n",
      " '齁 O ' '~ O ' ' ' '建 O ' '議 O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '的 O ' '朋 O ' '友 O ' '們 O ' '還 O ' '是 O ' '要 O '\n",
      " '勤 O ' '做 O ' '後 O ' '續 O ' '的 O ' '保 B-FUNC ApplyTo-0-A '\n",
      " '養 I-FUNC ApplyTo-0-A ' '動 O ' '作 O ' ' ' '我 O ' '的 O ' '美 O ' '麗 O '\n",
      " '日 O ' '記 O ' '一 O ' '直 O ' '是 O ' '我 O ' '的 O ' '愛 O ' '用 O ' '品 O '\n",
      " '之 O ' '一 O ' '， O ' '對 O ' '於 O ' '健 O ' '康 O ' '膚 O ' '色 O ' '油 O '\n",
      " '性 O ' '混 B-STAT ApplyTo-0-B ' '合 I-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '的 O ' '我 O ' '， O ' '挑 O ' '選 O ' '面 O ' '膜 O '\n",
      " '的 O ' '重 O ' '點 O ' '就 O ' '是 O ' '- O ' '- O ' '- O ' '可 O ' '以 O '\n",
      " '變 O ' '白 O ' '、 O ' '保 B-FUNC ApplyTo-0-A ' '濕 I-FUNC ApplyTo-0-A '\n",
      " '、 O ' '不 O ' '會 O ' '黏 O ' '黏 O ' '的 O ' '( O ' '很 O ' '重 O ' '要 O '\n",
      " ') O ' ' ' '顧 O ' '名 O ' '思 O ' '義 O ' '這 O ' '是 O ' '一 O ' '款 O ' '晚 O '\n",
      " '安 O ' '凍 O ' '膜 O ' '， O ' '乾 B-STAT ApplyTo-0-B '\n",
      " '肌 I-STAT ApplyTo-0-B ' '我 O ' '本 O ' '人 O ' '於 O ' '晚 O ' '上 O ' '卸 O '\n",
      " '妝 O ' '洗 O ' '臉 O ' '洗 O ' '澡 O ' '後 O ' '睡 O ' '前 O '\n",
      " '保 B-FUNC ApplyTo-0-A ' '養 I-FUNC ApplyTo-0-A ' '使 O ' '用 O ' ' ']\n",
      "=================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [770, 736]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-08f37ac09677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         print(decode_rel(batch_ent[0], batch_rel[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_input_dev\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0mbatch_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=========================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-a09d10607566>\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#                                           labels=np.unique(combine_outputs)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [770, 736]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always') \n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (batch_x, batch_ent, batch_rel, batch_index) in enumerate(dev_loader):\n",
    "        model.eval()\n",
    "        ent_output, rel_output = model(batch_x.cuda() if USE_CUDA else batch_x)\n",
    "        \n",
    "        ent_loss = criterion_tag(ent_output.cpu(), batch_ent.view(BATCH_SIZE*MAX_LEN))\n",
    "        ent_output = ent_argmax(ent_output)\n",
    "        \n",
    "        rel_loss = criterion_rel(rel_output.cpu(), batch_rel.view(BATCH_SIZE*MAX_LEN*MAX_LEN))\n",
    "        rel_output = rel_argmax(rel_output)\n",
    "    \n",
    "        print()\n",
    "        \n",
    "        print(\"Entity loss : %.4f\" % ent_loss)\n",
    "        print(\"Relation loss : %.4f\" % rel_loss)\n",
    "        print()\n",
    "        \n",
    "        # true\n",
    "#         print(decode_rel(batch_ent[0], batch_rel[0]))\n",
    "        \n",
    "        batch_decode(ent_output, rel_output, batch_index, raw_input_dev, \\\n",
    "                     batch_ent, batch_rel)\n",
    "        print()\n",
    "        print('==========================')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# def batch_decode(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel):\n",
    "#     rel_lists = []\n",
    "#     combine_outputs = []\n",
    "#     combine_trues = []\n",
    "    \n",
    "#     for e,r,i in zip(ent_output, rel_output, batch_index):\n",
    "#         r_list, appear_error = decode_rel(e, r)\n",
    "#         if appear_error:\n",
    "#             continue\n",
    "            \n",
    "#         rel_lists.append(r_list)\n",
    "        \n",
    "#         len_of_list = len(word_lists[i])\n",
    "#         word_list = word_lists[i]\n",
    "#         pridict_ent = index2tag(e, ix_to_ent_tag)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(word_list)\n",
    "#         print(pridict_ent[:len_of_list])\n",
    "#         print(r_list[:len_of_list])\n",
    "        \n",
    "#         for word in range(len_of_list):\n",
    "#             add_r = ''\n",
    "#             if type(r_list[word]) is list:\n",
    "#                 for single_r in r_list[word]:\n",
    "#                     add_r = add_r+single_r+' '\n",
    "#             combine_outputs.append(word_list[word]+' '+pridict_ent[word]+' '+add_r)\n",
    "        \n",
    "#         combine_outputs.append(\" \")\n",
    "#         print()\n",
    "#         print('==========================')\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "    \n",
    "#     print(precision_recall_fscore_support(combine_trues, combine_outputs, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "# def decode_rel(ent_output, rel_output):\n",
    "#     r_list = []\n",
    "#     r_dict = {}\n",
    "#     appear_error = False\n",
    "#     pred_ent = index2tag(ent_output, ix_to_ent_tag)\n",
    "\n",
    "#     e_loc = 0\n",
    "#     for loc, e in enumerate(pred_ent):\n",
    "#         if e[0]=='B':\n",
    "#             e_loc = loc\n",
    "#             r_dict[loc] = {\n",
    "#                 '_2ndtag':e[2:],\n",
    "#                 'end':loc,\n",
    "#             }\n",
    "#             r_list.append([])\n",
    "            \n",
    "        \n",
    "#         elif e[0]=='I':\n",
    "            \n",
    "#             # 錯誤來自於，entity預測錯誤，沒有預測到B tag，直接跳到I tag\n",
    "#             # 所以沒有紀錄e_loc\n",
    "#             try: \n",
    "#                 r_dict[e_loc]['end'] = loc\n",
    "#             except KeyError:\n",
    "#                 appear_error = True\n",
    "#                 break\n",
    "#             r_list.append([])\n",
    "            \n",
    "#         else:\n",
    "#             r_list.append(\"\")\n",
    "    \n",
    "    \n",
    "#     IsB = False\n",
    "#     IsNext = False\n",
    "#     num_reocrd = -1\n",
    "#     now_loc = 0\n",
    "#     end_loc = 0\n",
    "#     tag = \"\"\n",
    "#     preAorB = \"\"\n",
    "#     nowAorB = \"\"\n",
    "#     pre_complete_rel = \"\"\n",
    "#     now_complete_rel = \"\"\n",
    "#     rel_keyerror = False\n",
    "    \n",
    "#     for now in range(len(rel_output)):\n",
    "#         for loc, rel in enumerate(rel_output[now][:now+1]):\n",
    "#             rel = rel.cpu().numpy()\n",
    "\n",
    "#             if rel!=rel_tag_to_ix[REL_NONE] and IsB==False and IsNext==False:\n",
    "\n",
    "#                 IsB = True\n",
    "#                 IsNext = True\n",
    "#                 tag = ix_to_rel_tag[int(rel)]\n",
    "#                 num_reocrd+=1\n",
    "                \n",
    "#                 now_loc = loc\n",
    "                \n",
    "#                 # 錯誤來自於，now_loc找不到，也就是說，rel預測出來是有關係存在\n",
    "#                 # 但在entity中卻沒有預測出來，所以r_dict中沒有紀錄\n",
    "#                 try:\n",
    "#                     end_loc = r_dict[now_loc]['end']\n",
    "#                 except KeyError:\n",
    "#                     rel_keyerror = True\n",
    "#                     break\n",
    "                \n",
    "                \n",
    "#                 second_tag = r_dict[now_loc]['_2ndtag']\n",
    "#                 preAorB = check_loc(second_tag)\n",
    "#                 nowAorB = 'B' if preAorB=='A' else 'A'\n",
    "                \n",
    "#                 pre_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+preAorB\n",
    "#                 now_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+nowAorB\n",
    "                \n",
    "#                 for token in range(now_loc, end_loc+1):\n",
    "#                     r_list[token].append(pre_complete_rel)\n",
    "                \n",
    "#                 r_list[now].append(now_complete_rel)\n",
    "\n",
    "                \n",
    "#             elif rel!=rel_tag_to_ix[REL_NONE] and IsB:\n",
    "#                 if loc<=end_loc:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     IsB = False\n",
    "            \n",
    "#             elif rel!=rel_tag_to_ix[REL_NONE] and IsNext:\n",
    "#                 r_list[now] = r_list[now-1]\n",
    "                \n",
    "#             else:\n",
    "#                 IsB = False\n",
    "        \n",
    "#         if rel_keyerror:\n",
    "#             rel_keyerror = False\n",
    "#             break\n",
    "                \n",
    "                \n",
    "#     return r_list, appear_error\n",
    "                \n",
    "\n",
    "                \n",
    "# def check_loc(second_tag):\n",
    "#     if second_tag in rule[0]:\n",
    "#         return 'A'\n",
    "#     elif second_tag in rule[2]:\n",
    "#         return 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def batch_decode(ent_output, rel_output, batch_index, word_lists, true_ent, true_rel):\n",
    "    rel_lists = []\n",
    "    combine_outputs = []\n",
    "    combine_trues = []\n",
    "    \n",
    "    for e,r,i in zip(ent_output, rel_output, batch_index):\n",
    "        r_list, appear_error = decode_rel(e, r)\n",
    "        if appear_error:\n",
    "            continue\n",
    "            \n",
    "        rel_lists.append(r_list)\n",
    "        \n",
    "        len_of_list = len(word_lists[i])\n",
    "        word_list = word_lists[i]\n",
    "        pridict_ent = index2tag(e, ix_to_ent_tag)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(word_list)\n",
    "        print(pridict_ent[:len_of_list])\n",
    "        print(r_list[:len_of_list])\n",
    "        \n",
    "        for word in range(len_of_list):\n",
    "            add_r = ''\n",
    "            if type(r_list[word]) is list:\n",
    "                for single_r in r_list[word]:\n",
    "                    add_r = add_r+single_r+' '\n",
    "            combine_outputs.append(word_list[word]+' '+pridict_ent[word]+' '+add_r)\n",
    "        \n",
    "        combine_outputs.append(\" \")\n",
    "        print()\n",
    "        print('==========================')\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    # true \n",
    "    for e,r,i in zip(true_ent, true_rel, batch_index):\n",
    "        r_list, appear_error = decode_rel(e, r)\n",
    "        if appear_error:\n",
    "            continue\n",
    "            \n",
    "        rel_lists.append(r_list)\n",
    "        \n",
    "        len_of_list = len(word_lists[i])\n",
    "        word_list = word_lists[i]\n",
    "        pridict_ent = index2tag(e, ix_to_ent_tag)\n",
    "\n",
    "        \n",
    "        for word in range(len_of_list):\n",
    "            add_r = ''\n",
    "            if type(r_list[word]) is list:\n",
    "                for single_r in r_list[word]:\n",
    "                    add_r = add_r+single_r+' '\n",
    "            combine_trues.append(word_list[word]+' '+pridict_ent[word]+' '+add_r)\n",
    "        \n",
    "        combine_trues.append(\" \")\n",
    "    \n",
    "\n",
    "    \n",
    "    print(combine_outputs)\n",
    "    print(np.array(combine_outputs))\n",
    "    print('=================')\n",
    "    \n",
    "#     print(precision_recall_fscore_support(combine_trues, combine_outputs, average='macro', \\\n",
    "#                                           labels=np.unique(combine_outputs)))\n",
    "\n",
    "    print(precision_recall_fscore_support(combine_trues, combine_outputs, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "def decode_rel(ent_output, rel_output):\n",
    "    r_list = []\n",
    "    r_dict = {}\n",
    "    appear_error = False\n",
    "    pred_ent = index2tag(ent_output, ix_to_ent_tag)\n",
    "\n",
    "    e_loc = 0\n",
    "    for loc, e in enumerate(pred_ent):\n",
    "        if e[0]=='B':\n",
    "            e_loc = loc\n",
    "            r_dict[loc] = {\n",
    "                '_2ndtag':e[2:],\n",
    "                'end':loc,\n",
    "            }\n",
    "            r_list.append([])\n",
    "            \n",
    "        \n",
    "        elif e[0]=='I':\n",
    "            \n",
    "            # 錯誤來自於，entity預測錯誤，沒有預測到B tag，直接跳到I tag\n",
    "            # 所以沒有紀錄e_loc\n",
    "            try: \n",
    "                r_dict[e_loc]['end'] = loc\n",
    "            except KeyError:\n",
    "                appear_error = True\n",
    "                break\n",
    "            r_list.append([])\n",
    "            \n",
    "        else:\n",
    "            r_list.append(\"\")\n",
    "    \n",
    "    \n",
    "    IsB = False\n",
    "    IsNext = False\n",
    "    num_reocrd = -1\n",
    "    now_loc = 0\n",
    "    end_loc = 0\n",
    "    tag = \"\"\n",
    "    preAorB = \"\"\n",
    "    nowAorB = \"\"\n",
    "    pre_complete_rel = \"\"\n",
    "    now_complete_rel = \"\"\n",
    "    rel_keyerror = False\n",
    "    \n",
    "    for now in range(len(rel_output)):\n",
    "        for loc, rel in enumerate(rel_output[now][:now+1]):\n",
    "            rel = rel.cpu().numpy()\n",
    "\n",
    "            if rel!=rel_tag_to_ix[REL_NONE] and IsB==False and IsNext==False:\n",
    "\n",
    "                IsB = True\n",
    "                IsNext = True\n",
    "                tag = ix_to_rel_tag[int(rel)]\n",
    "                num_reocrd+=1\n",
    "                \n",
    "                now_loc = loc\n",
    "                \n",
    "                # 錯誤來自於，now_loc找不到，也就是說，rel預測出來是有關係存在\n",
    "                # 但預測是'O'\n",
    "                # 但在entity中卻沒有預測出來，所以r_dict中沒有紀錄\n",
    "                try:\n",
    "                    end_loc = r_dict[now_loc]['end']\n",
    "                except KeyError:\n",
    "                    rel_keyerror = True\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                second_tag = r_dict[now_loc]['_2ndtag']\n",
    "                preAorB = check_loc(second_tag)\n",
    "                nowAorB = 'B' if preAorB=='A' else 'A'\n",
    "                \n",
    "                pre_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+preAorB\n",
    "                now_complete_rel = tag+\"-\"+str(num_reocrd)+\"-\"+nowAorB\n",
    "                \n",
    "                for token in range(now_loc, end_loc+1):\n",
    "                    r_list[token].append(pre_complete_rel)\n",
    "                \n",
    "                r_list[now].append(now_complete_rel)\n",
    "\n",
    "                \n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsB:\n",
    "                if loc<=end_loc:\n",
    "                    pass\n",
    "                else:\n",
    "                    IsB = False\n",
    "            \n",
    "            elif rel!=rel_tag_to_ix[REL_NONE] and IsNext:\n",
    "                r_list[now] = r_list[now-1]\n",
    "                \n",
    "            else:\n",
    "                IsB = False\n",
    "        \n",
    "        if rel_keyerror:\n",
    "            rel_keyerror = False\n",
    "            break\n",
    "                \n",
    "                \n",
    "    return r_list, appear_error\n",
    "                \n",
    "\n",
    "                \n",
    "def check_loc(second_tag):\n",
    "    if second_tag in rule[0]:\n",
    "        return 'A'\n",
    "    elif second_tag in rule[2]:\n",
    "        return 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
