{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/notebooks/sinica/dataset/'\n",
    "train_data = root+'facial.train'\n",
    "dev_data = root+'facial.dev'\n",
    "test_data = root+'facial.test'\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "PAD_TAG = \"<PAD>\"\n",
    "tag_to_ix = {START_TAG: 0, STOP_TAG: 1, PAD_TAG:2, \"B-Func\": 3, \"I-Func\": 4, \"O\": 5}\n",
    "\n",
    "tagset_size = len(tag_to_ix)\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM1 = 10\n",
    "HIDDEN_DIM2 = 8\n",
    "LABEL_EMBED_DIM = 3\n",
    "DENSE_OUT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(data):\n",
    "    with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().splitlines()\n",
    "        \n",
    "    return content\n",
    "\n",
    "def get_word_and_label(_content, start_w, end_w):\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "    for word_set in _content[start_w:end_w]:\n",
    "        word_list.append(word_set[0])\n",
    "        tag_list.append(word_set[2:])\n",
    "    \n",
    "    return word_list, tag_list\n",
    "\n",
    "def split_to_list(content):\n",
    "    init = 0\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    for i, c in enumerate(content):\n",
    "        if c=='':\n",
    "            words, tags = get_word_and_label(content, init, i)\n",
    "            init = i+1\n",
    "            word_list.append(words)\n",
    "            tag_list.append(tags)\n",
    "            \n",
    "    return word_list, tag_list\n",
    "    \n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_all(seqs, to_ix):\n",
    "    seq_list = []\n",
    "    for i in range(len(seqs)):\n",
    "        seq_list.append(prepare_sequence(seqs[i], to_ix))\n",
    "        \n",
    "    seq_list = torch.stack(seq_list)\n",
    "        \n",
    "    return seq_list\n",
    "\n",
    "def word2index(word_list):\n",
    "    word_to_ix = {\"<START>\":0, \"<STOP>\":1, \"<PAD>\":2}\n",
    "    for sentence in word_list:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "                \n",
    "    return word_to_ix\n",
    "\n",
    "def find_max_len(word_list):\n",
    "    max_len = 0\n",
    "    for i in range(len(word_list)):\n",
    "        if max_len<len(word_list[i]):\n",
    "            max_len=len(word_list[i])\n",
    "            \n",
    "    return max_len\n",
    "\n",
    "def filter_len(word_list):\n",
    "    reserved_index = []\n",
    "    for i in range(len(word_list)):\n",
    "        if len(word_list[i])<MAX_LEN:\n",
    "            reserved_index.append(i)\n",
    "            \n",
    "    return reserved_index\n",
    "\n",
    "def filter_sentence(reserved_index, word_list, tag_list):\n",
    "    filter_word = list(word_list[i] for i in reserved_index)\n",
    "    filter_tag = list(tag_list[i] for i in reserved_index)\n",
    "    return filter_word, filter_tag\n",
    "\n",
    "def pad_seq(seq):\n",
    "    seq += [PAD_TAG for i in range(MAX_LEN-len(seq))]\n",
    "    return seq\n",
    "\n",
    "def pad_all(filter_word, filter_tag):\n",
    "    input_padded = [pad_seq(s) for s in filter_word]\n",
    "    target_padded = [pad_seq(s) for s in filter_tag]\n",
    "    \n",
    "    return input_padded, target_padded\n",
    "\n",
    "#======================================\n",
    "def dataload(input_var, target_var):\n",
    "    torch_dataset = Data.TensorDataset(input_var, target_var)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               \n",
    "        num_workers=2,       \n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim1, hidden_dim2, \\\n",
    "                 label_embed_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.label_embed_dim = label_embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim1 // 2,\n",
    "                            num_layers=2, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.dense = nn.Linear(hidden_dim1, DENSE_OUT)\n",
    "        \n",
    "        self.lstm = nn.LSTM(DENSE_OUT+label_embed_dim, hidden_dim2, batch_first=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim2, self.tagset_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.label_embed = nn.Linear(self.tagset_size, self.label_embed_dim)\n",
    "        \n",
    "#         self.hidden1 = self.init_hidden1()\n",
    "#         self.hidden2 = self.init_hidden2()\n",
    "#         self.to_label_embed = self.init_label_embed()\n",
    "        \n",
    "        \n",
    "    def init_hidden1(self):       \n",
    "        hidden = torch.randn(2*2, BATCH_SIZE, self.hidden_dim1 // 2)   \n",
    "#         hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_hidden2(self):       \n",
    "        hidden = torch.randn(1, BATCH_SIZE, self.hidden_dim2)        \n",
    "#         hidden = Variable(hidden.data, requires_grad=True)\n",
    "\n",
    "        return (hidden.cuda(), hidden.cuda())if USE_CUDA else (hidden,hidden)\n",
    "    \n",
    "    def init_label_embed(self):\n",
    "        hidden = torch.randn(BATCH_SIZE, MAX_LEN, self.label_embed_dim)\n",
    "        return hidden.cuda()if USE_CUDA else hidden\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        self.hidden1 = self.init_hidden1()\n",
    "        self.hidden2 = self.init_hidden2()\n",
    "        self.to_label_embed = self.init_label_embed()\n",
    "        \n",
    "        embeds = self.word_embeds(sentence)\n",
    "#         print(embeds.size())\n",
    "#         print()\n",
    "        \n",
    "        bilstm_out, self.hidden1 = self.bilstm(embeds, self.hidden1)\n",
    "#         print(bilstm_out.size())\n",
    "#         print(self.hidden1[0].size())\n",
    "#         print()\n",
    "\n",
    "        dense_out = self.dense(bilstm_out)\n",
    "        \n",
    "#         combine_lstm = torch.cat((dense_out, to_label_embed), 2)\n",
    "        combine_lstm = torch.cat((dense_out, self.to_label_embed), 2)\n",
    "        \n",
    "        lstm_out, self.hidden2 = self.lstm(combine_lstm, self.hidden2)\n",
    "#         print(lstm_out.size())\n",
    "#         print(self.hidden2[0].size())\n",
    "#         print()\n",
    "        \n",
    "        to_tags = self.hidden2tag(lstm_out)\n",
    "#         print(to_tags.size())\n",
    "\n",
    "        output = self.softmax(to_tags)\n",
    "#         print(output.size())\n",
    "#         print()\n",
    "        \n",
    "        self.to_label_embed = self.label_embed(output)\n",
    "#         print(self.to_label_embed.size())\n",
    "        \n",
    "        return output.view(BATCH_SIZE*MAX_LEN, self.tagset_size)\n",
    "#         return output.view(BATCH_SIZE, self.tagset_size, MAX_LEN)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = readfile(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, tag_list = split_to_list(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = word2index(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,   3,   4,   5,   6,  20,   3,  21,\n",
       "         22,  23,  24,  25,  26,  27,  28,   7,  29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(word_list[0], word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = find_max_len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_index = filter_len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_word, filter_tag = filter_sentence(reserved_index, word_list, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_padded, target_padded = pad_all(filter_word, filter_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = prepare_all(input_padded, word_to_ix)\n",
    "target_var = prepare_all(target_padded, tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2516"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_to_ix)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataload(input_var, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(vocab_size, tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM1, HIDDEN_DIM2, \\\n",
    "              LABEL_EMBED_DIM).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:02<02:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | loss 0.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:05<02:18,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | loss 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [00:08<02:15,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | loss 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [00:11<02:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | loss 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [00:14<02:07,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | loss 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [00:16<02:01,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | loss 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [00:19<01:59,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | loss 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [00:21<01:55,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | loss 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [00:24<01:52,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | loss 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [00:27<01:51,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | loss 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [00:30<01:48,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | loss 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [00:33<01:44,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 | loss 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [00:35<01:42,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 | loss 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [00:38<01:39,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 | loss 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [00:41<01:36,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 | loss 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [00:44<01:33,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 | loss 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [00:46<01:30,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | loss 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [00:49<01:27,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 | loss 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [00:51<01:23,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 | loss 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 20/50 [00:53<01:19,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | loss 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 21/50 [00:55<01:17,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 | loss 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 22/50 [00:58<01:14,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 | loss 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 23/50 [01:00<01:10,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 | loss 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 24/50 [01:02<01:07,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 | loss 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 25/50 [01:05<01:05,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 | loss 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 26/50 [01:07<01:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 | loss 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 27/50 [01:10<00:59,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 | loss 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 28/50 [01:13<00:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 | loss 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 29/50 [01:16<00:55,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 | loss 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 30/50 [01:18<00:52,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 | loss 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 31/50 [01:21<00:50,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 | loss 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 32/50 [01:24<00:47,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 | loss 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 33/50 [01:27<00:45,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 | loss 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 34/50 [01:29<00:42,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 | loss 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 35/50 [01:33<00:39,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 | loss 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 36/50 [01:35<00:37,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 | loss 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 37/50 [01:38<00:34,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 | loss 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 38/50 [01:41<00:32,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 | loss 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 39/50 [01:45<00:29,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 | loss 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 40/50 [01:48<00:27,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 | loss 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 41/50 [01:51<00:24,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 | loss 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 42/50 [01:54<00:21,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 | loss 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 43/50 [01:57<00:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 | loss 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 44/50 [02:00<00:16,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 | loss 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 45/50 [02:03<00:13,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 | loss 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 46/50 [02:06<00:11,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 | loss 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [02:09<00:08,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 | loss 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 48/50 [02:12<00:05,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 | loss 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 49/50 [02:15<00:02,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 | loss 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 50/50 [02:18<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 | loss 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 10\n",
    "print_every = 10\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(range(50)):  \n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x.cuda())\n",
    "#         print(output.size())\n",
    "#         print(batch_y.size())\n",
    "#         break\n",
    "        batch_y = batch_y.view(BATCH_SIZE*MAX_LEN)\n",
    "        loss = criterion(output, batch_y.cuda())\n",
    "        loss.backward()\n",
    "#         loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if step % print_every == 0:\n",
    "        #    print('%.4f| epoch: %d| step: %d| %s' % (loss, epoch, step, timeSince(start)))\n",
    "    print(\"epoch: %d | loss %.4f\"%(epoch,loss))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12800, 6])\n",
      "torch.Size([128, 100])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "for step, (batch_x, batch_y) in enumerate(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch_x.cuda())\n",
    "    print(output.size())\n",
    "    print(batch_y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
